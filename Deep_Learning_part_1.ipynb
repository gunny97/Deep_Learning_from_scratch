{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning1 - ch.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare optimizer with MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ana3\\envs\\t_f2.2\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ana3\\envs\\t_f2.2\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\ana3\\envs\\t_f2.2\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from layers_collection.ipynb\n",
      "importing Jupyter notebook from optimizer.ipynb\n",
      "importing Jupyter notebook from Neural_Network.ipynb\n"
     ]
    }
   ],
   "source": [
    "# needed library\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist     # same datasets as book\n",
    "sys.path.append(os.pardir)\n",
    "from collections import OrderedDict\n",
    "import import_ipynb\n",
    "from layers_collection import *\n",
    "from Neural_Network import neural_network\n",
    "from optimizer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### opitmizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:59: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========iteration:0===========\n",
      "SGD:2.217598020644401\n",
      "momentum:2.165926433442298\n",
      "adagrad:inf\n",
      "adam:inf\n",
      "===========iteration:100===========\n",
      "SGD:0.24659090157915367\n",
      "momentum:0.2965482360593935\n",
      "adagrad:0.24986376968087431\n",
      "adam:2.2994933354637226\n",
      "===========iteration:200===========\n",
      "SGD:0.16741219811495037\n",
      "momentum:0.39707796698564046\n",
      "adagrad:0.29832473124735337\n",
      "adam:2.312230980836386\n",
      "===========iteration:300===========\n",
      "SGD:0.2158307607476894\n",
      "momentum:0.30554227659645367\n",
      "adagrad:0.2802485407772219\n",
      "adam:2.29795196298522\n",
      "===========iteration:400===========\n",
      "SGD:0.2082270697082611\n",
      "momentum:0.3762171040575127\n",
      "adagrad:0.25842444529233005\n",
      "adam:2.3280049492451216\n",
      "===========iteration:500===========\n",
      "SGD:0.03368732354086977\n",
      "momentum:0.11680837591285786\n",
      "adagrad:0.07496011586357985\n",
      "adam:2.3021758798103344\n",
      "===========iteration:600===========\n",
      "SGD:0.04540191296467957\n",
      "momentum:0.15485721261297108\n",
      "adagrad:0.12544148010702313\n",
      "adam:2.3150861323021275\n",
      "===========iteration:700===========\n",
      "SGD:0.05034668531408991\n",
      "momentum:0.1460496008873712\n",
      "adagrad:0.1481900398411345\n",
      "adam:2.2933201399634937\n",
      "===========iteration:800===========\n",
      "SGD:0.08973568071156522\n",
      "momentum:0.20005659993089614\n",
      "adagrad:0.131786733595201\n",
      "adam:2.3068644188788507\n",
      "===========iteration:900===========\n",
      "SGD:0.09389383080791004\n",
      "momentum:0.14559120627172645\n",
      "adagrad:0.07149018428473129\n",
      "adam:2.3063005611749086\n",
      "===========iteration:1000===========\n",
      "SGD:0.05303132427939347\n",
      "momentum:0.10086182948863584\n",
      "adagrad:0.14625097222434122\n",
      "adam:2.3261013002175837\n",
      "===========iteration:1100===========\n",
      "SGD:0.014575770282440371\n",
      "momentum:0.061329431434733205\n",
      "adagrad:0.02732585540741775\n",
      "adam:2.2798186369823137\n",
      "===========iteration:1200===========\n",
      "SGD:0.031203644127909068\n",
      "momentum:0.058717301127373514\n",
      "adagrad:0.09718871342128584\n",
      "adam:2.314387696395178\n",
      "===========iteration:1300===========\n",
      "SGD:0.05471617135625829\n",
      "momentum:0.09937611618556147\n",
      "adagrad:0.07815501285926375\n",
      "adam:2.2946462660212776\n",
      "===========iteration:1400===========\n",
      "SGD:0.011138053576377906\n",
      "momentum:0.04968606145453695\n",
      "adagrad:0.030605533274377886\n",
      "adam:2.3258346288294254\n",
      "===========iteration:1500===========\n",
      "SGD:0.01964266198148336\n",
      "momentum:0.10232059775524995\n",
      "adagrad:0.027380612416457437\n",
      "adam:2.311242625553102\n",
      "===========iteration:1600===========\n",
      "SGD:0.02825726507913286\n",
      "momentum:0.13165985019169402\n",
      "adagrad:0.03734284251338703\n",
      "adam:2.319096604215459\n",
      "===========iteration:1700===========\n",
      "SGD:0.01745235771221267\n",
      "momentum:0.0575302602777076\n",
      "adagrad:0.04577612716209252\n",
      "adam:2.308824000189941\n",
      "===========iteration:1800===========\n",
      "SGD:0.02695124616960471\n",
      "momentum:0.1321085306972029\n",
      "adagrad:0.10120962194573202\n",
      "adam:2.280832340299244\n",
      "===========iteration:1900===========\n",
      "SGD:0.052862074479044716\n",
      "momentum:0.10089048260148428\n",
      "adagrad:0.12671234777066404\n",
      "adam:2.3114569912019007\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = load_mnist(normalize = True)\n",
    "\n",
    "batch_size = 128\n",
    "max_iter = 2000\n",
    "\n",
    "'''\n",
    "# learning rate: 0.1\n",
    "optimizers = {\n",
    "    'SGD' : sgd(lr=0.1),\n",
    "    'momentum' : momentum(lr=0.1),\n",
    "    'adagrad' : adagrad(lr=0.1),\n",
    "    'adam' : adam(lr=0.1)\n",
    "}\n",
    "''' \n",
    "#learning reate: 0.01\n",
    "optimizers = {\n",
    "    'SGD' : sgd(),\n",
    "    'momentum' : momentum(),\n",
    "    'adagrad' : adagrad(),\n",
    "    'adam' : adam()\n",
    "}\n",
    "\n",
    "each_neural_network = {}\n",
    "each_train_loss = {}\n",
    "\n",
    "for key in optimizers.keys():\n",
    "    each_neural_network[key] = neural_network(input_size=28*28, hidden_size=[100,100,100,100,100], output_size=10,\n",
    "                                              activation = 'relu', weight_init_std = 'relu', weight_decay_lambda=0)\n",
    "    each_train_loss[key] = []\n",
    "\n",
    "for i in range(max_iter):\n",
    "    batch_mask = np.random.choice(x_train.shape[0], batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "\n",
    "    for key in optimizers.keys():\n",
    "        grads = each_neural_network[key].gradient(x_batch, y_batch)\n",
    "        optimizers[key].update(each_neural_network[key].params, grads)\n",
    "\n",
    "        loss = each_neural_network[key].loss(x_batch, y_batch)\n",
    "        each_train_loss[key].append(loss)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(\"===========\" + \"iteration:\" + str(i) + \"===========\")\n",
    "        for key in optimizers.keys():\n",
    "            loss = each_neural_network[key].loss(x_batch, y_batch)\n",
    "            print(key + \":\" + str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(x):\n",
    "\n",
    "    window_len = 11\n",
    "    s = np.r_[x[window_len-1:0:-1], x, x[-1:-window_len:-1]]\n",
    "    w = np.kaiser(window_len, 2)\n",
    "    y = np.convolve(w/w.sum(), s, mode='valid')\n",
    "    return y[5:len(y)-5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "markers = {\"SGD\": \"o\", \"momentum\": \"x\", \"adagrad\": \"s\", \"adam\": \"D\"}\n",
    "x = np.arange(max_iter)\n",
    "for key in optimizers.keys():\n",
    "    plt.plot(x, smooth_curve(each_train_loss[key]), marker=markers[key],\n",
    "        markevery=500, label=key)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.ylim(0, 2.5)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare weight initialization methods with MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========iteration:0===========\n",
      "std=0.01:2.302497125891029\n",
      "Xavier:2.3050309437784997\n",
      "He:2.5013009984091057\n",
      "===========iteration:100===========\n",
      "std=0.01:2.302316213761485\n",
      "Xavier:2.274378343182759\n",
      "He:1.6343591404523916\n",
      "===========iteration:200===========\n",
      "std=0.01:2.302477915086877\n",
      "Xavier:2.2404329913798615\n",
      "He:0.7886218753459269\n",
      "===========iteration:300===========\n",
      "std=0.01:2.302860991725959\n",
      "Xavier:2.1304560042225775\n",
      "He:0.5698713099721606\n",
      "===========iteration:400===========\n",
      "std=0.01:2.2999040426356627\n",
      "Xavier:1.9061021034153747\n",
      "He:0.38917080031165385\n",
      "===========iteration:500===========\n",
      "std=0.01:2.304488393302383\n",
      "Xavier:1.3944243645990337\n",
      "He:0.2987980386067842\n",
      "===========iteration:600===========\n",
      "std=0.01:2.3028776032485\n",
      "Xavier:0.8337241676358909\n",
      "He:0.26897552658117463\n",
      "===========iteration:700===========\n",
      "std=0.01:2.302316724101747\n",
      "Xavier:0.6947476790437805\n",
      "He:0.33380198222064206\n",
      "===========iteration:800===========\n",
      "std=0.01:2.3029105443262745\n",
      "Xavier:0.5629359575498167\n",
      "He:0.2299220168528825\n",
      "===========iteration:900===========\n",
      "std=0.01:2.3000038838530643\n",
      "Xavier:0.4071555248689259\n",
      "He:0.2517091933314444\n",
      "===========iteration:1000===========\n",
      "std=0.01:2.300723416996536\n",
      "Xavier:0.563839677038726\n",
      "He:0.4494882140272331\n",
      "===========iteration:1100===========\n",
      "std=0.01:2.3001752420245087\n",
      "Xavier:0.3068311687003323\n",
      "He:0.1667517210237867\n",
      "===========iteration:1200===========\n",
      "std=0.01:2.295698641958642\n",
      "Xavier:0.2909437221186376\n",
      "He:0.1960936008229629\n",
      "===========iteration:1300===========\n",
      "std=0.01:2.3010169232318303\n",
      "Xavier:0.27719406454118006\n",
      "He:0.22878730034575478\n",
      "===========iteration:1400===========\n",
      "std=0.01:2.298940194800402\n",
      "Xavier:0.29560524617839823\n",
      "He:0.20661019006431225\n",
      "===========iteration:1500===========\n",
      "std=0.01:2.302274069822678\n",
      "Xavier:0.35867634066042264\n",
      "He:0.20015482312040753\n",
      "===========iteration:1600===========\n",
      "std=0.01:2.2965662801853153\n",
      "Xavier:0.2856857084057107\n",
      "He:0.17493691564855446\n",
      "===========iteration:1700===========\n",
      "std=0.01:2.301069913652915\n",
      "Xavier:0.28500518326376695\n",
      "He:0.2117653523187429\n",
      "===========iteration:1800===========\n",
      "std=0.01:2.307641426358799\n",
      "Xavier:0.33915384507063595\n",
      "He:0.2859741231941119\n",
      "===========iteration:1900===========\n",
      "std=0.01:2.2984967279257376\n",
      "Xavier:0.4324858559103951\n",
      "He:0.3689656188324951\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = load_mnist(normalize = True)\n",
    "\n",
    "batch_size = 128\n",
    "max_iter = 2000\n",
    "\n",
    "optimizer = sgd(lr=0.01)\n",
    "\n",
    "weight_init_types = {'std=0.01': 0.01, 'Xavier': 'sigmoid', 'He': 'relu'}\n",
    "\n",
    "each_neural_network = {}\n",
    "each_train_loss = {}\n",
    "\n",
    "for key, value in weight_init_types.items():\n",
    "    each_neural_network[key] = neural_network(input_size=28*28, hidden_size=[100,100,100,100,100], output_size=10,\n",
    "                                              activation = 'relu', weight_init_std = value, weight_decay_lambda=0)\n",
    "    each_train_loss[key] = []\n",
    "\n",
    "for i in range(max_iter):\n",
    "    batch_mask = np.random.choice(x_train.shape[0], batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "\n",
    "    for key in weight_init_types.keys():\n",
    "        grads = each_neural_network[key].gradient(x_batch, y_batch)\n",
    "        optimizer.update(each_neural_network[key].params, grads)\n",
    "\n",
    "        loss = each_neural_network[key].loss(x_batch, y_batch)\n",
    "        each_train_loss[key].append(loss)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(\"===========\" + \"iteration:\" + str(i) + \"===========\")\n",
    "        for key in weight_init_types.keys():\n",
    "            loss = each_neural_network[key].loss(x_batch, y_batch)\n",
    "            print(key + \":\" + str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "markers = {'std=0.01': 'o', 'Xavier': 's', 'He': 'D'}\n",
    "x = np.arange(max_iter)\n",
    "for key in weight_init_types.keys():\n",
    "    plt.plot(x, smooth_curve(each_train_loss[key]), marker=markers[key], markevery=500, label=key)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.ylim(0, 2.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = load_mnist(normalize = True)\n",
    "x_train = x_train[:1000]\n",
    "y_trian = y_train[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "def train_network(weight_init_std):\n",
    "    bn_network = neural_network(input_size=28*28,\n",
    "                                hidden_size = [100,100,100,100,100],\n",
    "                                output_size = 10,\n",
    "                                weight_init_std = weight_init_std,\n",
    "                                batchnorm =True)\n",
    "    simple_network = neural_network(input_size=28*28,\n",
    "                                    hidden_size=[100,100,100,100,100],\n",
    "                                    output_size = 10,\n",
    "                                    weight_init_std = weight_init_std)\n",
    "\n",
    "    optimizer = sgd(lr= learning_rate)\n",
    "    \n",
    "    simple_accuracy = []\n",
    "    bn_accuracy = []\n",
    "\n",
    "    iter_per_epoch = max(train_size / batch_size, 1)\n",
    "    epoch_ = 0\n",
    "\n",
    "    for i in range(1000000000):\n",
    "        batch_mask = np.random.choice(train_size, batch_size)\n",
    "        x_batch = x_train[batch_mask]\n",
    "        y_batch = y_trian[batch_mask]\n",
    "\n",
    "        for _network in (bn_network, simple_network):\n",
    "            grads = _network.gradient(x_batch, y_batch)\n",
    "            optimizer.update(_network.params, grads)\n",
    "\n",
    "        if i % iter_per_epoch == 0:\n",
    "            train_acc = simple_network.accuracy(x_train, y_trian)\n",
    "            bn_train_acc = bn_network.accuracy(x_train, y_trian)\n",
    "            simple_accuracy.append(train_acc)\n",
    "            bn_accuracy.append(bn_train_acc)\n",
    "\n",
    "            print(\"epoch:\" + str(epoch_) + \" | \" + str(train_acc) + \" - \"\n",
    "                  + str(bn_train_acc))\n",
    "\n",
    "            epoch_ += 1\n",
    "            if epoch_ >= max_epochs:\n",
    "                break\n",
    "\n",
    "    return simple_accuracy, bn_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== 1/16 ==============\n",
      "epoch:0 | 0.097 - 0.092\n",
      "epoch:1 | 0.097 - 0.125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:59: RuntimeWarning: divide by zero encountered in log\n",
      "<string>:86: RuntimeWarning: overflow encountered in square\n",
      "<string>:86: RuntimeWarning: invalid value encountered in double_scalars\n",
      "<string>:37: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:2 | 0.097 - 0.174\n",
      "epoch:3 | 0.097 - 0.202\n",
      "epoch:4 | 0.097 - 0.219\n",
      "epoch:5 | 0.097 - 0.231\n",
      "epoch:6 | 0.097 - 0.241\n",
      "epoch:7 | 0.097 - 0.261\n",
      "epoch:8 | 0.097 - 0.283\n",
      "epoch:9 | 0.097 - 0.303\n",
      "epoch:10 | 0.097 - 0.322\n",
      "epoch:11 | 0.097 - 0.351\n",
      "epoch:12 | 0.097 - 0.371\n",
      "epoch:13 | 0.097 - 0.394\n",
      "epoch:14 | 0.097 - 0.406\n",
      "epoch:15 | 0.097 - 0.423\n",
      "epoch:16 | 0.097 - 0.435\n",
      "epoch:17 | 0.097 - 0.439\n",
      "epoch:18 | 0.097 - 0.463\n",
      "epoch:19 | 0.097 - 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n",
      "<string>:59: RuntimeWarning: divide by zero encountered in log\n",
      "<string>:86: RuntimeWarning: overflow encountered in square\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== 2/16 ==============\n",
      "epoch:0 | 0.094 - 0.089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:86: RuntimeWarning: invalid value encountered in double_scalars\n",
      "C:\\ana3\\envs\\t_f2.2\\lib\\site-packages\\numpy\\core\\fromnumeric.py:87: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 | 0.097 - 0.124\n",
      "epoch:2 | 0.097 - 0.16\n",
      "epoch:3 | 0.097 - 0.176\n",
      "epoch:4 | 0.097 - 0.198\n",
      "epoch:5 | 0.097 - 0.219\n",
      "epoch:6 | 0.097 - 0.248\n",
      "epoch:7 | 0.097 - 0.28\n",
      "epoch:8 | 0.097 - 0.293\n",
      "epoch:9 | 0.097 - 0.317\n",
      "epoch:10 | 0.097 - 0.34\n",
      "epoch:11 | 0.097 - 0.366\n",
      "epoch:12 | 0.097 - 0.374\n",
      "epoch:13 | 0.097 - 0.391\n",
      "epoch:14 | 0.097 - 0.419\n",
      "epoch:15 | 0.097 - 0.417\n",
      "epoch:16 | 0.097 - 0.429\n",
      "epoch:17 | 0.097 - 0.446\n",
      "epoch:18 | 0.097 - 0.468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.097 - 0.479\n",
      "============== 3/16 ==============\n",
      "epoch:0 | 0.142 - 0.117\n",
      "epoch:1 | 0.451 - 0.123\n",
      "epoch:2 | 0.558 - 0.146\n",
      "epoch:3 | 0.666 - 0.203\n",
      "epoch:4 | 0.745 - 0.238\n",
      "epoch:5 | 0.78 - 0.281\n",
      "epoch:6 | 0.829 - 0.317\n",
      "epoch:7 | 0.857 - 0.332\n",
      "epoch:8 | 0.891 - 0.377\n",
      "epoch:9 | 0.915 - 0.412\n",
      "epoch:10 | 0.923 - 0.438\n",
      "epoch:11 | 0.952 - 0.471\n",
      "epoch:12 | 0.959 - 0.494\n",
      "epoch:13 | 0.969 - 0.523\n",
      "epoch:14 | 0.978 - 0.544\n",
      "epoch:15 | 0.98 - 0.572\n",
      "epoch:16 | 0.986 - 0.591\n",
      "epoch:17 | 0.991 - 0.599\n",
      "epoch:18 | 0.994 - 0.622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.995 - 0.641\n",
      "============== 4/16 ==============\n",
      "epoch:0 | 0.098 - 0.089\n",
      "epoch:1 | 0.264 - 0.132\n",
      "epoch:2 | 0.4 - 0.191\n",
      "epoch:3 | 0.501 - 0.279\n",
      "epoch:4 | 0.56 - 0.358\n",
      "epoch:5 | 0.62 - 0.433\n",
      "epoch:6 | 0.663 - 0.504\n",
      "epoch:7 | 0.691 - 0.55\n",
      "epoch:8 | 0.719 - 0.582\n",
      "epoch:9 | 0.749 - 0.627\n",
      "epoch:10 | 0.782 - 0.657\n",
      "epoch:11 | 0.794 - 0.691\n",
      "epoch:12 | 0.823 - 0.706\n",
      "epoch:13 | 0.819 - 0.721\n",
      "epoch:14 | 0.827 - 0.737\n",
      "epoch:15 | 0.838 - 0.743\n",
      "epoch:16 | 0.855 - 0.761\n",
      "epoch:17 | 0.862 - 0.77\n",
      "epoch:18 | 0.863 - 0.778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.872 - 0.787\n",
      "============== 5/16 ==============\n",
      "epoch:0 | 0.107 - 0.091\n",
      "epoch:1 | 0.111 - 0.157\n",
      "epoch:2 | 0.112 - 0.311\n",
      "epoch:3 | 0.115 - 0.438\n",
      "epoch:4 | 0.114 - 0.527\n",
      "epoch:5 | 0.114 - 0.574\n",
      "epoch:6 | 0.116 - 0.629\n",
      "epoch:7 | 0.117 - 0.671\n",
      "epoch:8 | 0.117 - 0.702\n",
      "epoch:9 | 0.119 - 0.734\n",
      "epoch:10 | 0.123 - 0.775\n",
      "epoch:11 | 0.126 - 0.793\n",
      "epoch:12 | 0.14 - 0.806\n",
      "epoch:13 | 0.146 - 0.829\n",
      "epoch:14 | 0.153 - 0.831\n",
      "epoch:15 | 0.159 - 0.845\n",
      "epoch:16 | 0.161 - 0.858\n",
      "epoch:17 | 0.166 - 0.87\n",
      "epoch:18 | 0.17 - 0.874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.182 - 0.886\n",
      "============== 6/16 ==============\n",
      "epoch:0 | 0.049 - 0.084\n",
      "epoch:1 | 0.098 - 0.162\n",
      "epoch:2 | 0.119 - 0.413\n",
      "epoch:3 | 0.123 - 0.6\n",
      "epoch:4 | 0.146 - 0.647\n",
      "epoch:5 | 0.117 - 0.697\n",
      "epoch:6 | 0.126 - 0.743\n",
      "epoch:7 | 0.125 - 0.775\n",
      "epoch:8 | 0.13 - 0.801\n",
      "epoch:9 | 0.117 - 0.828\n",
      "epoch:10 | 0.117 - 0.851\n",
      "epoch:11 | 0.117 - 0.865\n",
      "epoch:12 | 0.171 - 0.881\n",
      "epoch:13 | 0.17 - 0.893\n",
      "epoch:14 | 0.117 - 0.9\n",
      "epoch:15 | 0.117 - 0.91\n",
      "epoch:16 | 0.142 - 0.912\n",
      "epoch:17 | 0.119 - 0.93\n",
      "epoch:18 | 0.176 - 0.932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.144 - 0.946\n",
      "============== 7/16 ==============\n",
      "epoch:0 | 0.093 - 0.075\n",
      "epoch:1 | 0.116 - 0.373\n",
      "epoch:2 | 0.117 - 0.615\n",
      "epoch:3 | 0.117 - 0.699\n",
      "epoch:4 | 0.162 - 0.762\n",
      "epoch:5 | 0.117 - 0.806\n",
      "epoch:6 | 0.117 - 0.828\n",
      "epoch:7 | 0.117 - 0.858\n",
      "epoch:8 | 0.117 - 0.872\n",
      "epoch:9 | 0.117 - 0.9\n",
      "epoch:10 | 0.117 - 0.925\n",
      "epoch:11 | 0.117 - 0.942\n",
      "epoch:12 | 0.117 - 0.948\n",
      "epoch:13 | 0.117 - 0.964\n",
      "epoch:14 | 0.117 - 0.968\n",
      "epoch:15 | 0.117 - 0.976\n",
      "epoch:16 | 0.117 - 0.98\n",
      "epoch:17 | 0.116 - 0.985\n",
      "epoch:18 | 0.117 - 0.987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.117 - 0.991\n",
      "============== 8/16 ==============\n",
      "epoch:0 | 0.117 - 0.104\n",
      "epoch:1 | 0.117 - 0.36\n",
      "epoch:2 | 0.117 - 0.687\n",
      "epoch:3 | 0.117 - 0.756\n",
      "epoch:4 | 0.117 - 0.812\n",
      "epoch:5 | 0.117 - 0.859\n",
      "epoch:6 | 0.117 - 0.915\n",
      "epoch:7 | 0.116 - 0.949\n",
      "epoch:8 | 0.116 - 0.958\n",
      "epoch:9 | 0.116 - 0.97\n",
      "epoch:10 | 0.116 - 0.981\n",
      "epoch:11 | 0.116 - 0.99\n",
      "epoch:12 | 0.116 - 0.99\n",
      "epoch:13 | 0.116 - 0.993\n",
      "epoch:14 | 0.116 - 0.995\n",
      "epoch:15 | 0.116 - 0.997\n",
      "epoch:16 | 0.116 - 0.998\n",
      "epoch:17 | 0.116 - 0.999\n",
      "epoch:18 | 0.117 - 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.116 - 0.999\n",
      "============== 9/16 ==============\n",
      "epoch:0 | 0.093 - 0.167\n",
      "epoch:1 | 0.099 - 0.556\n",
      "epoch:2 | 0.117 - 0.75\n",
      "epoch:3 | 0.117 - 0.837\n",
      "epoch:4 | 0.117 - 0.903\n",
      "epoch:5 | 0.117 - 0.948\n",
      "epoch:6 | 0.117 - 0.972\n",
      "epoch:7 | 0.117 - 0.974\n",
      "epoch:8 | 0.117 - 0.989\n",
      "epoch:9 | 0.117 - 0.992\n",
      "epoch:10 | 0.117 - 0.993\n",
      "epoch:11 | 0.117 - 0.995\n",
      "epoch:12 | 0.117 - 0.998\n",
      "epoch:13 | 0.117 - 1.0\n",
      "epoch:14 | 0.117 - 1.0\n",
      "epoch:15 | 0.117 - 1.0\n",
      "epoch:16 | 0.117 - 1.0\n",
      "epoch:17 | 0.117 - 1.0\n",
      "epoch:18 | 0.117 - 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.117 - 1.0\n",
      "============== 10/16 ==============\n",
      "epoch:0 | 0.116 - 0.159\n",
      "epoch:1 | 0.105 - 0.575\n",
      "epoch:2 | 0.105 - 0.772\n",
      "epoch:3 | 0.117 - 0.895\n",
      "epoch:4 | 0.105 - 0.918\n",
      "epoch:5 | 0.105 - 0.934\n",
      "epoch:6 | 0.117 - 0.968\n",
      "epoch:7 | 0.117 - 0.961\n",
      "epoch:8 | 0.117 - 0.977\n",
      "epoch:9 | 0.117 - 0.99\n",
      "epoch:10 | 0.117 - 0.99\n",
      "epoch:11 | 0.117 - 0.992\n",
      "epoch:12 | 0.117 - 0.997\n",
      "epoch:13 | 0.117 - 0.997\n",
      "epoch:14 | 0.117 - 0.999\n",
      "epoch:15 | 0.117 - 0.994\n",
      "epoch:16 | 0.117 - 0.995\n",
      "epoch:17 | 0.117 - 0.96\n",
      "epoch:18 | 0.117 - 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.117 - 0.966\n",
      "============== 11/16 ==============\n",
      "epoch:0 | 0.097 - 0.224\n",
      "epoch:1 | 0.116 - 0.454\n",
      "epoch:2 | 0.116 - 0.711\n",
      "epoch:3 | 0.116 - 0.687\n",
      "epoch:4 | 0.116 - 0.744\n",
      "epoch:5 | 0.116 - 0.776\n",
      "epoch:6 | 0.116 - 0.803\n",
      "epoch:7 | 0.116 - 0.803\n",
      "epoch:8 | 0.116 - 0.793\n",
      "epoch:9 | 0.116 - 0.825\n",
      "epoch:10 | 0.117 - 0.875\n",
      "epoch:11 | 0.117 - 0.89\n",
      "epoch:12 | 0.117 - 0.889\n",
      "epoch:13 | 0.117 - 0.89\n",
      "epoch:14 | 0.116 - 0.89\n",
      "epoch:15 | 0.116 - 0.891\n",
      "epoch:16 | 0.116 - 0.886\n",
      "epoch:17 | 0.116 - 0.891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 | 0.116 - 0.877\n",
      "epoch:19 | 0.116 - 0.89\n",
      "============== 12/16 ==============\n",
      "epoch:0 | 0.116 - 0.191\n",
      "epoch:1 | 0.116 - 0.448\n",
      "epoch:2 | 0.116 - 0.598\n",
      "epoch:3 | 0.116 - 0.788\n",
      "epoch:4 | 0.116 - 0.819\n",
      "epoch:5 | 0.116 - 0.848\n",
      "epoch:6 | 0.117 - 0.867\n",
      "epoch:7 | 0.117 - 0.861\n",
      "epoch:8 | 0.117 - 0.884\n",
      "epoch:9 | 0.117 - 0.894\n",
      "epoch:10 | 0.117 - 0.887\n",
      "epoch:11 | 0.117 - 0.845\n",
      "epoch:12 | 0.117 - 0.895\n",
      "epoch:13 | 0.117 - 0.901\n",
      "epoch:14 | 0.117 - 0.9\n",
      "epoch:15 | 0.117 - 0.899\n",
      "epoch:16 | 0.117 - 0.902\n",
      "epoch:17 | 0.117 - 0.974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 | 0.117 - 0.986\n",
      "epoch:19 | 0.117 - 0.972\n",
      "============== 13/16 ==============\n",
      "epoch:0 | 0.116 - 0.225\n",
      "epoch:1 | 0.116 - 0.452\n",
      "epoch:2 | 0.116 - 0.554\n",
      "epoch:3 | 0.116 - 0.482\n",
      "epoch:4 | 0.117 - 0.531\n",
      "epoch:5 | 0.117 - 0.565\n",
      "epoch:6 | 0.117 - 0.609\n",
      "epoch:7 | 0.117 - 0.617\n",
      "epoch:8 | 0.117 - 0.678\n",
      "epoch:9 | 0.117 - 0.694\n",
      "epoch:10 | 0.117 - 0.561\n",
      "epoch:11 | 0.117 - 0.659\n",
      "epoch:12 | 0.117 - 0.695\n",
      "epoch:13 | 0.116 - 0.695\n",
      "epoch:14 | 0.117 - 0.694\n",
      "epoch:15 | 0.117 - 0.706\n",
      "epoch:16 | 0.116 - 0.707\n",
      "epoch:17 | 0.116 - 0.706\n",
      "epoch:18 | 0.116 - 0.706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.116 - 0.708\n",
      "============== 14/16 ==============\n",
      "epoch:0 | 0.116 - 0.12\n",
      "epoch:1 | 0.105 - 0.418\n",
      "epoch:2 | 0.117 - 0.472\n",
      "epoch:3 | 0.117 - 0.459\n",
      "epoch:4 | 0.117 - 0.508\n",
      "epoch:5 | 0.117 - 0.516\n",
      "epoch:6 | 0.117 - 0.599\n",
      "epoch:7 | 0.117 - 0.589\n",
      "epoch:8 | 0.117 - 0.489\n",
      "epoch:9 | 0.117 - 0.586\n",
      "epoch:10 | 0.117 - 0.601\n",
      "epoch:11 | 0.117 - 0.615\n",
      "epoch:12 | 0.117 - 0.611\n",
      "epoch:13 | 0.117 - 0.599\n",
      "epoch:14 | 0.117 - 0.605\n",
      "epoch:15 | 0.117 - 0.591\n",
      "epoch:16 | 0.117 - 0.607\n",
      "epoch:17 | 0.117 - 0.613\n",
      "epoch:18 | 0.117 - 0.622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.117 - 0.622\n",
      "============== 15/16 ==============\n",
      "epoch:0 | 0.116 - 0.096\n",
      "epoch:1 | 0.116 - 0.366\n",
      "epoch:2 | 0.117 - 0.467\n",
      "epoch:3 | 0.117 - 0.433\n",
      "epoch:4 | 0.117 - 0.495\n",
      "epoch:5 | 0.117 - 0.506\n",
      "epoch:6 | 0.117 - 0.507\n",
      "epoch:7 | 0.117 - 0.502\n",
      "epoch:8 | 0.117 - 0.507\n",
      "epoch:9 | 0.117 - 0.512\n",
      "epoch:10 | 0.117 - 0.575\n",
      "epoch:11 | 0.117 - 0.575\n",
      "epoch:12 | 0.117 - 0.555\n",
      "epoch:13 | 0.117 - 0.592\n",
      "epoch:14 | 0.117 - 0.517\n",
      "epoch:15 | 0.117 - 0.601\n",
      "epoch:16 | 0.117 - 0.588\n",
      "epoch:17 | 0.117 - 0.606\n",
      "epoch:18 | 0.117 - 0.611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 | 0.117 - 0.609\n",
      "============== 16/16 ==============\n",
      "epoch:0 | 0.116 - 0.152\n",
      "epoch:1 | 0.116 - 0.238\n",
      "epoch:2 | 0.116 - 0.375\n",
      "epoch:3 | 0.117 - 0.386\n",
      "epoch:4 | 0.116 - 0.41\n",
      "epoch:5 | 0.117 - 0.401\n",
      "epoch:6 | 0.117 - 0.411\n",
      "epoch:7 | 0.117 - 0.416\n",
      "epoch:8 | 0.117 - 0.417\n",
      "epoch:9 | 0.117 - 0.412\n",
      "epoch:10 | 0.117 - 0.461\n",
      "epoch:11 | 0.117 - 0.466\n",
      "epoch:12 | 0.117 - 0.491\n",
      "epoch:13 | 0.117 - 0.416\n",
      "epoch:14 | 0.117 - 0.422\n",
      "epoch:15 | 0.117 - 0.511\n",
      "epoch:16 | 0.116 - 0.518\n",
      "epoch:17 | 0.116 - 0.517\n",
      "epoch:18 | 0.116 - 0.515\n",
      "epoch:19 | 0.116 - 0.52\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "weight_init_candidates = np.logspace(0, -4, num=16)\n",
    "x = np.arange(max_epochs)\n",
    "\n",
    "plt.rc('font',size=8)\n",
    "plt.rc('axes',titlesize=8)\n",
    "plt.rc('axes',labelsize=8)\n",
    "plt.rc('legend',fontsize=8)\n",
    "plt.rc('xtick',labelsize=8)\n",
    "plt.rc('xtick',labelsize=8)\n",
    "\n",
    "for i, w in enumerate(weight_init_candidates):\n",
    "    print(\"============== \" + str(i+1) + \"/16\" + \" ==============\")\n",
    "    train_acc_list, bn_train_acc_list = train_network(w)\n",
    "\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.title(\"W:\" + str(w)[:5])\n",
    "    if i == 15:\n",
    "        plt.plot(x, bn_train_acc_list,\n",
    "                 label='Batch Normalization', markevery=2)\n",
    "        plt.plot(x, train_acc_list, linestyle=\"--\",\n",
    "                 label='Normal(without BatchNorm)', markevery=2)\n",
    "    else:\n",
    "        plt.plot(x, bn_train_acc_list, markevery=2)\n",
    "        plt.plot(x, train_acc_list, linestyle=\"--\", markevery=2)\n",
    "\n",
    "    plt.ylim(0, 1.0)\n",
    "    if i % 4:\n",
    "        plt.yticks([])\n",
    "    else:\n",
    "        plt.ylabel(\"accuracy\")\n",
    "    if i < 12:\n",
    "        plt.xticks([])\n",
    "    else:\n",
    "        plt.xlabel(\"epochs\")\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting - weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0   weight_decay_applied ->, train acc:0.12, test acc:0.1008\n",
      "naive network-> train acc:0.16333333333333333, test acc:0.1292\n",
      "epoch: 1   weight_decay_applied ->, train acc:0.12, test acc:0.099\n",
      "naive network-> train acc:0.19, test acc:0.145\n",
      "epoch: 2   weight_decay_applied ->, train acc:0.13, test acc:0.0995\n",
      "naive network-> train acc:0.23, test acc:0.1659\n",
      "epoch: 3   weight_decay_applied ->, train acc:0.15, test acc:0.1055\n",
      "naive network-> train acc:0.2633333333333333, test acc:0.1878\n",
      "epoch: 4   weight_decay_applied ->, train acc:0.17, test acc:0.1151\n",
      "naive network-> train acc:0.26666666666666666, test acc:0.2156\n",
      "epoch: 5   weight_decay_applied ->, train acc:0.17333333333333334, test acc:0.1194\n",
      "naive network-> train acc:0.2966666666666667, test acc:0.2356\n",
      "epoch: 6   weight_decay_applied ->, train acc:0.21666666666666667, test acc:0.1384\n",
      "naive network-> train acc:0.34, test acc:0.266\n",
      "epoch: 7   weight_decay_applied ->, train acc:0.23666666666666666, test acc:0.1531\n",
      "naive network-> train acc:0.36, test acc:0.2847\n",
      "epoch: 8   weight_decay_applied ->, train acc:0.29, test acc:0.1755\n",
      "naive network-> train acc:0.41, test acc:0.3152\n",
      "epoch: 9   weight_decay_applied ->, train acc:0.31, test acc:0.1912\n",
      "naive network-> train acc:0.4533333333333333, test acc:0.3407\n",
      "epoch: 10   weight_decay_applied ->, train acc:0.31333333333333335, test acc:0.2062\n",
      "naive network-> train acc:0.45, test acc:0.3544\n",
      "epoch: 11   weight_decay_applied ->, train acc:0.32666666666666666, test acc:0.2138\n",
      "naive network-> train acc:0.47, test acc:0.374\n",
      "epoch: 12   weight_decay_applied ->, train acc:0.35, test acc:0.2351\n",
      "naive network-> train acc:0.49, test acc:0.3957\n",
      "epoch: 13   weight_decay_applied ->, train acc:0.37333333333333335, test acc:0.2476\n",
      "naive network-> train acc:0.54, test acc:0.4254\n",
      "epoch: 14   weight_decay_applied ->, train acc:0.37666666666666665, test acc:0.2652\n",
      "naive network-> train acc:0.5533333333333333, test acc:0.4435\n",
      "epoch: 15   weight_decay_applied ->, train acc:0.3933333333333333, test acc:0.2782\n",
      "naive network-> train acc:0.58, test acc:0.4555\n",
      "epoch: 16   weight_decay_applied ->, train acc:0.41333333333333333, test acc:0.2991\n",
      "naive network-> train acc:0.63, test acc:0.4787\n",
      "epoch: 17   weight_decay_applied ->, train acc:0.43, test acc:0.3111\n",
      "naive network-> train acc:0.6533333333333333, test acc:0.5082\n",
      "epoch: 18   weight_decay_applied ->, train acc:0.43, test acc:0.3138\n",
      "naive network-> train acc:0.68, test acc:0.5235\n",
      "epoch: 19   weight_decay_applied ->, train acc:0.43666666666666665, test acc:0.3241\n",
      "naive network-> train acc:0.7033333333333334, test acc:0.5555\n",
      "epoch: 20   weight_decay_applied ->, train acc:0.46, test acc:0.3404\n",
      "naive network-> train acc:0.7233333333333334, test acc:0.567\n",
      "epoch: 21   weight_decay_applied ->, train acc:0.4633333333333333, test acc:0.3515\n",
      "naive network-> train acc:0.7466666666666667, test acc:0.577\n",
      "epoch: 22   weight_decay_applied ->, train acc:0.45666666666666667, test acc:0.3428\n",
      "naive network-> train acc:0.7866666666666666, test acc:0.6013\n",
      "epoch: 23   weight_decay_applied ->, train acc:0.4666666666666667, test acc:0.3547\n",
      "naive network-> train acc:0.78, test acc:0.606\n",
      "epoch: 24   weight_decay_applied ->, train acc:0.4866666666666667, test acc:0.3707\n",
      "naive network-> train acc:0.7933333333333333, test acc:0.6109\n",
      "epoch: 25   weight_decay_applied ->, train acc:0.49666666666666665, test acc:0.368\n",
      "naive network-> train acc:0.8066666666666666, test acc:0.6282\n",
      "epoch: 26   weight_decay_applied ->, train acc:0.5133333333333333, test acc:0.3839\n",
      "naive network-> train acc:0.7966666666666666, test acc:0.6326\n",
      "epoch: 27   weight_decay_applied ->, train acc:0.5233333333333333, test acc:0.3786\n",
      "naive network-> train acc:0.8333333333333334, test acc:0.6438\n",
      "epoch: 28   weight_decay_applied ->, train acc:0.53, test acc:0.3956\n",
      "naive network-> train acc:0.8266666666666667, test acc:0.6499\n",
      "epoch: 29   weight_decay_applied ->, train acc:0.5466666666666666, test acc:0.4006\n",
      "naive network-> train acc:0.83, test acc:0.6566\n",
      "epoch: 30   weight_decay_applied ->, train acc:0.5633333333333334, test acc:0.4068\n",
      "naive network-> train acc:0.84, test acc:0.6599\n",
      "epoch: 31   weight_decay_applied ->, train acc:0.5566666666666666, test acc:0.4043\n",
      "naive network-> train acc:0.84, test acc:0.6579\n",
      "epoch: 32   weight_decay_applied ->, train acc:0.5666666666666667, test acc:0.4036\n",
      "naive network-> train acc:0.8466666666666667, test acc:0.6666\n",
      "epoch: 33   weight_decay_applied ->, train acc:0.5866666666666667, test acc:0.4237\n",
      "naive network-> train acc:0.8633333333333333, test acc:0.6771\n",
      "epoch: 34   weight_decay_applied ->, train acc:0.5933333333333334, test acc:0.4357\n",
      "naive network-> train acc:0.8633333333333333, test acc:0.6907\n",
      "epoch: 35   weight_decay_applied ->, train acc:0.5933333333333334, test acc:0.4432\n",
      "naive network-> train acc:0.8733333333333333, test acc:0.6875\n",
      "epoch: 36   weight_decay_applied ->, train acc:0.5933333333333334, test acc:0.4502\n",
      "naive network-> train acc:0.87, test acc:0.6794\n",
      "epoch: 37   weight_decay_applied ->, train acc:0.59, test acc:0.4466\n",
      "naive network-> train acc:0.8666666666666667, test acc:0.6979\n",
      "epoch: 38   weight_decay_applied ->, train acc:0.5966666666666667, test acc:0.4652\n",
      "naive network-> train acc:0.8766666666666667, test acc:0.7037\n",
      "epoch: 39   weight_decay_applied ->, train acc:0.6033333333333334, test acc:0.4694\n",
      "naive network-> train acc:0.8733333333333333, test acc:0.6951\n",
      "epoch: 40   weight_decay_applied ->, train acc:0.6266666666666667, test acc:0.4851\n",
      "naive network-> train acc:0.8866666666666667, test acc:0.7124\n",
      "epoch: 41   weight_decay_applied ->, train acc:0.6333333333333333, test acc:0.4967\n",
      "naive network-> train acc:0.9, test acc:0.7093\n",
      "epoch: 42   weight_decay_applied ->, train acc:0.6366666666666667, test acc:0.5047\n",
      "naive network-> train acc:0.9, test acc:0.7122\n",
      "epoch: 43   weight_decay_applied ->, train acc:0.66, test acc:0.51\n",
      "naive network-> train acc:0.91, test acc:0.719\n",
      "epoch: 44   weight_decay_applied ->, train acc:0.6566666666666666, test acc:0.5124\n",
      "naive network-> train acc:0.92, test acc:0.7191\n",
      "epoch: 45   weight_decay_applied ->, train acc:0.6533333333333333, test acc:0.5189\n",
      "naive network-> train acc:0.9066666666666666, test acc:0.7173\n",
      "epoch: 46   weight_decay_applied ->, train acc:0.65, test acc:0.5054\n",
      "naive network-> train acc:0.91, test acc:0.7214\n",
      "epoch: 47   weight_decay_applied ->, train acc:0.6566666666666666, test acc:0.5162\n",
      "naive network-> train acc:0.9266666666666666, test acc:0.7281\n",
      "epoch: 48   weight_decay_applied ->, train acc:0.6733333333333333, test acc:0.5238\n",
      "naive network-> train acc:0.9133333333333333, test acc:0.7302\n",
      "epoch: 49   weight_decay_applied ->, train acc:0.6966666666666667, test acc:0.5418\n",
      "naive network-> train acc:0.93, test acc:0.7337\n",
      "epoch: 50   weight_decay_applied ->, train acc:0.7033333333333334, test acc:0.547\n",
      "naive network-> train acc:0.9333333333333333, test acc:0.7299\n",
      "epoch: 51   weight_decay_applied ->, train acc:0.7133333333333334, test acc:0.5557\n",
      "naive network-> train acc:0.94, test acc:0.7312\n",
      "epoch: 52   weight_decay_applied ->, train acc:0.7033333333333334, test acc:0.5563\n",
      "naive network-> train acc:0.94, test acc:0.7317\n",
      "epoch: 53   weight_decay_applied ->, train acc:0.7066666666666667, test acc:0.56\n",
      "naive network-> train acc:0.9433333333333334, test acc:0.7379\n",
      "epoch: 54   weight_decay_applied ->, train acc:0.73, test acc:0.5815\n",
      "naive network-> train acc:0.9433333333333334, test acc:0.7384\n",
      "epoch: 55   weight_decay_applied ->, train acc:0.7233333333333334, test acc:0.5787\n",
      "naive network-> train acc:0.9433333333333334, test acc:0.7375\n",
      "epoch: 56   weight_decay_applied ->, train acc:0.75, test acc:0.5988\n",
      "naive network-> train acc:0.9566666666666667, test acc:0.7449\n",
      "epoch: 57   weight_decay_applied ->, train acc:0.7566666666666667, test acc:0.6072\n",
      "naive network-> train acc:0.9533333333333334, test acc:0.7428\n",
      "epoch: 58   weight_decay_applied ->, train acc:0.7566666666666667, test acc:0.6107\n",
      "naive network-> train acc:0.9533333333333334, test acc:0.7435\n",
      "epoch: 59   weight_decay_applied ->, train acc:0.7533333333333333, test acc:0.6166\n",
      "naive network-> train acc:0.9566666666666667, test acc:0.7478\n",
      "epoch: 60   weight_decay_applied ->, train acc:0.7566666666666667, test acc:0.6074\n",
      "naive network-> train acc:0.9566666666666667, test acc:0.7437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 61   weight_decay_applied ->, train acc:0.7433333333333333, test acc:0.6141\n",
      "naive network-> train acc:0.96, test acc:0.7486\n",
      "epoch: 62   weight_decay_applied ->, train acc:0.7566666666666667, test acc:0.6168\n",
      "naive network-> train acc:0.96, test acc:0.7499\n",
      "epoch: 63   weight_decay_applied ->, train acc:0.76, test acc:0.6187\n",
      "naive network-> train acc:0.9633333333333334, test acc:0.7472\n",
      "epoch: 64   weight_decay_applied ->, train acc:0.7766666666666666, test acc:0.6299\n",
      "naive network-> train acc:0.97, test acc:0.7498\n",
      "epoch: 65   weight_decay_applied ->, train acc:0.7733333333333333, test acc:0.6319\n",
      "naive network-> train acc:0.97, test acc:0.7531\n",
      "epoch: 66   weight_decay_applied ->, train acc:0.7766666666666666, test acc:0.6407\n",
      "naive network-> train acc:0.9566666666666667, test acc:0.7518\n",
      "epoch: 67   weight_decay_applied ->, train acc:0.7733333333333333, test acc:0.6427\n",
      "naive network-> train acc:0.97, test acc:0.7536\n",
      "epoch: 68   weight_decay_applied ->, train acc:0.7733333333333333, test acc:0.6408\n",
      "naive network-> train acc:0.9733333333333334, test acc:0.752\n",
      "epoch: 69   weight_decay_applied ->, train acc:0.78, test acc:0.6397\n",
      "naive network-> train acc:0.97, test acc:0.75\n",
      "epoch: 70   weight_decay_applied ->, train acc:0.8, test acc:0.6448\n",
      "naive network-> train acc:0.9766666666666667, test acc:0.7554\n",
      "epoch: 71   weight_decay_applied ->, train acc:0.81, test acc:0.6476\n",
      "naive network-> train acc:0.97, test acc:0.7557\n",
      "epoch: 72   weight_decay_applied ->, train acc:0.7866666666666666, test acc:0.6411\n",
      "naive network-> train acc:0.9733333333333334, test acc:0.7546\n",
      "epoch: 73   weight_decay_applied ->, train acc:0.79, test acc:0.6511\n",
      "naive network-> train acc:0.9733333333333334, test acc:0.7615\n",
      "epoch: 74   weight_decay_applied ->, train acc:0.7933333333333333, test acc:0.6559\n",
      "naive network-> train acc:0.97, test acc:0.7592\n",
      "epoch: 75   weight_decay_applied ->, train acc:0.79, test acc:0.6519\n",
      "naive network-> train acc:0.9766666666666667, test acc:0.755\n",
      "epoch: 76   weight_decay_applied ->, train acc:0.8166666666666667, test acc:0.6558\n",
      "naive network-> train acc:0.9766666666666667, test acc:0.7594\n",
      "epoch: 77   weight_decay_applied ->, train acc:0.82, test acc:0.6585\n",
      "naive network-> train acc:0.9766666666666667, test acc:0.7559\n",
      "epoch: 78   weight_decay_applied ->, train acc:0.8033333333333333, test acc:0.6566\n",
      "naive network-> train acc:0.9833333333333333, test acc:0.7562\n",
      "epoch: 79   weight_decay_applied ->, train acc:0.8, test acc:0.6515\n",
      "naive network-> train acc:0.99, test acc:0.7624\n",
      "epoch: 80   weight_decay_applied ->, train acc:0.7966666666666666, test acc:0.6525\n",
      "naive network-> train acc:0.9866666666666667, test acc:0.7592\n",
      "epoch: 81   weight_decay_applied ->, train acc:0.8266666666666667, test acc:0.6695\n",
      "naive network-> train acc:0.99, test acc:0.762\n",
      "epoch: 82   weight_decay_applied ->, train acc:0.8266666666666667, test acc:0.6734\n",
      "naive network-> train acc:0.99, test acc:0.7618\n",
      "epoch: 83   weight_decay_applied ->, train acc:0.83, test acc:0.6811\n",
      "naive network-> train acc:0.99, test acc:0.7632\n",
      "epoch: 84   weight_decay_applied ->, train acc:0.8466666666666667, test acc:0.6829\n",
      "naive network-> train acc:0.9866666666666667, test acc:0.7603\n",
      "epoch: 85   weight_decay_applied ->, train acc:0.83, test acc:0.6762\n",
      "naive network-> train acc:0.99, test acc:0.7622\n",
      "epoch: 86   weight_decay_applied ->, train acc:0.8233333333333334, test acc:0.6685\n",
      "naive network-> train acc:0.99, test acc:0.7613\n",
      "epoch: 87   weight_decay_applied ->, train acc:0.8333333333333334, test acc:0.6803\n",
      "naive network-> train acc:0.9833333333333333, test acc:0.7614\n",
      "epoch: 88   weight_decay_applied ->, train acc:0.84, test acc:0.6838\n",
      "naive network-> train acc:0.9866666666666667, test acc:0.7625\n",
      "epoch: 89   weight_decay_applied ->, train acc:0.86, test acc:0.6963\n",
      "naive network-> train acc:0.99, test acc:0.7631\n",
      "epoch: 90   weight_decay_applied ->, train acc:0.8533333333333334, test acc:0.696\n",
      "naive network-> train acc:0.99, test acc:0.763\n",
      "epoch: 91   weight_decay_applied ->, train acc:0.8466666666666667, test acc:0.6915\n",
      "naive network-> train acc:0.99, test acc:0.7614\n",
      "epoch: 92   weight_decay_applied ->, train acc:0.8366666666666667, test acc:0.6787\n",
      "naive network-> train acc:0.9933333333333333, test acc:0.762\n",
      "epoch: 93   weight_decay_applied ->, train acc:0.8366666666666667, test acc:0.688\n",
      "naive network-> train acc:0.9933333333333333, test acc:0.7648\n",
      "epoch: 94   weight_decay_applied ->, train acc:0.8533333333333334, test acc:0.6873\n",
      "naive network-> train acc:0.99, test acc:0.7654\n",
      "epoch: 95   weight_decay_applied ->, train acc:0.83, test acc:0.681\n",
      "naive network-> train acc:0.99, test acc:0.7601\n",
      "epoch: 96   weight_decay_applied ->, train acc:0.8433333333333334, test acc:0.6939\n",
      "naive network-> train acc:0.9933333333333333, test acc:0.7644\n",
      "epoch: 97   weight_decay_applied ->, train acc:0.8433333333333334, test acc:0.6984\n",
      "naive network-> train acc:0.9966666666666667, test acc:0.7648\n",
      "epoch: 98   weight_decay_applied ->, train acc:0.86, test acc:0.7021\n",
      "naive network-> train acc:0.9966666666666667, test acc:0.7636\n",
      "epoch: 99   weight_decay_applied ->, train acc:0.8566666666666667, test acc:0.6948\n",
      "naive network-> train acc:0.9966666666666667, test acc:0.7646\n",
      "epoch: 100   weight_decay_applied ->, train acc:0.8666666666666667, test acc:0.7093\n",
      "naive network-> train acc:0.9966666666666667, test acc:0.765\n",
      "epoch: 101   weight_decay_applied ->, train acc:0.8533333333333334, test acc:0.7046\n",
      "naive network-> train acc:1.0, test acc:0.7665\n",
      "epoch: 102   weight_decay_applied ->, train acc:0.8633333333333333, test acc:0.7153\n",
      "naive network-> train acc:1.0, test acc:0.7659\n",
      "epoch: 103   weight_decay_applied ->, train acc:0.86, test acc:0.7106\n",
      "naive network-> train acc:1.0, test acc:0.767\n",
      "epoch: 104   weight_decay_applied ->, train acc:0.8666666666666667, test acc:0.7191\n",
      "naive network-> train acc:0.9966666666666667, test acc:0.7674\n",
      "epoch: 105   weight_decay_applied ->, train acc:0.8633333333333333, test acc:0.7111\n",
      "naive network-> train acc:1.0, test acc:0.7669\n",
      "epoch: 106   weight_decay_applied ->, train acc:0.8533333333333334, test acc:0.7126\n",
      "naive network-> train acc:1.0, test acc:0.7685\n",
      "epoch: 107   weight_decay_applied ->, train acc:0.8733333333333333, test acc:0.7156\n",
      "naive network-> train acc:1.0, test acc:0.7674\n",
      "epoch: 108   weight_decay_applied ->, train acc:0.8633333333333333, test acc:0.7043\n",
      "naive network-> train acc:1.0, test acc:0.766\n",
      "epoch: 109   weight_decay_applied ->, train acc:0.8466666666666667, test acc:0.6955\n",
      "naive network-> train acc:1.0, test acc:0.7682\n",
      "epoch: 110   weight_decay_applied ->, train acc:0.8633333333333333, test acc:0.7074\n",
      "naive network-> train acc:1.0, test acc:0.7678\n",
      "epoch: 111   weight_decay_applied ->, train acc:0.85, test acc:0.7041\n",
      "naive network-> train acc:1.0, test acc:0.7669\n",
      "epoch: 112   weight_decay_applied ->, train acc:0.8566666666666667, test acc:0.7088\n",
      "naive network-> train acc:1.0, test acc:0.7683\n",
      "epoch: 113   weight_decay_applied ->, train acc:0.87, test acc:0.7112\n",
      "naive network-> train acc:1.0, test acc:0.7677\n",
      "epoch: 114   weight_decay_applied ->, train acc:0.8666666666666667, test acc:0.7175\n",
      "naive network-> train acc:1.0, test acc:0.7695\n",
      "epoch: 115   weight_decay_applied ->, train acc:0.85, test acc:0.708\n",
      "naive network-> train acc:1.0, test acc:0.7698\n",
      "epoch: 116   weight_decay_applied ->, train acc:0.86, test acc:0.7062\n",
      "naive network-> train acc:1.0, test acc:0.7686\n",
      "epoch: 117   weight_decay_applied ->, train acc:0.87, test acc:0.7204\n",
      "naive network-> train acc:1.0, test acc:0.7687\n",
      "epoch: 118   weight_decay_applied ->, train acc:0.87, test acc:0.72\n",
      "naive network-> train acc:1.0, test acc:0.7685\n",
      "epoch: 119   weight_decay_applied ->, train acc:0.8666666666666667, test acc:0.7174\n",
      "naive network-> train acc:1.0, test acc:0.7682\n",
      "epoch: 120   weight_decay_applied ->, train acc:0.86, test acc:0.7132\n",
      "naive network-> train acc:1.0, test acc:0.769\n",
      "epoch: 121   weight_decay_applied ->, train acc:0.8766666666666667, test acc:0.7147\n",
      "naive network-> train acc:1.0, test acc:0.769\n",
      "epoch: 122   weight_decay_applied ->, train acc:0.8766666666666667, test acc:0.7195\n",
      "naive network-> train acc:1.0, test acc:0.7682\n",
      "epoch: 123   weight_decay_applied ->, train acc:0.87, test acc:0.723\n",
      "naive network-> train acc:1.0, test acc:0.7707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 124   weight_decay_applied ->, train acc:0.8666666666666667, test acc:0.723\n",
      "naive network-> train acc:1.0, test acc:0.7663\n",
      "epoch: 125   weight_decay_applied ->, train acc:0.8633333333333333, test acc:0.7179\n",
      "naive network-> train acc:1.0, test acc:0.7685\n",
      "epoch: 126   weight_decay_applied ->, train acc:0.8466666666666667, test acc:0.7045\n",
      "naive network-> train acc:1.0, test acc:0.7684\n",
      "epoch: 127   weight_decay_applied ->, train acc:0.86, test acc:0.7036\n",
      "naive network-> train acc:1.0, test acc:0.7684\n",
      "epoch: 128   weight_decay_applied ->, train acc:0.8633333333333333, test acc:0.7111\n",
      "naive network-> train acc:1.0, test acc:0.769\n",
      "epoch: 129   weight_decay_applied ->, train acc:0.8666666666666667, test acc:0.7153\n",
      "naive network-> train acc:1.0, test acc:0.7693\n",
      "epoch: 130   weight_decay_applied ->, train acc:0.8766666666666667, test acc:0.7058\n",
      "naive network-> train acc:1.0, test acc:0.7685\n",
      "epoch: 131   weight_decay_applied ->, train acc:0.8733333333333333, test acc:0.7078\n",
      "naive network-> train acc:1.0, test acc:0.7692\n",
      "epoch: 132   weight_decay_applied ->, train acc:0.87, test acc:0.7063\n",
      "naive network-> train acc:1.0, test acc:0.7703\n",
      "epoch: 133   weight_decay_applied ->, train acc:0.8666666666666667, test acc:0.7175\n",
      "naive network-> train acc:1.0, test acc:0.7705\n",
      "epoch: 134   weight_decay_applied ->, train acc:0.88, test acc:0.7243\n",
      "naive network-> train acc:1.0, test acc:0.7709\n",
      "epoch: 135   weight_decay_applied ->, train acc:0.8766666666666667, test acc:0.7198\n",
      "naive network-> train acc:1.0, test acc:0.7711\n",
      "epoch: 136   weight_decay_applied ->, train acc:0.8766666666666667, test acc:0.7241\n",
      "naive network-> train acc:1.0, test acc:0.7721\n",
      "epoch: 137   weight_decay_applied ->, train acc:0.8766666666666667, test acc:0.7271\n",
      "naive network-> train acc:1.0, test acc:0.772\n",
      "epoch: 138   weight_decay_applied ->, train acc:0.88, test acc:0.7249\n",
      "naive network-> train acc:1.0, test acc:0.7708\n",
      "epoch: 139   weight_decay_applied ->, train acc:0.87, test acc:0.725\n",
      "naive network-> train acc:1.0, test acc:0.7715\n",
      "epoch: 140   weight_decay_applied ->, train acc:0.87, test acc:0.725\n",
      "naive network-> train acc:1.0, test acc:0.7712\n",
      "epoch: 141   weight_decay_applied ->, train acc:0.88, test acc:0.7342\n",
      "naive network-> train acc:1.0, test acc:0.7716\n",
      "epoch: 142   weight_decay_applied ->, train acc:0.8766666666666667, test acc:0.7251\n",
      "naive network-> train acc:1.0, test acc:0.7732\n",
      "epoch: 143   weight_decay_applied ->, train acc:0.8833333333333333, test acc:0.7235\n",
      "naive network-> train acc:1.0, test acc:0.7722\n",
      "epoch: 144   weight_decay_applied ->, train acc:0.8766666666666667, test acc:0.7257\n",
      "naive network-> train acc:1.0, test acc:0.772\n",
      "epoch: 145   weight_decay_applied ->, train acc:0.8833333333333333, test acc:0.7173\n",
      "naive network-> train acc:1.0, test acc:0.7725\n",
      "epoch: 146   weight_decay_applied ->, train acc:0.88, test acc:0.7192\n",
      "naive network-> train acc:1.0, test acc:0.7712\n",
      "epoch: 147   weight_decay_applied ->, train acc:0.8833333333333333, test acc:0.7267\n",
      "naive network-> train acc:1.0, test acc:0.7717\n",
      "epoch: 148   weight_decay_applied ->, train acc:0.8866666666666667, test acc:0.7306\n",
      "naive network-> train acc:1.0, test acc:0.7716\n",
      "epoch: 149   weight_decay_applied ->, train acc:0.8766666666666667, test acc:0.7218\n",
      "naive network-> train acc:1.0, test acc:0.7718\n",
      "epoch: 150   weight_decay_applied ->, train acc:0.8866666666666667, test acc:0.7272\n",
      "naive network-> train acc:1.0, test acc:0.7738\n",
      "epoch: 151   weight_decay_applied ->, train acc:0.8766666666666667, test acc:0.727\n",
      "naive network-> train acc:1.0, test acc:0.7734\n",
      "epoch: 152   weight_decay_applied ->, train acc:0.8766666666666667, test acc:0.7311\n",
      "naive network-> train acc:1.0, test acc:0.7734\n",
      "epoch: 153   weight_decay_applied ->, train acc:0.87, test acc:0.7269\n",
      "naive network-> train acc:1.0, test acc:0.7729\n",
      "epoch: 154   weight_decay_applied ->, train acc:0.88, test acc:0.7172\n",
      "naive network-> train acc:1.0, test acc:0.7715\n",
      "epoch: 155   weight_decay_applied ->, train acc:0.88, test acc:0.7176\n",
      "naive network-> train acc:1.0, test acc:0.7718\n",
      "epoch: 156   weight_decay_applied ->, train acc:0.8866666666666667, test acc:0.7223\n",
      "naive network-> train acc:1.0, test acc:0.7728\n",
      "epoch: 157   weight_decay_applied ->, train acc:0.89, test acc:0.7303\n",
      "naive network-> train acc:1.0, test acc:0.7727\n",
      "epoch: 158   weight_decay_applied ->, train acc:0.88, test acc:0.7227\n",
      "naive network-> train acc:1.0, test acc:0.7724\n",
      "epoch: 159   weight_decay_applied ->, train acc:0.8766666666666667, test acc:0.7239\n",
      "naive network-> train acc:1.0, test acc:0.7721\n",
      "epoch: 160   weight_decay_applied ->, train acc:0.8833333333333333, test acc:0.7331\n",
      "naive network-> train acc:1.0, test acc:0.7738\n",
      "epoch: 161   weight_decay_applied ->, train acc:0.8933333333333333, test acc:0.737\n",
      "naive network-> train acc:1.0, test acc:0.7745\n",
      "epoch: 162   weight_decay_applied ->, train acc:0.9, test acc:0.7374\n",
      "naive network-> train acc:1.0, test acc:0.7748\n",
      "epoch: 163   weight_decay_applied ->, train acc:0.8866666666666667, test acc:0.7359\n",
      "naive network-> train acc:1.0, test acc:0.7745\n",
      "epoch: 164   weight_decay_applied ->, train acc:0.8866666666666667, test acc:0.7249\n",
      "naive network-> train acc:1.0, test acc:0.7738\n",
      "epoch: 165   weight_decay_applied ->, train acc:0.89, test acc:0.7239\n",
      "naive network-> train acc:1.0, test acc:0.7733\n",
      "epoch: 166   weight_decay_applied ->, train acc:0.8933333333333333, test acc:0.7367\n",
      "naive network-> train acc:1.0, test acc:0.7735\n",
      "epoch: 167   weight_decay_applied ->, train acc:0.8833333333333333, test acc:0.7269\n",
      "naive network-> train acc:1.0, test acc:0.7728\n",
      "epoch: 168   weight_decay_applied ->, train acc:0.8766666666666667, test acc:0.7251\n",
      "naive network-> train acc:1.0, test acc:0.7744\n",
      "epoch: 169   weight_decay_applied ->, train acc:0.8866666666666667, test acc:0.7341\n",
      "naive network-> train acc:1.0, test acc:0.7739\n",
      "epoch: 170   weight_decay_applied ->, train acc:0.88, test acc:0.7226\n",
      "naive network-> train acc:1.0, test acc:0.7734\n",
      "epoch: 171   weight_decay_applied ->, train acc:0.88, test acc:0.7221\n",
      "naive network-> train acc:1.0, test acc:0.7738\n",
      "epoch: 172   weight_decay_applied ->, train acc:0.8833333333333333, test acc:0.731\n",
      "naive network-> train acc:1.0, test acc:0.7745\n",
      "epoch: 173   weight_decay_applied ->, train acc:0.89, test acc:0.7229\n",
      "naive network-> train acc:1.0, test acc:0.7741\n",
      "epoch: 174   weight_decay_applied ->, train acc:0.8833333333333333, test acc:0.7222\n",
      "naive network-> train acc:1.0, test acc:0.7741\n",
      "epoch: 175   weight_decay_applied ->, train acc:0.88, test acc:0.7232\n",
      "naive network-> train acc:1.0, test acc:0.7742\n",
      "epoch: 176   weight_decay_applied ->, train acc:0.8933333333333333, test acc:0.733\n",
      "naive network-> train acc:1.0, test acc:0.7744\n",
      "epoch: 177   weight_decay_applied ->, train acc:0.8933333333333333, test acc:0.729\n",
      "naive network-> train acc:1.0, test acc:0.7752\n",
      "epoch: 178   weight_decay_applied ->, train acc:0.89, test acc:0.7288\n",
      "naive network-> train acc:1.0, test acc:0.7737\n",
      "epoch: 179   weight_decay_applied ->, train acc:0.8866666666666667, test acc:0.726\n",
      "naive network-> train acc:1.0, test acc:0.7748\n",
      "epoch: 180   weight_decay_applied ->, train acc:0.8933333333333333, test acc:0.736\n",
      "naive network-> train acc:1.0, test acc:0.7745\n",
      "epoch: 181   weight_decay_applied ->, train acc:0.8833333333333333, test acc:0.7308\n",
      "naive network-> train acc:1.0, test acc:0.7735\n",
      "epoch: 182   weight_decay_applied ->, train acc:0.8866666666666667, test acc:0.7262\n",
      "naive network-> train acc:1.0, test acc:0.7749\n",
      "epoch: 183   weight_decay_applied ->, train acc:0.89, test acc:0.7342\n",
      "naive network-> train acc:1.0, test acc:0.7759\n",
      "epoch: 184   weight_decay_applied ->, train acc:0.8966666666666666, test acc:0.7331\n",
      "naive network-> train acc:1.0, test acc:0.7759\n",
      "epoch: 185   weight_decay_applied ->, train acc:0.8966666666666666, test acc:0.7319\n",
      "naive network-> train acc:1.0, test acc:0.7754\n",
      "epoch: 186   weight_decay_applied ->, train acc:0.9, test acc:0.7266\n",
      "naive network-> train acc:1.0, test acc:0.7744\n",
      "epoch: 187   weight_decay_applied ->, train acc:0.89, test acc:0.7287\n",
      "naive network-> train acc:1.0, test acc:0.7767\n",
      "epoch: 188   weight_decay_applied ->, train acc:0.8933333333333333, test acc:0.7222\n",
      "naive network-> train acc:1.0, test acc:0.776\n",
      "epoch: 189   weight_decay_applied ->, train acc:0.8933333333333333, test acc:0.7229\n",
      "naive network-> train acc:1.0, test acc:0.7754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 190   weight_decay_applied ->, train acc:0.8933333333333333, test acc:0.7305\n",
      "naive network-> train acc:1.0, test acc:0.7753\n",
      "epoch: 191   weight_decay_applied ->, train acc:0.8933333333333333, test acc:0.7395\n",
      "naive network-> train acc:1.0, test acc:0.7758\n",
      "epoch: 192   weight_decay_applied ->, train acc:0.8933333333333333, test acc:0.7396\n",
      "naive network-> train acc:1.0, test acc:0.7755\n",
      "epoch: 193   weight_decay_applied ->, train acc:0.89, test acc:0.7324\n",
      "naive network-> train acc:1.0, test acc:0.775\n",
      "epoch: 194   weight_decay_applied ->, train acc:0.8866666666666667, test acc:0.7248\n",
      "naive network-> train acc:1.0, test acc:0.776\n",
      "epoch: 195   weight_decay_applied ->, train acc:0.8866666666666667, test acc:0.7246\n",
      "naive network-> train acc:1.0, test acc:0.7759\n",
      "epoch: 196   weight_decay_applied ->, train acc:0.8966666666666666, test acc:0.7216\n",
      "naive network-> train acc:1.0, test acc:0.7757\n",
      "epoch: 197   weight_decay_applied ->, train acc:0.8833333333333333, test acc:0.7275\n",
      "naive network-> train acc:1.0, test acc:0.7759\n",
      "epoch: 198   weight_decay_applied ->, train acc:0.8933333333333333, test acc:0.7285\n",
      "naive network-> train acc:1.0, test acc:0.7755\n",
      "epoch: 199   weight_decay_applied ->, train acc:0.8866666666666667, test acc:0.7271\n",
      "naive network-> train acc:1.0, test acc:0.7757\n",
      "epoch: 200   weight_decay_applied ->, train acc:0.8866666666666667, test acc:0.7207\n",
      "naive network-> train acc:1.0, test acc:0.7758\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = load_mnist(normalize=True)\n",
    "\n",
    "x_train = x_train[:300]\n",
    "y_train = y_train[:300]\n",
    "\n",
    "\n",
    "weight_decay_lambda = 0.1\n",
    "\n",
    "wieght_decay_network = neural_network(input_size=28*28, \n",
    "                                      hidden_size=[100, 100, 100, 100, 100, 100], \n",
    "                                      output_size=10,\n",
    "                                      weight_decay_lambda=weight_decay_lambda)\n",
    "simple_network = neural_network(input_size=28*28, \n",
    "                                      hidden_size=[100, 100, 100, 100, 100, 100], \n",
    "                                      output_size=10)\n",
    "\n",
    "optimizer = SGD(lr=0.01)\n",
    "\n",
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "w_train_acc_list = []\n",
    "w_test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch_cnt = 0\n",
    "\n",
    "for i in range(1000000000):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "    \n",
    "    for nn in (wieght_decay_network, simple_network):\n",
    "        grads = nn.gradient(x_batch, y_batch)\n",
    "        optimizer.update(nn.params, grads)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        w_train_acc = wieght_decay_network.accuracy(x_train, y_train)\n",
    "        w_test_acc = wieght_decay_network.accuracy(x_test, y_test)\n",
    "        w_train_acc_list.append(w_train_acc)\n",
    "        w_test_acc_list.append(w_test_acc)\n",
    "        \n",
    "        train_acc = simple_network.accuracy(x_train, y_train)\n",
    "        test_acc = simple_network.accuracy(x_test, y_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(\"epoch: \" + str(epoch_cnt) +'   weight_decay_applied ->' + \n",
    "              \", train acc:\" + str(w_train_acc) + \", test acc:\" + str(w_test_acc)\n",
    "              +'\\n'+ \"naive network-> train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) )\n",
    "\n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break\n",
    "\n",
    "\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, w_train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, w_test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting - Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0   weight_decay_applied ->, train acc:0.1, test acc:0.1037\n",
      "naive network-> train acc:0.09333333333333334, test acc:0.1034\n",
      "epoch: 1   weight_decay_applied ->, train acc:0.11333333333333333, test acc:0.1056\n",
      "naive network-> train acc:0.12666666666666668, test acc:0.1158\n",
      "epoch: 2   weight_decay_applied ->, train acc:0.11666666666666667, test acc:0.1094\n",
      "naive network-> train acc:0.14333333333333334, test acc:0.1301\n",
      "epoch: 3   weight_decay_applied ->, train acc:0.11666666666666667, test acc:0.1119\n",
      "naive network-> train acc:0.15, test acc:0.1379\n",
      "epoch: 4   weight_decay_applied ->, train acc:0.12, test acc:0.1116\n",
      "naive network-> train acc:0.16666666666666666, test acc:0.1512\n",
      "epoch: 5   weight_decay_applied ->, train acc:0.12, test acc:0.1114\n",
      "naive network-> train acc:0.19333333333333333, test acc:0.1595\n",
      "epoch: 6   weight_decay_applied ->, train acc:0.12666666666666668, test acc:0.1152\n",
      "naive network-> train acc:0.20333333333333334, test acc:0.1666\n",
      "epoch: 7   weight_decay_applied ->, train acc:0.12333333333333334, test acc:0.1157\n",
      "naive network-> train acc:0.21333333333333335, test acc:0.1748\n",
      "epoch: 8   weight_decay_applied ->, train acc:0.12666666666666668, test acc:0.1192\n",
      "naive network-> train acc:0.21666666666666667, test acc:0.1766\n",
      "epoch: 9   weight_decay_applied ->, train acc:0.13666666666666666, test acc:0.1214\n",
      "naive network-> train acc:0.22, test acc:0.183\n",
      "epoch: 10   weight_decay_applied ->, train acc:0.14, test acc:0.1238\n",
      "naive network-> train acc:0.24333333333333335, test acc:0.1921\n",
      "epoch: 11   weight_decay_applied ->, train acc:0.15, test acc:0.1265\n",
      "naive network-> train acc:0.24666666666666667, test acc:0.1929\n",
      "epoch: 12   weight_decay_applied ->, train acc:0.14333333333333334, test acc:0.1314\n",
      "naive network-> train acc:0.26, test acc:0.2\n",
      "epoch: 13   weight_decay_applied ->, train acc:0.15333333333333332, test acc:0.1412\n",
      "naive network-> train acc:0.30666666666666664, test acc:0.2121\n",
      "epoch: 14   weight_decay_applied ->, train acc:0.15333333333333332, test acc:0.1475\n",
      "naive network-> train acc:0.3333333333333333, test acc:0.224\n",
      "epoch: 15   weight_decay_applied ->, train acc:0.15666666666666668, test acc:0.1536\n",
      "naive network-> train acc:0.34, test acc:0.2398\n",
      "epoch: 16   weight_decay_applied ->, train acc:0.16333333333333333, test acc:0.1565\n",
      "naive network-> train acc:0.36, test acc:0.2516\n",
      "epoch: 17   weight_decay_applied ->, train acc:0.16333333333333333, test acc:0.1644\n",
      "naive network-> train acc:0.37666666666666665, test acc:0.262\n",
      "epoch: 18   weight_decay_applied ->, train acc:0.17, test acc:0.1697\n",
      "naive network-> train acc:0.3933333333333333, test acc:0.2807\n",
      "epoch: 19   weight_decay_applied ->, train acc:0.2, test acc:0.1762\n",
      "naive network-> train acc:0.42, test acc:0.2886\n",
      "epoch: 20   weight_decay_applied ->, train acc:0.21, test acc:0.1845\n",
      "naive network-> train acc:0.45666666666666667, test acc:0.3196\n",
      "epoch: 21   weight_decay_applied ->, train acc:0.21666666666666667, test acc:0.188\n",
      "naive network-> train acc:0.47333333333333333, test acc:0.3226\n",
      "epoch: 22   weight_decay_applied ->, train acc:0.22, test acc:0.1927\n",
      "naive network-> train acc:0.49666666666666665, test acc:0.33\n",
      "epoch: 23   weight_decay_applied ->, train acc:0.23666666666666666, test acc:0.2015\n",
      "naive network-> train acc:0.5066666666666667, test acc:0.343\n",
      "epoch: 24   weight_decay_applied ->, train acc:0.25, test acc:0.2109\n",
      "naive network-> train acc:0.54, test acc:0.3626\n",
      "epoch: 25   weight_decay_applied ->, train acc:0.27666666666666667, test acc:0.2203\n",
      "naive network-> train acc:0.5566666666666666, test acc:0.3754\n",
      "epoch: 26   weight_decay_applied ->, train acc:0.2833333333333333, test acc:0.2243\n",
      "naive network-> train acc:0.5666666666666667, test acc:0.392\n",
      "epoch: 27   weight_decay_applied ->, train acc:0.2866666666666667, test acc:0.2293\n",
      "naive network-> train acc:0.6066666666666667, test acc:0.4127\n",
      "epoch: 28   weight_decay_applied ->, train acc:0.29, test acc:0.2349\n",
      "naive network-> train acc:0.5866666666666667, test acc:0.4201\n",
      "epoch: 29   weight_decay_applied ->, train acc:0.29333333333333333, test acc:0.2369\n",
      "naive network-> train acc:0.6266666666666667, test acc:0.4308\n",
      "epoch: 30   weight_decay_applied ->, train acc:0.3233333333333333, test acc:0.2517\n",
      "naive network-> train acc:0.6533333333333333, test acc:0.4615\n",
      "epoch: 31   weight_decay_applied ->, train acc:0.32666666666666666, test acc:0.2576\n",
      "naive network-> train acc:0.6933333333333334, test acc:0.4743\n",
      "epoch: 32   weight_decay_applied ->, train acc:0.30666666666666664, test acc:0.2577\n",
      "naive network-> train acc:0.6933333333333334, test acc:0.4812\n",
      "epoch: 33   weight_decay_applied ->, train acc:0.33, test acc:0.2654\n",
      "naive network-> train acc:0.6966666666666667, test acc:0.497\n",
      "epoch: 34   weight_decay_applied ->, train acc:0.32666666666666666, test acc:0.2735\n",
      "naive network-> train acc:0.72, test acc:0.5128\n",
      "epoch: 35   weight_decay_applied ->, train acc:0.33666666666666667, test acc:0.2809\n",
      "naive network-> train acc:0.7333333333333333, test acc:0.5255\n",
      "epoch: 36   weight_decay_applied ->, train acc:0.35, test acc:0.2823\n",
      "naive network-> train acc:0.7366666666666667, test acc:0.5341\n",
      "epoch: 37   weight_decay_applied ->, train acc:0.35, test acc:0.2902\n",
      "naive network-> train acc:0.7533333333333333, test acc:0.553\n",
      "epoch: 38   weight_decay_applied ->, train acc:0.35, test acc:0.2892\n",
      "naive network-> train acc:0.7566666666666667, test acc:0.554\n",
      "epoch: 39   weight_decay_applied ->, train acc:0.35, test acc:0.2868\n",
      "naive network-> train acc:0.7433333333333333, test acc:0.5495\n",
      "epoch: 40   weight_decay_applied ->, train acc:0.36666666666666664, test acc:0.2938\n",
      "naive network-> train acc:0.7566666666666667, test acc:0.5579\n",
      "epoch: 41   weight_decay_applied ->, train acc:0.38333333333333336, test acc:0.298\n",
      "naive network-> train acc:0.7666666666666667, test acc:0.569\n",
      "epoch: 42   weight_decay_applied ->, train acc:0.39, test acc:0.3017\n",
      "naive network-> train acc:0.7933333333333333, test acc:0.5804\n",
      "epoch: 43   weight_decay_applied ->, train acc:0.39666666666666667, test acc:0.3039\n",
      "naive network-> train acc:0.8, test acc:0.5926\n",
      "epoch: 44   weight_decay_applied ->, train acc:0.3933333333333333, test acc:0.3062\n",
      "naive network-> train acc:0.7933333333333333, test acc:0.5954\n",
      "epoch: 45   weight_decay_applied ->, train acc:0.39666666666666667, test acc:0.3097\n",
      "naive network-> train acc:0.8033333333333333, test acc:0.6044\n",
      "epoch: 46   weight_decay_applied ->, train acc:0.4033333333333333, test acc:0.3099\n",
      "naive network-> train acc:0.82, test acc:0.6057\n",
      "epoch: 47   weight_decay_applied ->, train acc:0.4, test acc:0.3105\n",
      "naive network-> train acc:0.8166666666666667, test acc:0.6068\n",
      "epoch: 48   weight_decay_applied ->, train acc:0.4, test acc:0.3158\n",
      "naive network-> train acc:0.8433333333333334, test acc:0.6044\n",
      "epoch: 49   weight_decay_applied ->, train acc:0.4, test acc:0.3172\n",
      "naive network-> train acc:0.8266666666666667, test acc:0.6065\n",
      "epoch: 50   weight_decay_applied ->, train acc:0.4, test acc:0.3085\n",
      "naive network-> train acc:0.8366666666666667, test acc:0.6292\n",
      "epoch: 51   weight_decay_applied ->, train acc:0.39666666666666667, test acc:0.3065\n",
      "naive network-> train acc:0.84, test acc:0.6312\n",
      "epoch: 52   weight_decay_applied ->, train acc:0.39, test acc:0.3074\n",
      "naive network-> train acc:0.8366666666666667, test acc:0.6316\n",
      "epoch: 53   weight_decay_applied ->, train acc:0.3933333333333333, test acc:0.3065\n",
      "naive network-> train acc:0.8666666666666667, test acc:0.6346\n",
      "epoch: 54   weight_decay_applied ->, train acc:0.39666666666666667, test acc:0.3093\n",
      "naive network-> train acc:0.8666666666666667, test acc:0.6309\n",
      "epoch: 55   weight_decay_applied ->, train acc:0.4, test acc:0.3111\n",
      "naive network-> train acc:0.87, test acc:0.6383\n",
      "epoch: 56   weight_decay_applied ->, train acc:0.4066666666666667, test acc:0.314\n",
      "naive network-> train acc:0.8733333333333333, test acc:0.6363\n",
      "epoch: 57   weight_decay_applied ->, train acc:0.4066666666666667, test acc:0.309\n",
      "naive network-> train acc:0.8766666666666667, test acc:0.6516\n",
      "epoch: 58   weight_decay_applied ->, train acc:0.4066666666666667, test acc:0.3103\n",
      "naive network-> train acc:0.8833333333333333, test acc:0.637\n",
      "epoch: 59   weight_decay_applied ->, train acc:0.41, test acc:0.3083\n",
      "naive network-> train acc:0.88, test acc:0.6664\n",
      "epoch: 60   weight_decay_applied ->, train acc:0.41333333333333333, test acc:0.3111\n",
      "naive network-> train acc:0.89, test acc:0.6631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 61   weight_decay_applied ->, train acc:0.42, test acc:0.3107\n",
      "naive network-> train acc:0.8866666666666667, test acc:0.6691\n",
      "epoch: 62   weight_decay_applied ->, train acc:0.4166666666666667, test acc:0.3126\n",
      "naive network-> train acc:0.8966666666666666, test acc:0.6718\n",
      "epoch: 63   weight_decay_applied ->, train acc:0.42, test acc:0.3159\n",
      "naive network-> train acc:0.8966666666666666, test acc:0.6564\n",
      "epoch: 64   weight_decay_applied ->, train acc:0.4166666666666667, test acc:0.32\n",
      "naive network-> train acc:0.8966666666666666, test acc:0.6706\n",
      "epoch: 65   weight_decay_applied ->, train acc:0.41333333333333333, test acc:0.3172\n",
      "naive network-> train acc:0.9033333333333333, test acc:0.6763\n",
      "epoch: 66   weight_decay_applied ->, train acc:0.4166666666666667, test acc:0.3217\n",
      "naive network-> train acc:0.91, test acc:0.6713\n",
      "epoch: 67   weight_decay_applied ->, train acc:0.41, test acc:0.3157\n",
      "naive network-> train acc:0.9166666666666666, test acc:0.6686\n",
      "epoch: 68   weight_decay_applied ->, train acc:0.4066666666666667, test acc:0.317\n",
      "naive network-> train acc:0.9133333333333333, test acc:0.6779\n",
      "epoch: 69   weight_decay_applied ->, train acc:0.4066666666666667, test acc:0.3174\n",
      "naive network-> train acc:0.9166666666666666, test acc:0.6777\n",
      "epoch: 70   weight_decay_applied ->, train acc:0.4166666666666667, test acc:0.3245\n",
      "naive network-> train acc:0.9333333333333333, test acc:0.6876\n",
      "epoch: 71   weight_decay_applied ->, train acc:0.4266666666666667, test acc:0.3239\n",
      "naive network-> train acc:0.9233333333333333, test acc:0.6791\n",
      "epoch: 72   weight_decay_applied ->, train acc:0.42333333333333334, test acc:0.3228\n",
      "naive network-> train acc:0.9366666666666666, test acc:0.6884\n",
      "epoch: 73   weight_decay_applied ->, train acc:0.43333333333333335, test acc:0.3326\n",
      "naive network-> train acc:0.9266666666666666, test acc:0.6896\n",
      "epoch: 74   weight_decay_applied ->, train acc:0.43, test acc:0.3365\n",
      "naive network-> train acc:0.9533333333333334, test acc:0.6975\n",
      "epoch: 75   weight_decay_applied ->, train acc:0.44, test acc:0.3396\n",
      "naive network-> train acc:0.9466666666666667, test acc:0.6939\n",
      "epoch: 76   weight_decay_applied ->, train acc:0.44, test acc:0.3429\n",
      "naive network-> train acc:0.9433333333333334, test acc:0.684\n",
      "epoch: 77   weight_decay_applied ->, train acc:0.44, test acc:0.3413\n",
      "naive network-> train acc:0.95, test acc:0.6978\n",
      "epoch: 78   weight_decay_applied ->, train acc:0.44333333333333336, test acc:0.3451\n",
      "naive network-> train acc:0.9533333333333334, test acc:0.6969\n",
      "epoch: 79   weight_decay_applied ->, train acc:0.43666666666666665, test acc:0.3422\n",
      "naive network-> train acc:0.95, test acc:0.7039\n",
      "epoch: 80   weight_decay_applied ->, train acc:0.44, test acc:0.3447\n",
      "naive network-> train acc:0.9533333333333334, test acc:0.7027\n",
      "epoch: 81   weight_decay_applied ->, train acc:0.43666666666666665, test acc:0.3434\n",
      "naive network-> train acc:0.96, test acc:0.7019\n",
      "epoch: 82   weight_decay_applied ->, train acc:0.44, test acc:0.3416\n",
      "naive network-> train acc:0.9633333333333334, test acc:0.6982\n",
      "epoch: 83   weight_decay_applied ->, train acc:0.44666666666666666, test acc:0.3441\n",
      "naive network-> train acc:0.9566666666666667, test acc:0.7088\n",
      "epoch: 84   weight_decay_applied ->, train acc:0.45, test acc:0.3486\n",
      "naive network-> train acc:0.9633333333333334, test acc:0.7097\n",
      "epoch: 85   weight_decay_applied ->, train acc:0.45, test acc:0.3468\n",
      "naive network-> train acc:0.96, test acc:0.7084\n",
      "epoch: 86   weight_decay_applied ->, train acc:0.44333333333333336, test acc:0.3423\n",
      "naive network-> train acc:0.9633333333333334, test acc:0.7108\n",
      "epoch: 87   weight_decay_applied ->, train acc:0.44, test acc:0.3414\n",
      "naive network-> train acc:0.9633333333333334, test acc:0.7164\n",
      "epoch: 88   weight_decay_applied ->, train acc:0.44333333333333336, test acc:0.3448\n",
      "naive network-> train acc:0.9666666666666667, test acc:0.7025\n",
      "epoch: 89   weight_decay_applied ->, train acc:0.4533333333333333, test acc:0.3509\n",
      "naive network-> train acc:0.9633333333333334, test acc:0.7066\n",
      "epoch: 90   weight_decay_applied ->, train acc:0.44333333333333336, test acc:0.3488\n",
      "naive network-> train acc:0.9733333333333334, test acc:0.718\n",
      "epoch: 91   weight_decay_applied ->, train acc:0.45666666666666667, test acc:0.3558\n",
      "naive network-> train acc:0.97, test acc:0.7118\n",
      "epoch: 92   weight_decay_applied ->, train acc:0.4533333333333333, test acc:0.3536\n",
      "naive network-> train acc:0.9733333333333334, test acc:0.7185\n",
      "epoch: 93   weight_decay_applied ->, train acc:0.4666666666666667, test acc:0.3559\n",
      "naive network-> train acc:0.98, test acc:0.7148\n",
      "epoch: 94   weight_decay_applied ->, train acc:0.47, test acc:0.3584\n",
      "naive network-> train acc:0.98, test acc:0.7191\n",
      "epoch: 95   weight_decay_applied ->, train acc:0.4766666666666667, test acc:0.3548\n",
      "naive network-> train acc:0.9766666666666667, test acc:0.7205\n",
      "epoch: 96   weight_decay_applied ->, train acc:0.4766666666666667, test acc:0.358\n",
      "naive network-> train acc:0.9833333333333333, test acc:0.7185\n",
      "epoch: 97   weight_decay_applied ->, train acc:0.4766666666666667, test acc:0.3601\n",
      "naive network-> train acc:0.9766666666666667, test acc:0.7139\n",
      "epoch: 98   weight_decay_applied ->, train acc:0.4766666666666667, test acc:0.3655\n",
      "naive network-> train acc:0.98, test acc:0.7173\n",
      "epoch: 99   weight_decay_applied ->, train acc:0.4866666666666667, test acc:0.3704\n",
      "naive network-> train acc:0.98, test acc:0.7207\n",
      "epoch: 100   weight_decay_applied ->, train acc:0.49, test acc:0.3677\n",
      "naive network-> train acc:0.9866666666666667, test acc:0.7148\n",
      "epoch: 101   weight_decay_applied ->, train acc:0.4866666666666667, test acc:0.3655\n",
      "naive network-> train acc:0.99, test acc:0.7219\n",
      "epoch: 102   weight_decay_applied ->, train acc:0.49666666666666665, test acc:0.3688\n",
      "naive network-> train acc:0.99, test acc:0.7144\n",
      "epoch: 103   weight_decay_applied ->, train acc:0.49666666666666665, test acc:0.3726\n",
      "naive network-> train acc:0.9866666666666667, test acc:0.7198\n",
      "epoch: 104   weight_decay_applied ->, train acc:0.5133333333333333, test acc:0.3806\n",
      "naive network-> train acc:0.9866666666666667, test acc:0.7158\n",
      "epoch: 105   weight_decay_applied ->, train acc:0.5266666666666666, test acc:0.3839\n",
      "naive network-> train acc:0.9933333333333333, test acc:0.7162\n",
      "epoch: 106   weight_decay_applied ->, train acc:0.5066666666666667, test acc:0.3769\n",
      "naive network-> train acc:0.99, test acc:0.7173\n",
      "epoch: 107   weight_decay_applied ->, train acc:0.5066666666666667, test acc:0.3751\n",
      "naive network-> train acc:0.99, test acc:0.728\n",
      "epoch: 108   weight_decay_applied ->, train acc:0.5166666666666667, test acc:0.3779\n",
      "naive network-> train acc:0.99, test acc:0.7248\n",
      "epoch: 109   weight_decay_applied ->, train acc:0.5166666666666667, test acc:0.3798\n",
      "naive network-> train acc:0.99, test acc:0.7246\n",
      "epoch: 110   weight_decay_applied ->, train acc:0.51, test acc:0.3802\n",
      "naive network-> train acc:0.99, test acc:0.724\n",
      "epoch: 111   weight_decay_applied ->, train acc:0.52, test acc:0.3832\n",
      "naive network-> train acc:0.99, test acc:0.7222\n",
      "epoch: 112   weight_decay_applied ->, train acc:0.5266666666666666, test acc:0.3862\n",
      "naive network-> train acc:0.99, test acc:0.7185\n",
      "epoch: 113   weight_decay_applied ->, train acc:0.54, test acc:0.3966\n",
      "naive network-> train acc:0.99, test acc:0.7275\n",
      "epoch: 114   weight_decay_applied ->, train acc:0.55, test acc:0.4022\n",
      "naive network-> train acc:0.99, test acc:0.7263\n",
      "epoch: 115   weight_decay_applied ->, train acc:0.55, test acc:0.4045\n",
      "naive network-> train acc:0.99, test acc:0.7275\n",
      "epoch: 116   weight_decay_applied ->, train acc:0.56, test acc:0.4115\n",
      "naive network-> train acc:0.9933333333333333, test acc:0.7165\n",
      "epoch: 117   weight_decay_applied ->, train acc:0.5533333333333333, test acc:0.4139\n",
      "naive network-> train acc:0.99, test acc:0.7259\n",
      "epoch: 118   weight_decay_applied ->, train acc:0.5433333333333333, test acc:0.413\n",
      "naive network-> train acc:0.9933333333333333, test acc:0.7269\n",
      "epoch: 119   weight_decay_applied ->, train acc:0.56, test acc:0.4173\n",
      "naive network-> train acc:0.9933333333333333, test acc:0.725\n",
      "epoch: 120   weight_decay_applied ->, train acc:0.58, test acc:0.4228\n",
      "naive network-> train acc:0.9933333333333333, test acc:0.7237\n",
      "epoch: 121   weight_decay_applied ->, train acc:0.5666666666666667, test acc:0.4178\n",
      "naive network-> train acc:0.99, test acc:0.7279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 122   weight_decay_applied ->, train acc:0.56, test acc:0.4187\n",
      "naive network-> train acc:0.99, test acc:0.7309\n",
      "epoch: 123   weight_decay_applied ->, train acc:0.5733333333333334, test acc:0.4218\n",
      "naive network-> train acc:0.9933333333333333, test acc:0.7248\n",
      "epoch: 124   weight_decay_applied ->, train acc:0.57, test acc:0.4242\n",
      "naive network-> train acc:0.9933333333333333, test acc:0.7278\n",
      "epoch: 125   weight_decay_applied ->, train acc:0.5633333333333334, test acc:0.4193\n",
      "naive network-> train acc:0.9966666666666667, test acc:0.7317\n",
      "epoch: 126   weight_decay_applied ->, train acc:0.5633333333333334, test acc:0.4227\n",
      "naive network-> train acc:0.9966666666666667, test acc:0.7284\n",
      "epoch: 127   weight_decay_applied ->, train acc:0.57, test acc:0.4304\n",
      "naive network-> train acc:0.9933333333333333, test acc:0.7291\n",
      "epoch: 128   weight_decay_applied ->, train acc:0.5666666666666667, test acc:0.4298\n",
      "naive network-> train acc:0.9933333333333333, test acc:0.7328\n",
      "epoch: 129   weight_decay_applied ->, train acc:0.5766666666666667, test acc:0.4333\n",
      "naive network-> train acc:0.9933333333333333, test acc:0.7273\n",
      "epoch: 130   weight_decay_applied ->, train acc:0.5866666666666667, test acc:0.4395\n",
      "naive network-> train acc:0.9933333333333333, test acc:0.7295\n",
      "epoch: 131   weight_decay_applied ->, train acc:0.5966666666666667, test acc:0.4413\n",
      "naive network-> train acc:1.0, test acc:0.7288\n",
      "epoch: 132   weight_decay_applied ->, train acc:0.6066666666666667, test acc:0.4507\n",
      "naive network-> train acc:1.0, test acc:0.7325\n",
      "epoch: 133   weight_decay_applied ->, train acc:0.6133333333333333, test acc:0.4582\n",
      "naive network-> train acc:0.9966666666666667, test acc:0.7306\n",
      "epoch: 134   weight_decay_applied ->, train acc:0.62, test acc:0.4637\n",
      "naive network-> train acc:1.0, test acc:0.7321\n",
      "epoch: 135   weight_decay_applied ->, train acc:0.6266666666666667, test acc:0.4682\n",
      "naive network-> train acc:1.0, test acc:0.7301\n",
      "epoch: 136   weight_decay_applied ->, train acc:0.63, test acc:0.4717\n",
      "naive network-> train acc:1.0, test acc:0.7307\n",
      "epoch: 137   weight_decay_applied ->, train acc:0.6333333333333333, test acc:0.4732\n",
      "naive network-> train acc:1.0, test acc:0.7308\n",
      "epoch: 138   weight_decay_applied ->, train acc:0.6233333333333333, test acc:0.473\n",
      "naive network-> train acc:1.0, test acc:0.7335\n",
      "epoch: 139   weight_decay_applied ->, train acc:0.63, test acc:0.4786\n",
      "naive network-> train acc:1.0, test acc:0.7375\n",
      "epoch: 140   weight_decay_applied ->, train acc:0.6266666666666667, test acc:0.4825\n",
      "naive network-> train acc:1.0, test acc:0.7297\n",
      "epoch: 141   weight_decay_applied ->, train acc:0.63, test acc:0.4805\n",
      "naive network-> train acc:1.0, test acc:0.7286\n",
      "epoch: 142   weight_decay_applied ->, train acc:0.6233333333333333, test acc:0.4819\n",
      "naive network-> train acc:1.0, test acc:0.7359\n",
      "epoch: 143   weight_decay_applied ->, train acc:0.6233333333333333, test acc:0.4811\n",
      "naive network-> train acc:1.0, test acc:0.7294\n",
      "epoch: 144   weight_decay_applied ->, train acc:0.6333333333333333, test acc:0.4878\n",
      "naive network-> train acc:1.0, test acc:0.7347\n",
      "epoch: 145   weight_decay_applied ->, train acc:0.65, test acc:0.4892\n",
      "naive network-> train acc:1.0, test acc:0.7378\n",
      "epoch: 146   weight_decay_applied ->, train acc:0.6533333333333333, test acc:0.4967\n",
      "naive network-> train acc:1.0, test acc:0.731\n",
      "epoch: 147   weight_decay_applied ->, train acc:0.6533333333333333, test acc:0.4953\n",
      "naive network-> train acc:1.0, test acc:0.7313\n",
      "epoch: 148   weight_decay_applied ->, train acc:0.6566666666666666, test acc:0.4966\n",
      "naive network-> train acc:1.0, test acc:0.7365\n",
      "epoch: 149   weight_decay_applied ->, train acc:0.6566666666666666, test acc:0.4985\n",
      "naive network-> train acc:1.0, test acc:0.7391\n",
      "epoch: 150   weight_decay_applied ->, train acc:0.67, test acc:0.5048\n",
      "naive network-> train acc:1.0, test acc:0.7313\n",
      "epoch: 151   weight_decay_applied ->, train acc:0.6666666666666666, test acc:0.502\n",
      "naive network-> train acc:1.0, test acc:0.7327\n",
      "epoch: 152   weight_decay_applied ->, train acc:0.67, test acc:0.507\n",
      "naive network-> train acc:1.0, test acc:0.736\n",
      "epoch: 153   weight_decay_applied ->, train acc:0.6733333333333333, test acc:0.5095\n",
      "naive network-> train acc:1.0, test acc:0.7334\n",
      "epoch: 154   weight_decay_applied ->, train acc:0.6766666666666666, test acc:0.5127\n",
      "naive network-> train acc:1.0, test acc:0.7382\n",
      "epoch: 155   weight_decay_applied ->, train acc:0.6766666666666666, test acc:0.5123\n",
      "naive network-> train acc:1.0, test acc:0.7377\n",
      "epoch: 156   weight_decay_applied ->, train acc:0.6733333333333333, test acc:0.5122\n",
      "naive network-> train acc:1.0, test acc:0.7404\n",
      "epoch: 157   weight_decay_applied ->, train acc:0.6766666666666666, test acc:0.5209\n",
      "naive network-> train acc:1.0, test acc:0.7386\n",
      "epoch: 158   weight_decay_applied ->, train acc:0.68, test acc:0.5238\n",
      "naive network-> train acc:1.0, test acc:0.7353\n",
      "epoch: 159   weight_decay_applied ->, train acc:0.6833333333333333, test acc:0.5219\n",
      "naive network-> train acc:1.0, test acc:0.7361\n",
      "epoch: 160   weight_decay_applied ->, train acc:0.6833333333333333, test acc:0.523\n",
      "naive network-> train acc:1.0, test acc:0.7374\n",
      "epoch: 161   weight_decay_applied ->, train acc:0.6766666666666666, test acc:0.5252\n",
      "naive network-> train acc:1.0, test acc:0.7407\n",
      "epoch: 162   weight_decay_applied ->, train acc:0.6833333333333333, test acc:0.527\n",
      "naive network-> train acc:1.0, test acc:0.7384\n",
      "epoch: 163   weight_decay_applied ->, train acc:0.6833333333333333, test acc:0.5288\n",
      "naive network-> train acc:1.0, test acc:0.7362\n",
      "epoch: 164   weight_decay_applied ->, train acc:0.6833333333333333, test acc:0.5351\n",
      "naive network-> train acc:1.0, test acc:0.739\n",
      "epoch: 165   weight_decay_applied ->, train acc:0.69, test acc:0.5418\n",
      "naive network-> train acc:1.0, test acc:0.7356\n",
      "epoch: 166   weight_decay_applied ->, train acc:0.6866666666666666, test acc:0.5411\n",
      "naive network-> train acc:1.0, test acc:0.7337\n",
      "epoch: 167   weight_decay_applied ->, train acc:0.69, test acc:0.5382\n",
      "naive network-> train acc:1.0, test acc:0.7346\n",
      "epoch: 168   weight_decay_applied ->, train acc:0.7066666666666667, test acc:0.5412\n",
      "naive network-> train acc:1.0, test acc:0.7338\n",
      "epoch: 169   weight_decay_applied ->, train acc:0.7133333333333334, test acc:0.5441\n",
      "naive network-> train acc:1.0, test acc:0.7377\n",
      "epoch: 170   weight_decay_applied ->, train acc:0.7066666666666667, test acc:0.5484\n",
      "naive network-> train acc:1.0, test acc:0.7372\n",
      "epoch: 171   weight_decay_applied ->, train acc:0.7133333333333334, test acc:0.5509\n",
      "naive network-> train acc:1.0, test acc:0.7388\n",
      "epoch: 172   weight_decay_applied ->, train acc:0.72, test acc:0.5544\n",
      "naive network-> train acc:1.0, test acc:0.7389\n",
      "epoch: 173   weight_decay_applied ->, train acc:0.7233333333333334, test acc:0.5555\n",
      "naive network-> train acc:1.0, test acc:0.7371\n",
      "epoch: 174   weight_decay_applied ->, train acc:0.73, test acc:0.5605\n",
      "naive network-> train acc:1.0, test acc:0.7418\n",
      "epoch: 175   weight_decay_applied ->, train acc:0.7266666666666667, test acc:0.5637\n",
      "naive network-> train acc:1.0, test acc:0.735\n",
      "epoch: 176   weight_decay_applied ->, train acc:0.7266666666666667, test acc:0.5683\n",
      "naive network-> train acc:1.0, test acc:0.7378\n",
      "epoch: 177   weight_decay_applied ->, train acc:0.73, test acc:0.5674\n",
      "naive network-> train acc:1.0, test acc:0.7342\n",
      "epoch: 178   weight_decay_applied ->, train acc:0.7366666666666667, test acc:0.5769\n",
      "naive network-> train acc:1.0, test acc:0.7373\n",
      "epoch: 179   weight_decay_applied ->, train acc:0.74, test acc:0.5769\n",
      "naive network-> train acc:1.0, test acc:0.7344\n",
      "epoch: 180   weight_decay_applied ->, train acc:0.7366666666666667, test acc:0.5727\n",
      "naive network-> train acc:1.0, test acc:0.7382\n",
      "epoch: 181   weight_decay_applied ->, train acc:0.7266666666666667, test acc:0.5705\n",
      "naive network-> train acc:1.0, test acc:0.7399\n",
      "epoch: 182   weight_decay_applied ->, train acc:0.7333333333333333, test acc:0.5714\n",
      "naive network-> train acc:1.0, test acc:0.7376\n",
      "epoch: 183   weight_decay_applied ->, train acc:0.7366666666666667, test acc:0.5758\n",
      "naive network-> train acc:1.0, test acc:0.7396\n",
      "epoch: 184   weight_decay_applied ->, train acc:0.7433333333333333, test acc:0.5781\n",
      "naive network-> train acc:1.0, test acc:0.7377\n",
      "epoch: 185   weight_decay_applied ->, train acc:0.75, test acc:0.5808\n",
      "naive network-> train acc:1.0, test acc:0.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 186   weight_decay_applied ->, train acc:0.74, test acc:0.578\n",
      "naive network-> train acc:1.0, test acc:0.7387\n",
      "epoch: 187   weight_decay_applied ->, train acc:0.7466666666666667, test acc:0.5868\n",
      "naive network-> train acc:1.0, test acc:0.7379\n",
      "epoch: 188   weight_decay_applied ->, train acc:0.75, test acc:0.5902\n",
      "naive network-> train acc:1.0, test acc:0.7394\n",
      "epoch: 189   weight_decay_applied ->, train acc:0.7466666666666667, test acc:0.5909\n",
      "naive network-> train acc:1.0, test acc:0.74\n",
      "epoch: 190   weight_decay_applied ->, train acc:0.7466666666666667, test acc:0.5895\n",
      "naive network-> train acc:1.0, test acc:0.7381\n",
      "epoch: 191   weight_decay_applied ->, train acc:0.7466666666666667, test acc:0.5908\n",
      "naive network-> train acc:1.0, test acc:0.7378\n",
      "epoch: 192   weight_decay_applied ->, train acc:0.7533333333333333, test acc:0.5943\n",
      "naive network-> train acc:1.0, test acc:0.7391\n",
      "epoch: 193   weight_decay_applied ->, train acc:0.7466666666666667, test acc:0.5939\n",
      "naive network-> train acc:1.0, test acc:0.7373\n",
      "epoch: 194   weight_decay_applied ->, train acc:0.75, test acc:0.5945\n",
      "naive network-> train acc:1.0, test acc:0.7377\n",
      "epoch: 195   weight_decay_applied ->, train acc:0.7533333333333333, test acc:0.5953\n",
      "naive network-> train acc:1.0, test acc:0.7396\n",
      "epoch: 196   weight_decay_applied ->, train acc:0.7566666666666667, test acc:0.5958\n",
      "naive network-> train acc:1.0, test acc:0.7364\n",
      "epoch: 197   weight_decay_applied ->, train acc:0.7533333333333333, test acc:0.6001\n",
      "naive network-> train acc:1.0, test acc:0.7414\n",
      "epoch: 198   weight_decay_applied ->, train acc:0.77, test acc:0.605\n",
      "naive network-> train acc:1.0, test acc:0.742\n",
      "epoch: 199   weight_decay_applied ->, train acc:0.7733333333333333, test acc:0.6082\n",
      "naive network-> train acc:1.0, test acc:0.7409\n",
      "epoch: 200   weight_decay_applied ->, train acc:0.7666666666666667, test acc:0.6072\n",
      "naive network-> train acc:1.0, test acc:0.7414\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = load_mnist(normalize=True)\n",
    "x_train = x_train[:300]\n",
    "y_train = y_train[:300]\n",
    "\n",
    "dropout_ratio = 0.15\n",
    "\n",
    "\n",
    "dropout_network = neural_network(input_size=28*28, \n",
    "                                      hidden_size=[100, 100, 100, 100, 100, 100], \n",
    "                                      output_size=10,\n",
    "                                      dropout = True,\n",
    "                                      dropout_p = dropout_ratio)\n",
    "simple_network = neural_network(input_size=28*28, \n",
    "                                      hidden_size=[100, 100, 100, 100, 100, 100], \n",
    "                                      output_size=10)\n",
    "\n",
    "optimizer = sgd(lr=0.01)\n",
    "\n",
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "d_train_acc_list = []\n",
    "d_test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch_cnt = 0\n",
    "\n",
    "for i in range(1000000000):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "    \n",
    "    for nn in (dropout_network, simple_network):\n",
    "        grads = nn.gradient(x_batch, y_batch)\n",
    "        optimizer.update(nn.params, grads)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        d_train_acc = dropout_network.accuracy(x_train, y_train)\n",
    "        d_test_acc = dropout_network.accuracy(x_test, y_test)\n",
    "        d_train_acc_list.append(d_train_acc)\n",
    "        d_test_acc_list.append(d_test_acc)\n",
    "        \n",
    "        train_acc = simple_network.accuracy(x_train, y_train)\n",
    "        test_acc = simple_network.accuracy(x_test, y_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(\"epoch: \" + str(epoch_cnt) +'   weight_decay_applied ->' + \n",
    "              \", train acc:\" + str(d_train_acc) + \", test acc:\" + str(d_test_acc)\n",
    "              +'\\n'+ \"naive network-> train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) )\n",
    "\n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break\n",
    "            \n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, d_train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, d_test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = load_mnist(normalize=True)\n",
    "\n",
    "x_train = x_train[:500]\n",
    "y_train = y_train[:500]\n",
    "\n",
    "val_rate = 0.2\n",
    "val_num = int(x_train.shape[0]*val_rate)\n",
    "\n",
    "permutation = np.random.permutation(x_train.shape[0])\n",
    "x_train = x_train[permutation,:] if x_train.ndim ==2 else x_train[permutation, :, :, :]\n",
    "y_train = y_train[permutation]\n",
    "\n",
    "x_val = x_train[:val_num]\n",
    "y_val = y_train[:val_num]\n",
    "\n",
    "x_train = x_train[val_num:]\n",
    "y_train = y_train[val_num:]\n",
    "\n",
    "optimization_num_trial = 100 \n",
    "result_val = {}\n",
    "result_train = {}\n",
    "\n",
    "batch_size =100\n",
    "train_size = x_train.shape[0]\n",
    "max_epochs=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc:0.38 | lr:0.0033913114161568973, weight decay:6.0025965158250195e-06\n",
      "val acc:0.11 | lr:2.263178395237921e-06, weight decay:2.439624805037282e-05\n",
      "val acc:0.43 | lr:0.0015727675632035323, weight decay:4.975251301455668e-07\n",
      "val acc:0.34 | lr:0.0022192394254010516, weight decay:1.3246517503374562e-07\n",
      "val acc:0.1 | lr:5.25814378933436e-06, weight decay:1.7033459454869813e-07\n",
      "val acc:0.05 | lr:0.0004718048229559609, weight decay:4.193316319460944e-06\n",
      "val acc:0.11 | lr:0.0001068569065459731, weight decay:1.904844892233652e-08\n",
      "val acc:0.06 | lr:3.953243804897508e-05, weight decay:1.3251377022564185e-06\n",
      "val acc:0.2 | lr:0.0006606354300343381, weight decay:1.285415380376562e-07\n",
      "val acc:0.67 | lr:0.007835217164380135, weight decay:6.76247193521217e-08\n",
      "val acc:0.14 | lr:0.00014294351287308304, weight decay:2.1965579541364663e-06\n",
      "val acc:0.07 | lr:1.409935892459458e-06, weight decay:2.1798283275275678e-08\n",
      "val acc:0.29 | lr:0.0021334855278148743, weight decay:2.2913782003232577e-06\n",
      "val acc:0.12 | lr:6.419308901724824e-05, weight decay:5.9147520909981946e-05\n",
      "val acc:0.08 | lr:3.896663986741409e-05, weight decay:1.3560711970419553e-05\n",
      "val acc:0.09 | lr:0.00011595084251857787, weight decay:5.1597130848169486e-05\n",
      "val acc:0.1 | lr:6.930881144784596e-06, weight decay:1.3341188796930924e-05\n",
      "val acc:0.06 | lr:1.1793799827358977e-06, weight decay:2.709348956814349e-05\n",
      "val acc:0.14 | lr:6.724508329574699e-05, weight decay:5.622777638972513e-05\n",
      "val acc:0.54 | lr:0.004239089938996643, weight decay:2.931227211713402e-05\n",
      "val acc:0.07 | lr:4.343150691442273e-06, weight decay:8.414409736115433e-08\n",
      "val acc:0.11 | lr:4.684748033266226e-05, weight decay:3.085820391964751e-05\n",
      "val acc:0.09 | lr:2.205095801731605e-05, weight decay:1.1868101028579233e-07\n",
      "val acc:0.6 | lr:0.0033534712571795812, weight decay:8.27141717112055e-05\n",
      "val acc:0.13 | lr:3.483114571917346e-05, weight decay:3.5710172059943625e-07\n",
      "val acc:0.12 | lr:0.000578183975336864, weight decay:6.383437353777172e-05\n",
      "val acc:0.11 | lr:5.943192150214278e-05, weight decay:2.692906281897304e-07\n",
      "val acc:0.42 | lr:0.0023478534578497143, weight decay:1.4513946415131484e-06\n",
      "val acc:0.08 | lr:8.574604135549983e-06, weight decay:6.853430610710372e-06\n",
      "val acc:0.53 | lr:0.0031324264051240352, weight decay:2.4716785120756205e-06\n",
      "val acc:0.18 | lr:2.9659330977175336e-05, weight decay:2.1985316535687538e-08\n",
      "val acc:0.08 | lr:0.00025102053998924117, weight decay:4.4874679056629594e-07\n",
      "val acc:0.11 | lr:7.779823612820954e-05, weight decay:3.4378584204638726e-08\n",
      "val acc:0.8 | lr:0.009070662998634075, weight decay:1.0458115908497345e-06\n",
      "val acc:0.48 | lr:0.002656063204506433, weight decay:6.560471424270365e-05\n",
      "val acc:0.12 | lr:2.509087008457852e-05, weight decay:8.865831972200797e-06\n",
      "val acc:0.07 | lr:5.986739202289899e-06, weight decay:3.688897712414e-06\n",
      "val acc:0.09 | lr:1.0693045476345123e-06, weight decay:1.7229856278661875e-07\n",
      "val acc:0.08 | lr:1.7921033966987304e-05, weight decay:1.712784695976869e-05\n",
      "val acc:0.1 | lr:1.6441426988901783e-05, weight decay:3.876476261546286e-05\n",
      "val acc:0.1 | lr:1.017215794620858e-05, weight decay:4.980572620269867e-05\n",
      "val acc:0.1 | lr:7.182594267083802e-05, weight decay:2.3262148474633706e-05\n",
      "val acc:0.1 | lr:5.7459942576875506e-06, weight decay:3.7717170879451586e-08\n",
      "val acc:0.12 | lr:0.0005574310923744801, weight decay:4.7820394034217485e-08\n",
      "val acc:0.11 | lr:2.874750133554419e-05, weight decay:7.814796889491052e-05\n",
      "val acc:0.09 | lr:0.0002117060571692685, weight decay:7.305594560532338e-08\n",
      "val acc:0.12 | lr:0.00015720786951957304, weight decay:3.66901014978187e-06\n",
      "val acc:0.05 | lr:1.723710524892699e-05, weight decay:7.614769295944532e-05\n",
      "val acc:0.11 | lr:0.00011609825442493545, weight decay:4.3122618335468926e-08\n",
      "val acc:0.1 | lr:3.419163431852637e-05, weight decay:1.2032704100965317e-06\n",
      "val acc:0.09 | lr:3.0542387241665505e-05, weight decay:2.895441564612976e-08\n",
      "val acc:0.14 | lr:0.00012607276535977005, weight decay:7.222128995806277e-08\n",
      "val acc:0.07 | lr:4.023111706844466e-06, weight decay:3.073812457943142e-07\n",
      "val acc:0.13 | lr:0.0001152154148645915, weight decay:1.7735719439209705e-08\n",
      "val acc:0.1 | lr:0.0007460208194380567, weight decay:2.14217921162096e-06\n",
      "val acc:0.66 | lr:0.00482007975960926, weight decay:1.0205901638249568e-07\n",
      "val acc:0.11 | lr:3.801597324816972e-05, weight decay:1.3660317690369374e-05\n",
      "val acc:0.11 | lr:0.00018387904362827492, weight decay:3.368074301560137e-05\n",
      "val acc:0.11 | lr:1.8722705955644371e-06, weight decay:7.513557346877405e-06\n",
      "val acc:0.07 | lr:2.642178617621041e-05, weight decay:2.3309688159704684e-06\n",
      "val acc:0.05 | lr:2.1992488264834657e-05, weight decay:1.2758956899313515e-06\n",
      "val acc:0.07 | lr:3.253333930350339e-05, weight decay:1.3607867112468244e-05\n",
      "val acc:0.11 | lr:2.1797610394274697e-06, weight decay:8.466530915035408e-06\n",
      "val acc:0.09 | lr:2.9497233310372135e-06, weight decay:3.0193169828526107e-05\n",
      "val acc:0.13 | lr:0.00022300221582214778, weight decay:1.4770880318902277e-05\n",
      "val acc:0.13 | lr:5.3396915570234647e-05, weight decay:8.661771639871143e-07\n",
      "val acc:0.1 | lr:0.00017017074564992945, weight decay:3.2656686580932074e-06\n",
      "val acc:0.09 | lr:1.8250376894584745e-06, weight decay:7.477795452355345e-08\n",
      "val acc:0.13 | lr:1.0402726253496267e-06, weight decay:2.471130520730955e-06\n",
      "val acc:0.09 | lr:1.0530287964781215e-06, weight decay:5.928526425819553e-07\n",
      "val acc:0.08 | lr:1.0466496386069724e-05, weight decay:1.767607169637046e-05\n",
      "val acc:0.1 | lr:5.1661768837046523e-05, weight decay:2.799765938258219e-07\n",
      "val acc:0.41 | lr:0.0013608566392720872, weight decay:3.5006585578300097e-07\n",
      "val acc:0.12 | lr:0.0002732261975197094, weight decay:1.7562809528322782e-06\n",
      "val acc:0.1 | lr:2.78578392656165e-05, weight decay:2.9528627909976024e-06\n",
      "val acc:0.1 | lr:6.1216431025036685e-06, weight decay:4.4659375529539326e-07\n",
      "val acc:0.14 | lr:4.511316845206839e-05, weight decay:2.676992995788688e-05\n",
      "val acc:0.09 | lr:8.759847408142232e-05, weight decay:1.1857830820637529e-07\n",
      "val acc:0.07 | lr:1.4385250107427318e-06, weight decay:7.819588332549113e-07\n",
      "val acc:0.12 | lr:4.902749478334767e-06, weight decay:1.259930239670175e-05\n",
      "val acc:0.78 | lr:0.00808074130462405, weight decay:1.4324738830873476e-07\n",
      "val acc:0.71 | lr:0.008145089825354841, weight decay:1.2106947227640387e-07\n",
      "val acc:0.1 | lr:1.1141635735171466e-06, weight decay:4.125867271181259e-08\n",
      "val acc:0.09 | lr:4.579548594179665e-05, weight decay:1.3815935809438347e-06\n",
      "val acc:0.77 | lr:0.00833212681986843, weight decay:5.082336219801535e-08\n",
      "val acc:0.55 | lr:0.0037954438510289655, weight decay:1.7287837490609687e-07\n",
      "val acc:0.09 | lr:0.0003679775285239363, weight decay:5.131403269542366e-06\n",
      "val acc:0.14 | lr:9.381225469906128e-06, weight decay:1.6638570080260638e-07\n",
      "val acc:0.09 | lr:3.114891553379147e-05, weight decay:1.1091321658600186e-08\n",
      "val acc:0.51 | lr:0.003047492771870915, weight decay:9.837010755532915e-08\n",
      "val acc:0.13 | lr:1.835846721423293e-05, weight decay:6.02088463815225e-07\n",
      "val acc:0.16 | lr:0.00015730411709629459, weight decay:9.894734054482719e-07\n",
      "val acc:0.14 | lr:2.6357779655010212e-05, weight decay:2.024047786433226e-05\n",
      "val acc:0.38 | lr:0.0019109169479330998, weight decay:1.138867282981841e-06\n",
      "val acc:0.09 | lr:0.0017267259723627315, weight decay:3.138704826778964e-07\n",
      "val acc:0.4 | lr:0.0018436518226755724, weight decay:6.019739378778093e-05\n",
      "val acc:0.13 | lr:0.0002369043884038023, weight decay:6.120952895512874e-06\n",
      "val acc:0.77 | lr:0.008000397294608348, weight decay:2.45789820314952e-07\n",
      "val acc:0.13 | lr:0.00011015760503009247, weight decay:2.3715369063450755e-05\n",
      "val acc:0.33 | lr:0.0013057256406185853, weight decay:6.230722810324558e-07\n"
     ]
    }
   ],
   "source": [
    "for _ in range(optimization_num_trial):\n",
    "    \n",
    "    weight_decay = 10 ** np.random.uniform(-8,-4)\n",
    "    learning_rate = 10 ** np.random.uniform(-6,-2)\n",
    "    \n",
    "    network = neural_network(input_size=28*28,\n",
    "                             hidden_size = [100,100,100,100,100,100],\n",
    "                             output_size=10,\n",
    "                             weight_decay_lambda = weight_decay)\n",
    "    \n",
    "    optimizer = sgd(lr=learning_rate)\n",
    "    \n",
    "    train_acc_list = []\n",
    "    val_acc_list = []\n",
    "    \n",
    "    iter_per_epoch = max(train_size / batch_size, 1)\n",
    "    epochs = 50\n",
    "    epoch_cnt = 0\n",
    "    max_iter = int(epochs*iter_per_epoch)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        batch_mask = np.random.choice(train_size, batch_size)\n",
    "        x_batch = x_train[batch_mask]\n",
    "        y_batch = y_train[batch_mask]\n",
    "\n",
    "        grads = network.gradient(x_batch, y_batch)\n",
    "        optimizer.update(network.params, grads)\n",
    "\n",
    "        if i % iter_per_epoch == 0:\n",
    "\n",
    "            train_acc = network.accuracy(x_train, y_train)\n",
    "            val_acc = network.accuracy(x_val, y_val)\n",
    "            train_acc_list.append(train_acc)\n",
    "            val_acc_list.append(val_acc)\n",
    "            \n",
    "            epoch_cnt += 1\n",
    "            if epoch_cnt >= max_iter:\n",
    "                print('\\n')\n",
    "                break\n",
    "            \n",
    "    print(\"val acc:\" + str(val_acc_list[-1]) + \" | lr:\" + str(learning_rate) + \", weight decay:\" + str(weight_decay))\n",
    "\n",
    "    key = \"lr:\" + str(learning_rate) + \", weight decay:\" + str(weight_decay)\n",
    "    result_val[key] = val_acc_list\n",
    "    result_train[key] = train_acc_list\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== Hyper-Parameter Optimization Result ===========\n",
      "Best-1(val acc:0.8) | lr:0.009070662998634075, weight decay:1.0458115908497345e-06\n",
      "Best-2(val acc:0.78) | lr:0.00808074130462405, weight decay:1.4324738830873476e-07\n",
      "Best-3(val acc:0.77) | lr:0.00833212681986843, weight decay:5.082336219801535e-08\n",
      "Best-4(val acc:0.77) | lr:0.008000397294608348, weight decay:2.45789820314952e-07\n",
      "Best-5(val acc:0.71) | lr:0.008145089825354841, weight decay:1.2106947227640387e-07\n",
      "Best-6(val acc:0.67) | lr:0.007835217164380135, weight decay:6.76247193521217e-08\n",
      "Best-7(val acc:0.66) | lr:0.00482007975960926, weight decay:1.0205901638249568e-07\n",
      "Best-8(val acc:0.6) | lr:0.0033534712571795812, weight decay:8.27141717112055e-05\n",
      "Best-9(val acc:0.55) | lr:0.0037954438510289655, weight decay:1.7287837490609687e-07\n",
      "Best-10(val acc:0.54) | lr:0.004239089938996643, weight decay:2.931227211713402e-05\n",
      "Best-11(val acc:0.53) | lr:0.0031324264051240352, weight decay:2.4716785120756205e-06\n",
      "Best-12(val acc:0.51) | lr:0.003047492771870915, weight decay:9.837010755532915e-08\n",
      "Best-13(val acc:0.48) | lr:0.002656063204506433, weight decay:6.560471424270365e-05\n",
      "Best-14(val acc:0.43) | lr:0.0015727675632035323, weight decay:4.975251301455668e-07\n",
      "Best-15(val acc:0.42) | lr:0.0023478534578497143, weight decay:1.4513946415131484e-06\n",
      "Best-16(val acc:0.41) | lr:0.0013608566392720872, weight decay:3.5006585578300097e-07\n",
      "Best-17(val acc:0.4) | lr:0.0018436518226755724, weight decay:6.019739378778093e-05\n",
      "Best-18(val acc:0.38) | lr:0.0033913114161568973, weight decay:6.0025965158250195e-06\n",
      "Best-19(val acc:0.38) | lr:0.0019109169479330998, weight decay:1.138867282981841e-06\n",
      "Best-20(val acc:0.34) | lr:0.0022192394254010516, weight decay:1.3246517503374562e-07\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"=========== Hyper-Parameter Optimization Result ===========\")\n",
    "graph_draw_num = 20\n",
    "col_num = 5\n",
    "row_num = int(np.ceil(graph_draw_num / col_num))\n",
    "i = 0\n",
    "\n",
    "plt.rc('font',size=8)\n",
    "plt.rc('axes',titlesize=8)\n",
    "plt.rc('axes',labelsize=8)\n",
    "plt.rc('legend',fontsize=8)\n",
    "plt.rc('xtick',labelsize=8)\n",
    "plt.rc('xtick',labelsize=8)\n",
    "for key, val_acc_list in sorted(result_val.items(), key=lambda x: x[1][-1], reverse=True):\n",
    "    print(\"Best-\" + str(i+1) + \"(val acc:\" + str(val_acc_list[-1]) + \") | \" + key)\n",
    "\n",
    "    plt.subplot(row_num, col_num, i+1)\n",
    "    plt.title(\"Best-\" + str(i+1))\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    if i % 5:\n",
    "        plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    x = np.arange(len(val_acc_list))\n",
    "    plt.plot(x, val_acc_list)\n",
    "    plt.plot(x, result_train[key], \"--\")\n",
    "    i += 1\n",
    "\n",
    "    if i >= graph_draw_num:\n",
    "        break\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning1 - Ch.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1,28,28),\n",
    "                 conv_param={'filter_num':30,\n",
    "                             'filter_size':5,\n",
    "                             'pad':0,\n",
    "                             'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        \n",
    "    \n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad)/filter_stride + 1\n",
    "        pool_output_size = int(filter_num*(conv_output_size/2)*(conv_output_size/2))\n",
    "\n",
    "        self.params = {}\n",
    "        self.params['W1']= weight_init_std*np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2']= weight_init_std*np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std*np.random.randn(hidden_size,output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'],\n",
    "                                           self.params['b1'],\n",
    "                                           conv_param['stride'],\n",
    "                                           conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2,pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'],\n",
    "                                        self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'],\n",
    "                                        self.params['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self,x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y,t)\n",
    "\n",
    "    def gradient(self,x,t):\n",
    "        self.loss(x,t)\n",
    "        \n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "        return grads\n",
    "    \n",
    "    \n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "    \n",
    "    \n",
    "    def save_params(self, file_name=\"CNNparams.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"CNNparams.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.301298162138348\n",
      "=== epoch:1, train acc:0.164, test acc:0.199 ===\n",
      "train loss:2.3001079644622844\n",
      "train loss:2.2989902651712266\n",
      "train loss:2.295436262034194\n",
      "train loss:2.2931976845862767\n",
      "train loss:2.2907532825342076\n",
      "train loss:2.2814068724768584\n",
      "train loss:2.273370261405911\n",
      "train loss:2.2655328875183396\n",
      "train loss:2.2569858420137\n",
      "train loss:2.2408696728166957\n",
      "train loss:2.2227686032237086\n",
      "train loss:2.1791179180802716\n",
      "train loss:2.163335242285674\n",
      "train loss:2.119484306497101\n",
      "train loss:2.1119425489162897\n",
      "train loss:2.054377061890631\n",
      "train loss:1.9613038996469743\n",
      "train loss:1.885383489987761\n",
      "train loss:1.8812117325369038\n",
      "train loss:1.7468378740775505\n",
      "train loss:1.7185419963467203\n",
      "train loss:1.6400714353623942\n",
      "train loss:1.5155980787193613\n",
      "train loss:1.5323123843435018\n",
      "train loss:1.3895772378386133\n",
      "train loss:1.19171491474267\n",
      "train loss:1.1967768916618966\n",
      "train loss:1.2139627019412738\n",
      "train loss:0.9932565494267139\n",
      "train loss:1.0813891389615344\n",
      "train loss:1.0185692072883483\n",
      "train loss:0.9471072023009832\n",
      "train loss:1.0117377817740354\n",
      "train loss:0.9155306053600429\n",
      "train loss:0.9543004940799278\n",
      "train loss:0.7681972405677128\n",
      "train loss:0.8569456621656185\n",
      "train loss:0.8925241690535286\n",
      "train loss:0.6943619225799204\n",
      "train loss:0.7225507124903748\n",
      "train loss:0.6618200297540401\n",
      "train loss:0.687553485599703\n",
      "train loss:0.5801413870050434\n",
      "train loss:0.6272562863958592\n",
      "train loss:0.7714804061270294\n",
      "train loss:0.4864532883287187\n",
      "train loss:0.6288216164678658\n",
      "train loss:0.5623332173204557\n",
      "train loss:0.5286332603467411\n",
      "train loss:0.5589872284426309\n",
      "train loss:0.5390893698078376\n",
      "train loss:0.6156715977711535\n",
      "train loss:0.6240882226085916\n",
      "train loss:0.5223366099358895\n",
      "train loss:0.5483934422696229\n",
      "train loss:0.5590475216465773\n",
      "train loss:0.4479559048529332\n",
      "train loss:0.4610060936675787\n",
      "train loss:0.4488060138440144\n",
      "train loss:0.33353798434934157\n",
      "train loss:0.49741796727194937\n",
      "train loss:0.5231289102997971\n",
      "train loss:0.3695258809290719\n",
      "train loss:0.5591111494518133\n",
      "train loss:0.597505797431573\n",
      "train loss:0.513254141411656\n",
      "train loss:0.4604608343551854\n",
      "train loss:0.49814347943247356\n",
      "train loss:0.5401909423681066\n",
      "train loss:0.6328586294048445\n",
      "train loss:0.5104748981388619\n",
      "train loss:0.3970858161968009\n",
      "train loss:0.5650558346922425\n",
      "train loss:0.3756400593734508\n",
      "train loss:0.3533311568321941\n",
      "train loss:0.6665832896324271\n",
      "train loss:0.4130665079898092\n",
      "train loss:0.4740219206916649\n",
      "train loss:0.38903029033460995\n",
      "train loss:0.46286137180935016\n",
      "train loss:0.4070955131843606\n",
      "train loss:0.5912152797727753\n",
      "train loss:0.43498365188893734\n",
      "train loss:0.42880716437867455\n",
      "train loss:0.3129525447508346\n",
      "train loss:0.2924226576267144\n",
      "train loss:0.46765704479711756\n",
      "train loss:0.3987043692551852\n",
      "train loss:0.4417236326924753\n",
      "train loss:0.35019116324085126\n",
      "train loss:0.34697382630510243\n",
      "train loss:0.3049817579013315\n",
      "train loss:0.44608215621604363\n",
      "train loss:0.32797339082356713\n",
      "train loss:0.37276597683483387\n",
      "train loss:0.4165428670099027\n",
      "train loss:0.32402123030762603\n",
      "train loss:0.22408340593740458\n",
      "train loss:0.32495396241039665\n",
      "train loss:0.3712542595180466\n",
      "train loss:0.42225924758112626\n",
      "train loss:0.4901198591382525\n",
      "train loss:0.30605784249136997\n",
      "train loss:0.5579791156192653\n",
      "train loss:0.4630145714165512\n",
      "train loss:0.3337158733536599\n",
      "train loss:0.32212069189296416\n",
      "train loss:0.4095525776105712\n",
      "train loss:0.3916565176822672\n",
      "train loss:0.3193999240376762\n",
      "train loss:0.3924053696875407\n",
      "train loss:0.36157462390553546\n",
      "train loss:0.2718900941071655\n",
      "train loss:0.29952955417466504\n",
      "train loss:0.45037938119299087\n",
      "train loss:0.40859435866959953\n",
      "train loss:0.39055218338393816\n",
      "train loss:0.3737855722795039\n",
      "train loss:0.4867089405653649\n",
      "train loss:0.3087088277452962\n",
      "train loss:0.520042905103345\n",
      "train loss:0.2938680757657025\n",
      "train loss:0.37326530421715043\n",
      "train loss:0.3483904233955141\n",
      "train loss:0.22384528975489265\n",
      "train loss:0.2897612813391434\n",
      "train loss:0.3386895513968436\n",
      "train loss:0.36795075177401787\n",
      "train loss:0.35040961609095944\n",
      "train loss:0.2904586742802558\n",
      "train loss:0.22767747013783626\n",
      "train loss:0.40368967979149945\n",
      "train loss:0.4132846622968598\n",
      "train loss:0.2653347719557177\n",
      "train loss:0.24483593775524745\n",
      "train loss:0.3320992321696945\n",
      "train loss:0.35840567437694704\n",
      "train loss:0.28101640285419516\n",
      "train loss:0.3036662800047925\n",
      "train loss:0.3377423375958308\n",
      "train loss:0.3841230652669923\n",
      "train loss:0.21491976503397037\n",
      "train loss:0.3181014079481276\n",
      "train loss:0.274799519464505\n",
      "train loss:0.2810188199221999\n",
      "train loss:0.3560114875099821\n",
      "train loss:0.4102127130475833\n",
      "train loss:0.39738867695750324\n",
      "train loss:0.2008690830889753\n",
      "train loss:0.4427507480724841\n",
      "train loss:0.23210918241230377\n",
      "train loss:0.46641832731827043\n",
      "train loss:0.43782345006140344\n",
      "train loss:0.33311228199845316\n",
      "train loss:0.3608988455640862\n",
      "train loss:0.3696683564531423\n",
      "train loss:0.4286742959417113\n",
      "train loss:0.2723128045700469\n",
      "train loss:0.3292707448772167\n",
      "train loss:0.3559576632775574\n",
      "train loss:0.288522519199397\n",
      "train loss:0.37024242661602375\n",
      "train loss:0.3774399408125609\n",
      "train loss:0.27439143651794123\n",
      "train loss:0.3732843769553598\n",
      "train loss:0.23319405205008614\n",
      "train loss:0.2567337997608946\n",
      "train loss:0.208534755032138\n",
      "train loss:0.42814833895109067\n",
      "train loss:0.3816059399099756\n",
      "train loss:0.41575241994474177\n",
      "train loss:0.21723345966534832\n",
      "train loss:0.33513479791813333\n",
      "train loss:0.20512606579701098\n",
      "train loss:0.37858613574796685\n",
      "train loss:0.31400595209113136\n",
      "train loss:0.42516373115400896\n",
      "train loss:0.3241162568055367\n",
      "train loss:0.23278351942750866\n",
      "train loss:0.4285271973936125\n",
      "train loss:0.20357138629390753\n",
      "train loss:0.2997892556950645\n",
      "train loss:0.2595686721617039\n",
      "train loss:0.26317350145473806\n",
      "train loss:0.3041673140727925\n",
      "train loss:0.3174410702187444\n",
      "train loss:0.2255274375697423\n",
      "train loss:0.20252385566262973\n",
      "train loss:0.2566799667422113\n",
      "train loss:0.41137551146103485\n",
      "train loss:0.2215544863312974\n",
      "train loss:0.42111965393572054\n",
      "train loss:0.22089752049684244\n",
      "train loss:0.30690835343762257\n",
      "train loss:0.4233872497517586\n",
      "train loss:0.35421818086032775\n",
      "train loss:0.31565129572961\n",
      "train loss:0.3789890457922916\n",
      "train loss:0.3844256275051539\n",
      "train loss:0.18892243517558568\n",
      "train loss:0.3469347526431251\n",
      "train loss:0.2714453950292468\n",
      "train loss:0.29175795943506516\n",
      "train loss:0.24447884563471267\n",
      "train loss:0.2779109737511241\n",
      "train loss:0.2514958493837371\n",
      "train loss:0.3349709529727726\n",
      "train loss:0.25941767169852176\n",
      "train loss:0.22583070782429127\n",
      "train loss:0.38053507631998895\n",
      "train loss:0.2268047351514128\n",
      "train loss:0.28738668155402464\n",
      "train loss:0.27941389333965805\n",
      "train loss:0.3638129850619196\n",
      "train loss:0.2502680051789825\n",
      "train loss:0.3058791610681528\n",
      "train loss:0.3201071785546467\n",
      "train loss:0.16989069741856\n",
      "train loss:0.2004579674826337\n",
      "train loss:0.3504993594325616\n",
      "train loss:0.3061667336631213\n",
      "train loss:0.355208848000448\n",
      "train loss:0.21640542055382667\n",
      "train loss:0.2648413458319292\n",
      "train loss:0.12729764162714002\n",
      "train loss:0.15620499509808405\n",
      "train loss:0.288403119749299\n",
      "train loss:0.20818113373913272\n",
      "train loss:0.3418690876940111\n",
      "train loss:0.18040610476850222\n",
      "train loss:0.23854980276151008\n",
      "train loss:0.28827117510748895\n",
      "train loss:0.10742352736427063\n",
      "train loss:0.1811416421635733\n",
      "train loss:0.2032589841603575\n",
      "train loss:0.18959666081153836\n",
      "train loss:0.2233209734503672\n",
      "train loss:0.3258457836389427\n",
      "train loss:0.23029974506102532\n",
      "train loss:0.35066393670530194\n",
      "train loss:0.3126590379026207\n",
      "train loss:0.17687034580992037\n",
      "train loss:0.2076057764823144\n",
      "train loss:0.11004925358958115\n",
      "train loss:0.19813097956124817\n",
      "train loss:0.22024365125328615\n",
      "train loss:0.33187694724468586\n",
      "train loss:0.19352674786644006\n",
      "train loss:0.29727862636295516\n",
      "train loss:0.1443599339002743\n",
      "train loss:0.2612685885786552\n",
      "train loss:0.19252701984812415\n",
      "train loss:0.2391923688791481\n",
      "train loss:0.22680999964689771\n",
      "train loss:0.30154355908350355\n",
      "train loss:0.1753914992124885\n",
      "train loss:0.1842264729507336\n",
      "train loss:0.2592428892210612\n",
      "train loss:0.25696084225797866\n",
      "train loss:0.28826616291194995\n",
      "train loss:0.2637239660363202\n",
      "train loss:0.18248225855465106\n",
      "train loss:0.2351916970198092\n",
      "train loss:0.3441767300373023\n",
      "train loss:0.2612062450288597\n",
      "train loss:0.17922467397279307\n",
      "train loss:0.10302244493880133\n",
      "train loss:0.3426590217510014\n",
      "train loss:0.292771326539878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.288960778174893\n",
      "train loss:0.21393493383368573\n",
      "train loss:0.23569411628050752\n",
      "train loss:0.3464968327056775\n",
      "train loss:0.2390268858095065\n",
      "train loss:0.35172054832792227\n",
      "train loss:0.15783928520927676\n",
      "train loss:0.33780025899608096\n",
      "train loss:0.21899001515312874\n",
      "train loss:0.19154662948487072\n",
      "train loss:0.20339323535461595\n",
      "train loss:0.37188213608995535\n",
      "train loss:0.22319762931943182\n",
      "train loss:0.19605525780245425\n",
      "train loss:0.12288432746374343\n",
      "train loss:0.18665931218728993\n",
      "train loss:0.16210459302165897\n",
      "train loss:0.30154231692895184\n",
      "train loss:0.2787478605266021\n",
      "train loss:0.18720437649718974\n",
      "train loss:0.32764670197551626\n",
      "train loss:0.2586137847044208\n",
      "train loss:0.3344355664331774\n",
      "train loss:0.15419640638652862\n",
      "train loss:0.33432137278816737\n",
      "train loss:0.2384650522119723\n",
      "train loss:0.19174548135660086\n",
      "train loss:0.2857640589506633\n",
      "train loss:0.16996330326839348\n",
      "train loss:0.25522169244039494\n",
      "train loss:0.1521700213694827\n",
      "train loss:0.225610439877554\n",
      "train loss:0.16564107540765324\n",
      "train loss:0.3020068511397039\n",
      "train loss:0.2607330743540937\n",
      "train loss:0.16410978631952744\n",
      "train loss:0.21256947257239367\n",
      "train loss:0.21526997612358212\n",
      "train loss:0.20928373365778952\n",
      "train loss:0.12209879913176012\n",
      "train loss:0.17251773051203106\n",
      "train loss:0.24214993516671812\n",
      "train loss:0.16769199726785786\n",
      "train loss:0.35686260849778284\n",
      "train loss:0.25280656872988794\n",
      "train loss:0.2118608718863969\n",
      "train loss:0.15499951892829972\n",
      "train loss:0.1108239476892058\n",
      "train loss:0.3933422015879844\n",
      "train loss:0.22369625486698427\n",
      "train loss:0.2665545726696435\n",
      "train loss:0.14037246196365072\n",
      "train loss:0.09241450104693896\n",
      "train loss:0.14703093254241698\n",
      "train loss:0.10179702089454476\n",
      "train loss:0.21124209516365838\n",
      "train loss:0.16639164584129187\n",
      "train loss:0.09125807775792637\n",
      "train loss:0.1266217888443475\n",
      "train loss:0.1766133778317162\n",
      "train loss:0.1790025970882455\n",
      "train loss:0.14964446046663096\n",
      "train loss:0.17783181891421113\n",
      "train loss:0.24699899903001815\n",
      "train loss:0.1416238755434227\n",
      "train loss:0.17425788571032072\n",
      "train loss:0.18650949745814735\n",
      "train loss:0.20641458024546538\n",
      "train loss:0.26601984523348443\n",
      "train loss:0.2600104430188878\n",
      "train loss:0.23979847655855738\n",
      "train loss:0.1050251090685212\n",
      "train loss:0.09582834455795897\n",
      "train loss:0.2460375241250607\n",
      "train loss:0.13906105948807718\n",
      "train loss:0.182290312559073\n",
      "train loss:0.19892164126663248\n",
      "train loss:0.16633949997611158\n",
      "train loss:0.20841584580889236\n",
      "train loss:0.19989354113551372\n",
      "train loss:0.2034826543063337\n",
      "train loss:0.1933295616734236\n",
      "train loss:0.08909103317501664\n",
      "train loss:0.16027424051893222\n",
      "train loss:0.1328894356633354\n",
      "train loss:0.13695879819358242\n",
      "train loss:0.08529570074770605\n",
      "train loss:0.2021718283999641\n",
      "train loss:0.25296118382373195\n",
      "train loss:0.36247003471430084\n",
      "train loss:0.26562978801733517\n",
      "train loss:0.1592726087158376\n",
      "train loss:0.26283825338149724\n",
      "train loss:0.1796962940387669\n",
      "train loss:0.15306386713331188\n",
      "train loss:0.15400985877448098\n",
      "train loss:0.10022980166656356\n",
      "train loss:0.2424899820848568\n",
      "train loss:0.1242699237106418\n",
      "train loss:0.23545951573870283\n",
      "train loss:0.3001749138193062\n",
      "train loss:0.15355374574080807\n",
      "train loss:0.19745045635065023\n",
      "train loss:0.10739649642314207\n",
      "train loss:0.13195521516836725\n",
      "train loss:0.2410479259981467\n",
      "train loss:0.17076009549889284\n",
      "train loss:0.0953076670772394\n",
      "train loss:0.29659643290039844\n",
      "train loss:0.18651947116832499\n",
      "train loss:0.08802831281401667\n",
      "train loss:0.10945215956461832\n",
      "train loss:0.2279840573943891\n",
      "train loss:0.14754143301874398\n",
      "train loss:0.20298248923640197\n",
      "train loss:0.16663915443222507\n",
      "train loss:0.10332158971198627\n",
      "train loss:0.2492589831949295\n",
      "train loss:0.17004336596812553\n",
      "train loss:0.23407941783531985\n",
      "train loss:0.05501628758614993\n",
      "train loss:0.1320827155769801\n",
      "train loss:0.18273416877572615\n",
      "train loss:0.10806510476519188\n",
      "train loss:0.1804510393135578\n",
      "train loss:0.11338377709154815\n",
      "train loss:0.11384558397827096\n",
      "train loss:0.2226376394002914\n",
      "train loss:0.1447484987753854\n",
      "train loss:0.09845007699903664\n",
      "train loss:0.1440858631467024\n",
      "train loss:0.10551543321444708\n",
      "train loss:0.12166532499496516\n",
      "train loss:0.16361373089481168\n",
      "train loss:0.15299615950017764\n",
      "train loss:0.16891641640284227\n",
      "train loss:0.13542168479919206\n",
      "train loss:0.10378701070583904\n",
      "train loss:0.20078359789367886\n",
      "train loss:0.1472451431261359\n",
      "train loss:0.15869129662947615\n",
      "train loss:0.13455227645868276\n",
      "train loss:0.10098312621933342\n",
      "train loss:0.06842782315476519\n",
      "train loss:0.13457432252645654\n",
      "train loss:0.19894513240070652\n",
      "train loss:0.1493619319314418\n",
      "train loss:0.2303750360501812\n",
      "train loss:0.06405006754031543\n",
      "train loss:0.21829269373422192\n",
      "train loss:0.08390634271794144\n",
      "train loss:0.09755564918141127\n",
      "train loss:0.19081120893241835\n",
      "train loss:0.13662466412432112\n",
      "train loss:0.2078341363058991\n",
      "train loss:0.08518175404255407\n",
      "train loss:0.15215342032447318\n",
      "train loss:0.10238036972396886\n",
      "train loss:0.14663272075173805\n",
      "train loss:0.1269427895112465\n",
      "train loss:0.06159027560578096\n",
      "train loss:0.18198597757010745\n",
      "train loss:0.08506998248956155\n",
      "train loss:0.10473516129312492\n",
      "train loss:0.2615846942892709\n",
      "train loss:0.10655788401524678\n",
      "train loss:0.18394450828192288\n",
      "train loss:0.16516108647826097\n",
      "train loss:0.2518118285336446\n",
      "train loss:0.32075515936301424\n",
      "train loss:0.16037460066654854\n",
      "train loss:0.13908671663771033\n",
      "train loss:0.07279497762213864\n",
      "train loss:0.13959008763278186\n",
      "train loss:0.11061910404085187\n",
      "train loss:0.1971911641537518\n",
      "train loss:0.12460106806613512\n",
      "train loss:0.17611888940400258\n",
      "train loss:0.16157517093487886\n",
      "train loss:0.11514741032408074\n",
      "train loss:0.10702443997976899\n",
      "train loss:0.11308775879580608\n",
      "train loss:0.09990952816008493\n",
      "train loss:0.09068299980394916\n",
      "train loss:0.10852357046178117\n",
      "train loss:0.13603138980866986\n",
      "train loss:0.10305499453094673\n",
      "train loss:0.10287661448496006\n",
      "train loss:0.2941939402057564\n",
      "train loss:0.04019787769401671\n",
      "train loss:0.08571721348622084\n",
      "train loss:0.18043699274232353\n",
      "train loss:0.1712486484899389\n",
      "train loss:0.14756356591978867\n",
      "train loss:0.09696754950642159\n",
      "train loss:0.111126021259344\n",
      "train loss:0.08963091773238288\n",
      "train loss:0.1474098361972171\n",
      "train loss:0.098200303723476\n",
      "train loss:0.08515157643943183\n",
      "train loss:0.09983945781992926\n",
      "train loss:0.08723486343269019\n",
      "train loss:0.12132502966012\n",
      "train loss:0.11283974866326574\n",
      "train loss:0.0840324871504146\n",
      "train loss:0.14828081581060404\n",
      "train loss:0.21529650710213702\n",
      "train loss:0.09521401735484578\n",
      "train loss:0.07770811490772188\n",
      "train loss:0.30685214026216195\n",
      "train loss:0.1633842963378915\n",
      "train loss:0.1643656710377801\n",
      "train loss:0.1991454816749924\n",
      "train loss:0.12311658181891479\n",
      "train loss:0.12725429517922712\n",
      "train loss:0.11108203496864233\n",
      "train loss:0.09482620509012263\n",
      "train loss:0.09396439815561088\n",
      "train loss:0.15393038569646778\n",
      "train loss:0.2339830921291991\n",
      "train loss:0.07754781553813471\n",
      "train loss:0.1771955941891876\n",
      "train loss:0.14232432204185763\n",
      "train loss:0.10312308205536866\n",
      "train loss:0.08179161116891473\n",
      "train loss:0.12229690725667944\n",
      "train loss:0.14242580309830882\n",
      "train loss:0.0913538556048628\n",
      "train loss:0.10042411116897465\n",
      "train loss:0.09776064987167477\n",
      "train loss:0.13012770772776178\n",
      "train loss:0.14785612937494133\n",
      "train loss:0.17624796007249033\n",
      "train loss:0.16601869172887276\n",
      "train loss:0.07563857864381511\n",
      "train loss:0.22586620129508306\n",
      "train loss:0.11004062645865456\n",
      "train loss:0.12797187430474036\n",
      "train loss:0.1724005316024629\n",
      "train loss:0.07884683619276904\n",
      "train loss:0.1908078871105069\n",
      "train loss:0.1647610607498232\n",
      "train loss:0.11765236067214323\n",
      "train loss:0.09051462920452114\n",
      "train loss:0.1380519332505373\n",
      "train loss:0.10540404854765387\n",
      "train loss:0.17504316865188632\n",
      "train loss:0.19438030540223417\n",
      "train loss:0.18780735574450258\n",
      "train loss:0.10224857083970454\n",
      "train loss:0.16243267591309593\n",
      "train loss:0.10773463304523956\n",
      "train loss:0.24908135253832234\n",
      "train loss:0.1334823023667543\n",
      "train loss:0.11569249325083346\n",
      "train loss:0.19410470149711515\n",
      "train loss:0.09186709081476266\n",
      "train loss:0.07493940388477024\n",
      "train loss:0.09499898629202848\n",
      "train loss:0.19608780485619662\n",
      "train loss:0.08941204080117884\n",
      "train loss:0.11895805349193803\n",
      "train loss:0.2167482545766053\n",
      "train loss:0.10793104062426767\n",
      "train loss:0.07886495902793805\n",
      "train loss:0.08992834173369676\n",
      "train loss:0.10154907614952217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.18656789132927792\n",
      "train loss:0.0896347062613446\n",
      "train loss:0.26617591580661076\n",
      "train loss:0.06875675516075451\n",
      "train loss:0.14279382620829756\n",
      "train loss:0.13667027127199213\n",
      "train loss:0.10769914942949269\n",
      "train loss:0.07519793791335577\n",
      "train loss:0.13896595312789212\n",
      "train loss:0.132148202515896\n",
      "train loss:0.09490823872445352\n",
      "train loss:0.08827007975497009\n",
      "train loss:0.1708729684785347\n",
      "train loss:0.14522757992191088\n",
      "train loss:0.11280192059671093\n",
      "train loss:0.201916306779324\n",
      "train loss:0.14092579426035967\n",
      "train loss:0.06346544642441419\n",
      "train loss:0.11480023673182346\n",
      "train loss:0.09993770886986306\n",
      "train loss:0.1000127236252087\n",
      "train loss:0.14227732869995927\n",
      "train loss:0.06902861503951267\n",
      "train loss:0.12079156210794005\n",
      "train loss:0.06644972695641577\n",
      "train loss:0.1362929337907755\n",
      "train loss:0.09301816676804839\n",
      "train loss:0.07498372824910268\n",
      "train loss:0.051271294126827724\n",
      "train loss:0.1337263679297619\n",
      "train loss:0.06350430024342307\n",
      "train loss:0.08016818064099228\n",
      "train loss:0.12519187370942167\n",
      "train loss:0.10686835556387403\n",
      "train loss:0.07312305560078254\n",
      "train loss:0.12696253227805657\n",
      "train loss:0.10609829148704032\n",
      "train loss:0.04592619717575653\n",
      "train loss:0.09861231174338592\n",
      "train loss:0.12217927542849599\n",
      "train loss:0.08036660900368502\n",
      "train loss:0.10773883114379292\n",
      "train loss:0.1611842539010912\n",
      "train loss:0.20582795440996837\n",
      "train loss:0.1391690020431459\n",
      "train loss:0.08869907167397174\n",
      "train loss:0.0658568002920488\n",
      "train loss:0.10877213191960199\n",
      "train loss:0.10110969752421171\n",
      "train loss:0.15121602147757524\n",
      "train loss:0.08639482553148362\n",
      "train loss:0.06732928998321658\n",
      "train loss:0.1503795930633561\n",
      "train loss:0.07699141802521461\n",
      "train loss:0.11427297564110582\n",
      "train loss:0.09981656121622202\n",
      "train loss:0.1477079817535586\n",
      "train loss:0.0841192673169616\n",
      "train loss:0.19510205318831347\n",
      "train loss:0.14480292244438345\n",
      "train loss:0.10678280193304003\n",
      "train loss:0.08635656649306302\n",
      "train loss:0.06739775743718507\n",
      "train loss:0.2022427639141987\n",
      "=== epoch:2, train acc:0.958, test acc:0.961 ===\n",
      "train loss:0.07280359662676436\n",
      "train loss:0.038723911058767185\n",
      "train loss:0.14494274601255627\n",
      "train loss:0.11828011663145166\n",
      "train loss:0.09187011337973151\n",
      "train loss:0.10543566586118183\n",
      "train loss:0.07588601174984722\n",
      "train loss:0.07749312104361879\n",
      "train loss:0.11647746814817697\n",
      "train loss:0.08249085208144112\n",
      "train loss:0.15902723558436072\n",
      "train loss:0.09683636564408736\n",
      "train loss:0.06121693918982698\n",
      "train loss:0.062377096755847655\n",
      "train loss:0.17755090416967348\n",
      "train loss:0.12139799737050161\n",
      "train loss:0.07607958931323833\n",
      "train loss:0.16325889972822563\n",
      "train loss:0.22933821900037216\n",
      "train loss:0.11908306422669375\n",
      "train loss:0.09372938613809\n",
      "train loss:0.04912184683431046\n",
      "train loss:0.09152241152286086\n",
      "train loss:0.08877549413199672\n",
      "train loss:0.09238798516001272\n",
      "train loss:0.15728345518121223\n",
      "train loss:0.16560249162165136\n",
      "train loss:0.09248409566498214\n",
      "train loss:0.07904734768884346\n",
      "train loss:0.16180917631237954\n",
      "train loss:0.07745233590395595\n",
      "train loss:0.1498474819491817\n",
      "train loss:0.1897482233131173\n",
      "train loss:0.0905202721432995\n",
      "train loss:0.13263708541060615\n",
      "train loss:0.09206666507004484\n",
      "train loss:0.042084201406983136\n",
      "train loss:0.0621996525151144\n",
      "train loss:0.14075930535195835\n",
      "train loss:0.1281819827323876\n",
      "train loss:0.14345423844842953\n",
      "train loss:0.07256881739143978\n",
      "train loss:0.04403413265442965\n",
      "train loss:0.056918120741601845\n",
      "train loss:0.09930202377206244\n",
      "train loss:0.08321482722040613\n",
      "train loss:0.18133739953817152\n",
      "train loss:0.06268927630276877\n",
      "train loss:0.08633648058537885\n",
      "train loss:0.16831070051814653\n",
      "train loss:0.1400697123298217\n",
      "train loss:0.03761838841045138\n",
      "train loss:0.07295971075449394\n",
      "train loss:0.12201145122895361\n",
      "train loss:0.053131314950204594\n",
      "train loss:0.06769660239952147\n",
      "train loss:0.12945560948412457\n",
      "train loss:0.16211849737260967\n",
      "train loss:0.06214479534890553\n",
      "train loss:0.055666692411628055\n",
      "train loss:0.14205912690834363\n",
      "train loss:0.16441862561606035\n",
      "train loss:0.06853318619767293\n",
      "train loss:0.07917308317959137\n",
      "train loss:0.10417131121461402\n",
      "train loss:0.07608595575329369\n",
      "train loss:0.04187646252560469\n",
      "train loss:0.041620877252013654\n",
      "train loss:0.08789039088977316\n",
      "train loss:0.12773689767655869\n",
      "train loss:0.13732150968421075\n",
      "train loss:0.0737306930810972\n",
      "train loss:0.13267047355964295\n",
      "train loss:0.11572639269997367\n",
      "train loss:0.1184690006489063\n",
      "train loss:0.08954630004029086\n",
      "train loss:0.06803268118862635\n",
      "train loss:0.08224388057554582\n",
      "train loss:0.0646130072461542\n",
      "train loss:0.07061907798653112\n",
      "train loss:0.03644338868864076\n",
      "train loss:0.03717510661559104\n",
      "train loss:0.16886815805926805\n",
      "train loss:0.07201347206279604\n",
      "train loss:0.05976904720561743\n",
      "train loss:0.08428217213167953\n",
      "train loss:0.09764946979895907\n",
      "train loss:0.05235169719361841\n",
      "train loss:0.04229318088962948\n",
      "train loss:0.07769078043485914\n",
      "train loss:0.08319478093615439\n",
      "train loss:0.16801352760041588\n",
      "train loss:0.06960038650351508\n",
      "train loss:0.1196393526777894\n",
      "train loss:0.13429714372399984\n",
      "train loss:0.14560805359497636\n",
      "train loss:0.14265406404442055\n",
      "train loss:0.08740195574613917\n",
      "train loss:0.12451887298720157\n",
      "train loss:0.11087209370847885\n",
      "train loss:0.0968331322023441\n",
      "train loss:0.09205029041907822\n",
      "train loss:0.06091056574036915\n",
      "train loss:0.05021972064826259\n",
      "train loss:0.05256195401808971\n",
      "train loss:0.08111237521164404\n",
      "train loss:0.12363436440486664\n",
      "train loss:0.06525226423901748\n",
      "train loss:0.10311368024841842\n",
      "train loss:0.038002135610241135\n",
      "train loss:0.03628702304620657\n",
      "train loss:0.09667287455215438\n",
      "train loss:0.22674760356662405\n",
      "train loss:0.08511121825136221\n",
      "train loss:0.08453386735334487\n",
      "train loss:0.07267247602996764\n",
      "train loss:0.11586746968342909\n",
      "train loss:0.024744098650644846\n",
      "train loss:0.0886421543160523\n",
      "train loss:0.030856703526099937\n",
      "train loss:0.06828222181064604\n",
      "train loss:0.11717366440992763\n",
      "train loss:0.16154060526670488\n",
      "train loss:0.05887935341178166\n",
      "train loss:0.1060598515132837\n",
      "train loss:0.09906958715561602\n",
      "train loss:0.06649547204562806\n",
      "train loss:0.07107887977362108\n",
      "train loss:0.04054663830168912\n",
      "train loss:0.08750744316362341\n",
      "train loss:0.042021503115488947\n",
      "train loss:0.05865312976361554\n",
      "train loss:0.03280429894570101\n",
      "train loss:0.07629862850671573\n",
      "train loss:0.05954660817218535\n",
      "train loss:0.1301701775648927\n",
      "train loss:0.05280523819236829\n",
      "train loss:0.06818480673203071\n",
      "train loss:0.09430975033531797\n",
      "train loss:0.104258622736058\n",
      "train loss:0.05831909373984568\n",
      "train loss:0.06591711469886935\n",
      "train loss:0.062482183477453004\n",
      "train loss:0.0757266548807687\n",
      "train loss:0.09896392603954894\n",
      "train loss:0.16917565636355214\n",
      "train loss:0.06350280961069636\n",
      "train loss:0.07086766590007339\n",
      "train loss:0.07150015990938136\n",
      "train loss:0.0727249323539011\n",
      "train loss:0.05593337426119667\n",
      "train loss:0.06991478052654133\n",
      "train loss:0.07317368545824127\n",
      "train loss:0.035986132771635246\n",
      "train loss:0.0353871844508527\n",
      "train loss:0.06015799977974007\n",
      "train loss:0.06792082224404612\n",
      "train loss:0.20255946532707012\n",
      "train loss:0.053940626498912694\n",
      "train loss:0.05125721646714964\n",
      "train loss:0.09334238313701154\n",
      "train loss:0.05491048310560573\n",
      "train loss:0.11200960080877417\n",
      "train loss:0.03722275984156377\n",
      "train loss:0.04080919983123217\n",
      "train loss:0.0635988782584496\n",
      "train loss:0.06321224815929342\n",
      "train loss:0.04402754422970813\n",
      "train loss:0.0600446781675003\n",
      "train loss:0.093312254871446\n",
      "train loss:0.15064278330834324\n",
      "train loss:0.09561011672436999\n",
      "train loss:0.0480915006908583\n",
      "train loss:0.04719061901825574\n",
      "train loss:0.09227764813671867\n",
      "train loss:0.12374691416549656\n",
      "train loss:0.029398131358004762\n",
      "train loss:0.0750907312569146\n",
      "train loss:0.06439994169465084\n",
      "train loss:0.12435718549651355\n",
      "train loss:0.09684172652667913\n",
      "train loss:0.13457887040706115\n",
      "train loss:0.04149988584048425\n",
      "train loss:0.031805324995041954\n",
      "train loss:0.19132666800414402\n",
      "train loss:0.06639009157931157\n",
      "train loss:0.11997877119818873\n",
      "train loss:0.045334965256219155\n",
      "train loss:0.07083316795906373\n",
      "train loss:0.051443213132569984\n",
      "train loss:0.04475700810710438\n",
      "train loss:0.17015623708689998\n",
      "train loss:0.04428934200314328\n",
      "train loss:0.14208636168030953\n",
      "train loss:0.09185245658701577\n",
      "train loss:0.06451664827114921\n",
      "train loss:0.03793981440202217\n",
      "train loss:0.05959908363552886\n",
      "train loss:0.036251093891018604\n",
      "train loss:0.08756950630606873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.16418404219803623\n",
      "train loss:0.17023014696816655\n",
      "train loss:0.03615173436502844\n",
      "train loss:0.08115046496127806\n",
      "train loss:0.12722984246594563\n",
      "train loss:0.07144588198093667\n",
      "train loss:0.05379323564016075\n",
      "train loss:0.08168855770305711\n",
      "train loss:0.06679463936195634\n",
      "train loss:0.08420073888341817\n",
      "train loss:0.043142679793297264\n",
      "train loss:0.15539784696582956\n",
      "train loss:0.10764316917918075\n",
      "train loss:0.12184387037463905\n",
      "train loss:0.14655162946780564\n",
      "train loss:0.07122735759022723\n",
      "train loss:0.07624832875808144\n",
      "train loss:0.05426329410893028\n",
      "train loss:0.03213460776765789\n",
      "train loss:0.06340103899041805\n",
      "train loss:0.06806126913647823\n",
      "train loss:0.06766733469962034\n",
      "train loss:0.08714719275279607\n",
      "train loss:0.05418820718665663\n",
      "train loss:0.09647835745038201\n",
      "train loss:0.05469061053898835\n",
      "train loss:0.08730158967042084\n",
      "train loss:0.045538719704142985\n",
      "train loss:0.1656536707231873\n",
      "train loss:0.03301290783007131\n",
      "train loss:0.059457010064031444\n",
      "train loss:0.05129596884252476\n",
      "train loss:0.11382590942967415\n",
      "train loss:0.04965053087901599\n",
      "train loss:0.05994719530024076\n",
      "train loss:0.04860782480654823\n",
      "train loss:0.07145602725068734\n",
      "train loss:0.08238131824302558\n",
      "train loss:0.04738468624026965\n",
      "train loss:0.13484752468358976\n",
      "train loss:0.05652164253562714\n",
      "train loss:0.10739208457646156\n",
      "train loss:0.07305582936468827\n",
      "train loss:0.17552118306938666\n",
      "train loss:0.06578923049074505\n",
      "train loss:0.06167382025440331\n",
      "train loss:0.2880265665113056\n",
      "train loss:0.08844350056270908\n",
      "train loss:0.09404413747409629\n",
      "train loss:0.09726131739287772\n",
      "train loss:0.10852096272329373\n",
      "train loss:0.07337151749269244\n",
      "train loss:0.05482683736834825\n",
      "train loss:0.12894946560712747\n",
      "train loss:0.08291420428063219\n",
      "train loss:0.11764556783125098\n",
      "train loss:0.06588233258753821\n",
      "train loss:0.12283912423617448\n",
      "train loss:0.06560020551073593\n",
      "train loss:0.17226270303201782\n",
      "train loss:0.10161389617543076\n",
      "train loss:0.04640172363817577\n",
      "train loss:0.11022013649055941\n",
      "train loss:0.08244425148550938\n",
      "train loss:0.101474226125021\n",
      "train loss:0.0811183744612905\n",
      "train loss:0.09008272257528838\n",
      "train loss:0.05687262783290896\n",
      "train loss:0.06968277963452481\n",
      "train loss:0.04612357515888707\n",
      "train loss:0.0760487226832318\n",
      "train loss:0.08863413110489655\n",
      "train loss:0.03514548799740354\n",
      "train loss:0.08393043211688214\n",
      "train loss:0.08070634046717032\n",
      "train loss:0.09778531223350846\n",
      "train loss:0.05779436529945024\n",
      "train loss:0.12719898854382383\n",
      "train loss:0.12216483599214842\n",
      "train loss:0.07698938842754113\n",
      "train loss:0.07126785059275394\n",
      "train loss:0.08201605294523696\n",
      "train loss:0.026256533429825896\n",
      "train loss:0.017105543084481055\n",
      "train loss:0.04597147276341765\n",
      "train loss:0.11640991709154992\n",
      "train loss:0.0589631690442416\n",
      "train loss:0.07861404859406382\n",
      "train loss:0.058605602912823906\n",
      "train loss:0.13475899133184363\n",
      "train loss:0.07627601304500069\n",
      "train loss:0.04931310548214092\n",
      "train loss:0.07579103467664386\n",
      "train loss:0.04119340649760311\n",
      "train loss:0.0650381446051419\n",
      "train loss:0.10324799508077107\n",
      "train loss:0.0370061568194651\n",
      "train loss:0.03750651010032599\n",
      "train loss:0.07578655854973619\n",
      "train loss:0.09554705942040392\n",
      "train loss:0.02888464983436528\n",
      "train loss:0.07670843035445114\n",
      "train loss:0.05087084757427733\n",
      "train loss:0.0693801687682054\n",
      "train loss:0.05723798991846404\n",
      "train loss:0.02660531410202026\n",
      "train loss:0.02408029016281562\n",
      "train loss:0.10169139008766327\n",
      "train loss:0.09560316489742135\n",
      "train loss:0.05422624434022419\n",
      "train loss:0.061545294996616845\n",
      "train loss:0.031993048198138994\n",
      "train loss:0.06947208976155786\n",
      "train loss:0.06244406692825995\n",
      "train loss:0.1478523322032314\n",
      "train loss:0.06208587478397052\n",
      "train loss:0.05346121256989884\n",
      "train loss:0.17537847965766812\n",
      "train loss:0.04173095843970789\n",
      "train loss:0.0639476013320911\n",
      "train loss:0.0577961303908338\n",
      "train loss:0.028673965905042103\n",
      "train loss:0.15904067875044256\n",
      "train loss:0.025239611981538904\n",
      "train loss:0.051522921259740756\n",
      "train loss:0.03014137040211905\n",
      "train loss:0.10112355796904286\n",
      "train loss:0.20953239846680227\n",
      "train loss:0.05566133725225542\n",
      "train loss:0.09981608752144275\n",
      "train loss:0.08927212706618738\n",
      "train loss:0.11692287910418007\n",
      "train loss:0.08154743995698112\n",
      "train loss:0.054753368947754684\n",
      "train loss:0.16539541200843114\n",
      "train loss:0.0683163194703893\n",
      "train loss:0.06226024163420915\n",
      "train loss:0.128236736529019\n",
      "train loss:0.046605284643993806\n",
      "train loss:0.06804451240560519\n",
      "train loss:0.08807085975463223\n",
      "train loss:0.022066954861346008\n",
      "train loss:0.08384854746033749\n",
      "train loss:0.09697549553898326\n",
      "train loss:0.029686746529212712\n",
      "train loss:0.03307731773941335\n",
      "train loss:0.06320509016635449\n",
      "train loss:0.025844122164625408\n",
      "train loss:0.043344596223850625\n",
      "train loss:0.07112188712103902\n",
      "train loss:0.029321258361464674\n",
      "train loss:0.11900742235730878\n",
      "train loss:0.08590502640499081\n",
      "train loss:0.022909780225739085\n",
      "train loss:0.06258195838091242\n",
      "train loss:0.07350668726254414\n",
      "train loss:0.09541550547651084\n",
      "train loss:0.06078663949877374\n",
      "train loss:0.04946065709579191\n",
      "train loss:0.09508187906006209\n",
      "train loss:0.05495993059163797\n",
      "train loss:0.018129806663601155\n",
      "train loss:0.03342826977288739\n",
      "train loss:0.07191194286478751\n",
      "train loss:0.06561299380266812\n",
      "train loss:0.05584505019378983\n",
      "train loss:0.0641060640578151\n",
      "train loss:0.06620298815426316\n",
      "train loss:0.07068725466034004\n",
      "train loss:0.03691317489168286\n",
      "train loss:0.03747343729311901\n",
      "train loss:0.057169292454412365\n",
      "train loss:0.20418183150276234\n",
      "train loss:0.019505953051607003\n",
      "train loss:0.03904158929900039\n",
      "train loss:0.07509967020271677\n",
      "train loss:0.017132086648743702\n",
      "train loss:0.03841018015162622\n",
      "train loss:0.0438187199399502\n",
      "train loss:0.02406716965991436\n",
      "train loss:0.05577114824514053\n",
      "train loss:0.08714316520968372\n",
      "train loss:0.027323323266474145\n",
      "train loss:0.03059796645821474\n",
      "train loss:0.04618540079771783\n",
      "train loss:0.026188426524050665\n",
      "train loss:0.04392303454171733\n",
      "train loss:0.06739734772302153\n",
      "train loss:0.044073001009124126\n",
      "train loss:0.03883027295426724\n",
      "train loss:0.11190892459822326\n",
      "train loss:0.061590709996783\n",
      "train loss:0.08481784179780405\n",
      "train loss:0.04096198993086508\n",
      "train loss:0.06609536490016665\n",
      "train loss:0.0393403460006689\n",
      "train loss:0.03100061894918201\n",
      "train loss:0.0642447118247122\n",
      "train loss:0.07737605816834776\n",
      "train loss:0.11225618759344032\n",
      "train loss:0.031164321895721957\n",
      "train loss:0.1472716644389498\n",
      "train loss:0.05713692489470956\n",
      "train loss:0.038133348326217544\n",
      "train loss:0.04496508097309517\n",
      "train loss:0.05657367913860105\n",
      "train loss:0.07829914940479644\n",
      "train loss:0.10836379957761415\n",
      "train loss:0.01815842146830211\n",
      "train loss:0.1349662225029308\n",
      "train loss:0.06388852836967125\n",
      "train loss:0.05164372425027644\n",
      "train loss:0.08058147981272125\n",
      "train loss:0.08782685756612948\n",
      "train loss:0.07279469300866875\n",
      "train loss:0.07248467301143303\n",
      "train loss:0.023415877507449633\n",
      "train loss:0.0424489459988625\n",
      "train loss:0.058203962311957715\n",
      "train loss:0.070877143504966\n",
      "train loss:0.05474407278382414\n",
      "train loss:0.0716716384973991\n",
      "train loss:0.03219113386953738\n",
      "train loss:0.04325426841167797\n",
      "train loss:0.04074141081498922\n",
      "train loss:0.09887034745232348\n",
      "train loss:0.01571536193866979\n",
      "train loss:0.05461952265565285\n",
      "train loss:0.06170249722459581\n",
      "train loss:0.11392011090048132\n",
      "train loss:0.07735397169616312\n",
      "train loss:0.10627694993824431\n",
      "train loss:0.07107973612702992\n",
      "train loss:0.059671777811325294\n",
      "train loss:0.08289769852297138\n",
      "train loss:0.033561745059638916\n",
      "train loss:0.061799651802238474\n",
      "train loss:0.06731104195508653\n",
      "train loss:0.022522165514354136\n",
      "train loss:0.09866014570344556\n",
      "train loss:0.05038291368017555\n",
      "train loss:0.04462419833773642\n",
      "train loss:0.06874322058823126\n",
      "train loss:0.04996917166985829\n",
      "train loss:0.044613554966219986\n",
      "train loss:0.09265347068070641\n",
      "train loss:0.13164955866752748\n",
      "train loss:0.040479789744295995\n",
      "train loss:0.04208138227902919\n",
      "train loss:0.020112768322363334\n",
      "train loss:0.11095380736800124\n",
      "train loss:0.04895353472937519\n",
      "train loss:0.06286102417649085\n",
      "train loss:0.03432938528721332\n",
      "train loss:0.12698240519394255\n",
      "train loss:0.021966614791698872\n",
      "train loss:0.018951623859617097\n",
      "train loss:0.030441263253989704\n",
      "train loss:0.037968695729574745\n",
      "train loss:0.07237025953455087\n",
      "train loss:0.04170703793513963\n",
      "train loss:0.06056310500672824\n",
      "train loss:0.08390163990950315\n",
      "train loss:0.08260989042585287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0418681038145084\n",
      "train loss:0.1432283559526938\n",
      "train loss:0.1333775950366875\n",
      "train loss:0.04584494931738278\n",
      "train loss:0.05806162834711062\n",
      "train loss:0.04033496610473278\n",
      "train loss:0.09027113046113971\n",
      "train loss:0.08122063867591339\n",
      "train loss:0.13535525474853174\n",
      "train loss:0.04135468401303581\n",
      "train loss:0.09162148380221141\n",
      "train loss:0.01669260225103474\n",
      "train loss:0.07131786994793166\n",
      "train loss:0.09010719103862011\n",
      "train loss:0.03309778211816565\n",
      "train loss:0.03924147645954404\n",
      "train loss:0.0646031795951922\n",
      "train loss:0.045361248079835735\n",
      "train loss:0.033672047016400065\n",
      "train loss:0.09461022479031868\n",
      "train loss:0.030540315920572842\n",
      "train loss:0.04421298900869904\n",
      "train loss:0.07902862361349061\n",
      "train loss:0.08452307844523427\n",
      "train loss:0.07453275414586381\n",
      "train loss:0.06400778050582653\n",
      "train loss:0.0495533657594655\n",
      "train loss:0.03139876069493786\n",
      "train loss:0.05388333354195329\n",
      "train loss:0.06883101657299258\n",
      "train loss:0.0865026982019803\n",
      "train loss:0.041711280690215055\n",
      "train loss:0.04424565831469146\n",
      "train loss:0.04826733637228799\n",
      "train loss:0.10782623150268425\n",
      "train loss:0.10698054540363186\n",
      "train loss:0.10834069641031911\n",
      "train loss:0.1292140217356972\n",
      "train loss:0.05849177569908627\n",
      "train loss:0.04142967265622546\n",
      "train loss:0.07929825080989814\n",
      "train loss:0.05560899583877081\n",
      "train loss:0.0635344347061817\n",
      "train loss:0.04504822874241582\n",
      "train loss:0.04225030586909662\n",
      "train loss:0.052945872975295044\n",
      "train loss:0.1077822997420159\n",
      "train loss:0.03631572223878606\n",
      "train loss:0.18738955706975408\n",
      "train loss:0.0844874322566465\n",
      "train loss:0.024329949844949787\n",
      "train loss:0.07737634631146151\n",
      "train loss:0.067028799494798\n",
      "train loss:0.05513057759819713\n",
      "train loss:0.010338840354232775\n",
      "train loss:0.08911995456866723\n",
      "train loss:0.08374160646589363\n",
      "train loss:0.03195944427618561\n",
      "train loss:0.03562348095544899\n",
      "train loss:0.056314678268121636\n",
      "train loss:0.03432213103040718\n",
      "train loss:0.08873163025941537\n",
      "train loss:0.0745190041377775\n",
      "train loss:0.0564709336108956\n",
      "train loss:0.10524021601913935\n",
      "train loss:0.020915442552639853\n",
      "train loss:0.046691468307781155\n",
      "train loss:0.06807997829287807\n",
      "train loss:0.04892132398871994\n",
      "train loss:0.05204507403121635\n",
      "train loss:0.03274956609844643\n",
      "train loss:0.03716538580258344\n",
      "train loss:0.12455998541050926\n",
      "train loss:0.038287475414491075\n",
      "train loss:0.056500534312493106\n",
      "train loss:0.049059960722330924\n",
      "train loss:0.07259807397667231\n",
      "train loss:0.008741773897292556\n",
      "train loss:0.05364216518614372\n",
      "train loss:0.05823865974846406\n",
      "train loss:0.046970043764014334\n",
      "train loss:0.07876812083941634\n",
      "train loss:0.058641069245232894\n",
      "train loss:0.06807255989477264\n",
      "train loss:0.05151254449584869\n",
      "train loss:0.0835333873978418\n",
      "train loss:0.09356106377641106\n",
      "train loss:0.035671286477029066\n",
      "train loss:0.03671019200679119\n",
      "train loss:0.053338752074867664\n",
      "train loss:0.0223789482486468\n",
      "train loss:0.05178986117308043\n",
      "train loss:0.07822764667559723\n",
      "train loss:0.06213948051171454\n",
      "train loss:0.0438009237407554\n",
      "train loss:0.10973569962692446\n",
      "train loss:0.04366442851211035\n",
      "train loss:0.034459675616142944\n",
      "train loss:0.09322177205651236\n",
      "train loss:0.018458396153046522\n",
      "train loss:0.02457839250783571\n",
      "train loss:0.02389082494954677\n",
      "train loss:0.06412822111250538\n",
      "train loss:0.053472800060463704\n",
      "train loss:0.03360566105183505\n",
      "train loss:0.1509868779089097\n",
      "train loss:0.057006377435127285\n",
      "train loss:0.04224546155037622\n",
      "train loss:0.060058056499354896\n",
      "train loss:0.06929994671086256\n",
      "train loss:0.026192541619741844\n",
      "train loss:0.07436816805257486\n",
      "train loss:0.03684740516391509\n",
      "train loss:0.06817754114488893\n",
      "train loss:0.06539469427190096\n",
      "train loss:0.03447757322147473\n",
      "train loss:0.06947401688361735\n",
      "train loss:0.025082118219782366\n",
      "train loss:0.08502024905971106\n",
      "train loss:0.03992914077838952\n",
      "train loss:0.05588871057364617\n",
      "train loss:0.029895300130910414\n",
      "train loss:0.039130034280136304\n",
      "train loss:0.05067741684744492\n",
      "train loss:0.024529193246812655\n",
      "train loss:0.04713709668785202\n",
      "train loss:0.02771677388643823\n",
      "train loss:0.06412259084471826\n",
      "train loss:0.03534965183962847\n",
      "train loss:0.03090088640214077\n",
      "train loss:0.1200788027548067\n",
      "train loss:0.1556376486365499\n",
      "train loss:0.02319025945098284\n",
      "train loss:0.09220277075239595\n",
      "train loss:0.0628630328384049\n",
      "train loss:0.07312184670658574\n",
      "=== epoch:3, train acc:0.981, test acc:0.977 ===\n",
      "train loss:0.051687563734919555\n",
      "train loss:0.03591252184815281\n",
      "train loss:0.12045908391464252\n",
      "train loss:0.057459270114688765\n",
      "train loss:0.09231024067646823\n",
      "train loss:0.05758921970819813\n",
      "train loss:0.12217138576028805\n",
      "train loss:0.08297967701308609\n",
      "train loss:0.026752103012525275\n",
      "train loss:0.05390476357206356\n",
      "train loss:0.13002088326405187\n",
      "train loss:0.06447324163136942\n",
      "train loss:0.049925580930628904\n",
      "train loss:0.048182709019042896\n",
      "train loss:0.0849035789779155\n",
      "train loss:0.047625838487849986\n",
      "train loss:0.06843191409087461\n",
      "train loss:0.028158817101932132\n",
      "train loss:0.022522090555619975\n",
      "train loss:0.20095524499106027\n",
      "train loss:0.06671624561512791\n",
      "train loss:0.03997163179599253\n",
      "train loss:0.016114239688300214\n",
      "train loss:0.05889943319733179\n",
      "train loss:0.04737134583658869\n",
      "train loss:0.028302823807681565\n",
      "train loss:0.06149083141054028\n",
      "train loss:0.02314121433201275\n",
      "train loss:0.04224794502173298\n",
      "train loss:0.05615772471778253\n",
      "train loss:0.10517783535672665\n",
      "train loss:0.06725142950755116\n",
      "train loss:0.05379298126690868\n",
      "train loss:0.05894046201007511\n",
      "train loss:0.05034872487842982\n",
      "train loss:0.029905331918063528\n",
      "train loss:0.02820557673952475\n",
      "train loss:0.03225641204916015\n",
      "train loss:0.11504789566703495\n",
      "train loss:0.028690706206042783\n",
      "train loss:0.01806499320084452\n",
      "train loss:0.019460725597856835\n",
      "train loss:0.03331797369366117\n",
      "train loss:0.08919670099049705\n",
      "train loss:0.1121814180098422\n",
      "train loss:0.06445237171137828\n",
      "train loss:0.03408414320752474\n",
      "train loss:0.02932108187814989\n",
      "train loss:0.0483064375546538\n",
      "train loss:0.017022329213069363\n",
      "train loss:0.016408366454084108\n",
      "train loss:0.04535534286280013\n",
      "train loss:0.04180229532739972\n",
      "train loss:0.09132281921093018\n",
      "train loss:0.05405403154720057\n",
      "train loss:0.06765843706038382\n",
      "train loss:0.15002797790494285\n",
      "train loss:0.08121015058015224\n",
      "train loss:0.035555790947348974\n",
      "train loss:0.10772761515899078\n",
      "train loss:0.06736028475581154\n",
      "train loss:0.06568067547142376\n",
      "train loss:0.04542095754818337\n",
      "train loss:0.03462315393916422\n",
      "train loss:0.05228748071347153\n",
      "train loss:0.03511573392670098\n",
      "train loss:0.03043895251141266\n",
      "train loss:0.03668145384367587\n",
      "train loss:0.1785628230456547\n",
      "train loss:0.07575684864743348\n",
      "train loss:0.03143154788670816\n",
      "train loss:0.051482677107663834\n",
      "train loss:0.03587289140157724\n",
      "train loss:0.1467474558431369\n",
      "train loss:0.03155156373206669\n",
      "train loss:0.02207465791773719\n",
      "train loss:0.04816253940662362\n",
      "train loss:0.0965670347524969\n",
      "train loss:0.060683785743631875\n",
      "train loss:0.11439043588656571\n",
      "train loss:0.10079475091232608\n",
      "train loss:0.06377197847208\n",
      "train loss:0.06089136251495441\n",
      "train loss:0.054427918407641496\n",
      "train loss:0.12527397858678746\n",
      "train loss:0.060116400695817855\n",
      "train loss:0.061082570579934686\n",
      "train loss:0.055092928795466435\n",
      "train loss:0.061290590431330526\n",
      "train loss:0.06936508493208994\n",
      "train loss:0.026656569362158825\n",
      "train loss:0.08235272225557798\n",
      "train loss:0.062849380402311\n",
      "train loss:0.009883720029509368\n",
      "train loss:0.04334345652691975\n",
      "train loss:0.05621694591565518\n",
      "train loss:0.08399950782665322\n",
      "train loss:0.06284753514655525\n",
      "train loss:0.03476672239352267\n",
      "train loss:0.045790766314227384\n",
      "train loss:0.10918716234545453\n",
      "train loss:0.01978600409263941\n",
      "train loss:0.057189936630953425\n",
      "train loss:0.060076925040733385\n",
      "train loss:0.03706619543173626\n",
      "train loss:0.010178388040852667\n",
      "train loss:0.022165325837929174\n",
      "train loss:0.07190652698606938\n",
      "train loss:0.013508642564107453\n",
      "train loss:0.056729914937620946\n",
      "train loss:0.017123397160684072\n",
      "train loss:0.08421233828565926\n",
      "train loss:0.13471529750858283\n",
      "train loss:0.1291401256558444\n",
      "train loss:0.03762187373730768\n",
      "train loss:0.10255822220445301\n",
      "train loss:0.054942611096059454\n",
      "train loss:0.032189643931480535\n",
      "train loss:0.03646333167565116\n",
      "train loss:0.1178320390196323\n",
      "train loss:0.04273304915319983\n",
      "train loss:0.04554864618876428\n",
      "train loss:0.060683457357451454\n",
      "train loss:0.01621561295205967\n",
      "train loss:0.062406483460645236\n",
      "train loss:0.056731285411733084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.09618629937507522\n",
      "train loss:0.05505913457260749\n",
      "train loss:0.0648198470794864\n",
      "train loss:0.08913972407558085\n",
      "train loss:0.0432766180426965\n",
      "train loss:0.027116161672033255\n",
      "train loss:0.03064074491649666\n",
      "train loss:0.05081800278993361\n",
      "train loss:0.03495521137115696\n",
      "train loss:0.07227186641322599\n",
      "train loss:0.050876138874822396\n",
      "train loss:0.023182796814305698\n",
      "train loss:0.0690721773034912\n",
      "train loss:0.06468102920853731\n",
      "train loss:0.03249249198444246\n",
      "train loss:0.08971457597892801\n",
      "train loss:0.1017254324687136\n",
      "train loss:0.06886562439950696\n",
      "train loss:0.02871059699653622\n",
      "train loss:0.1227100443014608\n",
      "train loss:0.08884928002512904\n",
      "train loss:0.03286835615511819\n",
      "train loss:0.0440586401408224\n",
      "train loss:0.014054031473293822\n",
      "train loss:0.047289504642821265\n",
      "train loss:0.01640663486880603\n",
      "train loss:0.055152884905081245\n",
      "train loss:0.0802017447158561\n",
      "train loss:0.025666687221646046\n",
      "train loss:0.017777534439203088\n",
      "train loss:0.04244877808562996\n",
      "train loss:0.08709117689126261\n",
      "train loss:0.09625556291479608\n",
      "train loss:0.05168655000314331\n",
      "train loss:0.015150424750422357\n",
      "train loss:0.03236716722930083\n",
      "train loss:0.06764425990054196\n",
      "train loss:0.03630085924605145\n",
      "train loss:0.11446405117802481\n",
      "train loss:0.0799953768433313\n",
      "train loss:0.03345192949850906\n",
      "train loss:0.10147060873786783\n",
      "train loss:0.06170080442256354\n",
      "train loss:0.020148692780651015\n",
      "train loss:0.030766889719651237\n",
      "train loss:0.09729315400186747\n",
      "train loss:0.055957091789926236\n",
      "train loss:0.08380697397846866\n",
      "train loss:0.007957890086594994\n",
      "train loss:0.08903084507783796\n",
      "train loss:0.03110557572140201\n",
      "train loss:0.03171050162017401\n",
      "train loss:0.05599178715959413\n",
      "train loss:0.18272354159269633\n",
      "train loss:0.013558750319795587\n",
      "train loss:0.02187573730313957\n",
      "train loss:0.07313448489061736\n",
      "train loss:0.10381166075989931\n",
      "train loss:0.07923071256118021\n",
      "train loss:0.063382228627785\n",
      "train loss:0.03441684393517296\n",
      "train loss:0.04161395192380534\n",
      "train loss:0.019421335884217343\n",
      "train loss:0.021609123739077436\n",
      "train loss:0.018679655474885988\n",
      "train loss:0.0853732724782818\n",
      "train loss:0.0510407373812631\n",
      "train loss:0.10531406034860716\n",
      "train loss:0.07672361842348431\n",
      "train loss:0.051657069450528004\n",
      "train loss:0.08407062369703988\n",
      "train loss:0.0346386187473944\n",
      "train loss:0.03422995811300453\n",
      "train loss:0.06491033464756119\n",
      "train loss:0.04771268489158174\n",
      "train loss:0.10706139708431907\n",
      "train loss:0.029391750196351683\n",
      "train loss:0.03931175248222758\n",
      "train loss:0.04813986741273516\n",
      "train loss:0.04673949803685267\n",
      "train loss:0.030606101924242974\n",
      "train loss:0.039088342797250285\n",
      "train loss:0.037753349402696125\n",
      "train loss:0.02949461499726022\n",
      "train loss:0.06647275284814945\n",
      "train loss:0.03415145650854777\n",
      "train loss:0.010866977286683979\n",
      "train loss:0.011355067868334337\n",
      "train loss:0.044477102215932984\n",
      "train loss:0.05209010733133761\n",
      "train loss:0.02373276831646168\n",
      "train loss:0.05067483279185285\n",
      "train loss:0.05282421172867399\n",
      "train loss:0.03172867358494251\n",
      "train loss:0.04137087967343117\n",
      "train loss:0.04772689294212227\n",
      "train loss:0.012436069185948482\n",
      "train loss:0.0885595109397393\n",
      "train loss:0.06282058762785211\n",
      "train loss:0.03838636415710078\n",
      "train loss:0.05442645652048352\n",
      "train loss:0.04890129151524137\n",
      "train loss:0.03494494094395999\n",
      "train loss:0.029825167415590735\n",
      "train loss:0.036653270113391034\n",
      "train loss:0.029266203247781442\n",
      "train loss:0.044751795131616555\n",
      "train loss:0.024227284142535947\n",
      "train loss:0.010413341509837293\n",
      "train loss:0.037512859320914804\n",
      "train loss:0.01455642484190383\n",
      "train loss:0.0643783179501635\n",
      "train loss:0.07730106281890657\n",
      "train loss:0.04369712810327339\n",
      "train loss:0.028130860973555757\n",
      "train loss:0.027458127518352544\n",
      "train loss:0.017787916284006132\n",
      "train loss:0.08880246607798925\n",
      "train loss:0.007773264836733628\n",
      "train loss:0.010449629057196024\n",
      "train loss:0.010756123099055805\n",
      "train loss:0.022170397747508185\n",
      "train loss:0.02275358980516766\n",
      "train loss:0.09159804151221622\n",
      "train loss:0.03139311520962612\n",
      "train loss:0.06540481699556297\n",
      "train loss:0.08098784013305683\n",
      "train loss:0.0050847885396394866\n",
      "train loss:0.038043952660507745\n",
      "train loss:0.057298128947908325\n",
      "train loss:0.05080652605031319\n",
      "train loss:0.019511954188955075\n",
      "train loss:0.04251816696304698\n",
      "train loss:0.01971932900182706\n",
      "train loss:0.06102433676733735\n",
      "train loss:0.015114555918006612\n",
      "train loss:0.03577174195835445\n",
      "train loss:0.03630606637811585\n",
      "train loss:0.06579684381344157\n",
      "train loss:0.008915615613940345\n",
      "train loss:0.024770071769896287\n",
      "train loss:0.027424534439949536\n",
      "train loss:0.02969106636978541\n",
      "train loss:0.04765124949444014\n",
      "train loss:0.036065279432321254\n",
      "train loss:0.008869112979757761\n",
      "train loss:0.017979376279949902\n",
      "train loss:0.06732154885985457\n",
      "train loss:0.023811995277485013\n",
      "train loss:0.027248868797950066\n",
      "train loss:0.032139759046500925\n",
      "train loss:0.07441466561314665\n",
      "train loss:0.027940238715776382\n",
      "train loss:0.022462022695978977\n",
      "train loss:0.018064866273911948\n",
      "train loss:0.022134021643499823\n",
      "train loss:0.017418624100198534\n",
      "train loss:0.019933244816762757\n",
      "train loss:0.13957835913994723\n",
      "train loss:0.0689722078334652\n",
      "train loss:0.048153150445280325\n",
      "train loss:0.012539905049778373\n",
      "train loss:0.09366059299363867\n",
      "train loss:0.06242162683587748\n",
      "train loss:0.030922016206435934\n",
      "train loss:0.0057428250831188365\n",
      "train loss:0.014755953431665114\n",
      "train loss:0.06555883379450751\n",
      "train loss:0.07314127881066247\n",
      "train loss:0.016392573644597203\n",
      "train loss:0.09233464362326049\n",
      "train loss:0.013854954323310657\n",
      "train loss:0.04879503151020309\n",
      "train loss:0.027799930036357962\n",
      "train loss:0.1046015674363943\n",
      "train loss:0.026086361576997518\n",
      "train loss:0.04060216374863011\n",
      "train loss:0.07898092467588598\n",
      "train loss:0.051668630303547715\n",
      "train loss:0.14261937773073005\n",
      "train loss:0.03276133844275213\n",
      "train loss:0.08502825687177823\n",
      "train loss:0.09801603247832703\n",
      "train loss:0.06359123844724786\n",
      "train loss:0.03956353565720757\n",
      "train loss:0.04202548186615465\n",
      "train loss:0.053444316710466906\n",
      "train loss:0.041331732646184005\n",
      "train loss:0.024600259682810374\n",
      "train loss:0.03688264399829077\n",
      "train loss:0.0135865795236225\n",
      "train loss:0.047848258547648355\n",
      "train loss:0.03135757717733969\n",
      "train loss:0.021578020294095522\n",
      "train loss:0.030137739782617447\n",
      "train loss:0.10883035584292372\n",
      "train loss:0.028626621602296468\n",
      "train loss:0.057127961088670715\n",
      "train loss:0.08747249132375541\n",
      "train loss:0.06263422440862632\n",
      "train loss:0.05801519773273164\n",
      "train loss:0.038821930728154125\n",
      "train loss:0.027604179214843677\n",
      "train loss:0.0931877804643036\n",
      "train loss:0.03802359658758622\n",
      "train loss:0.11061852174623932\n",
      "train loss:0.12628076784247266\n",
      "train loss:0.05031833896962568\n",
      "train loss:0.04691596040667302\n",
      "train loss:0.03326446086391938\n",
      "train loss:0.028365007758502446\n",
      "train loss:0.028841564948907127\n",
      "train loss:0.033381747082008084\n",
      "train loss:0.039216567786281124\n",
      "train loss:0.03318775796044994\n",
      "train loss:0.027050359409141262\n",
      "train loss:0.01877770467565255\n",
      "train loss:0.036073471528574264\n",
      "train loss:0.08229457165863026\n",
      "train loss:0.05321005172529707\n",
      "train loss:0.07321977529351612\n",
      "train loss:0.009985783371755564\n",
      "train loss:0.019660261098288733\n",
      "train loss:0.03079487746087116\n",
      "train loss:0.025495113374605336\n",
      "train loss:0.013035112620362652\n",
      "train loss:0.027575282811831286\n",
      "train loss:0.046580378513724235\n",
      "train loss:0.035538652420291965\n",
      "train loss:0.021717747749991582\n",
      "train loss:0.016991229667706092\n",
      "train loss:0.04888937182435846\n",
      "train loss:0.0303009382667297\n",
      "train loss:0.029577715796833458\n",
      "train loss:0.03257544099185095\n",
      "train loss:0.0608860027221507\n",
      "train loss:0.04938390190756337\n",
      "train loss:0.05179249431111179\n",
      "train loss:0.013063796354251346\n",
      "train loss:0.023416007996837164\n",
      "train loss:0.035448009646627206\n",
      "train loss:0.11731727965384589\n",
      "train loss:0.08500400664861062\n",
      "train loss:0.023051486383845566\n",
      "train loss:0.015722543865780456\n",
      "train loss:0.036644042429289085\n",
      "train loss:0.04617481311516124\n",
      "train loss:0.029750792805578515\n",
      "train loss:0.023518585496917887\n",
      "train loss:0.03296822924078085\n",
      "train loss:0.013181765304496195\n",
      "train loss:0.055266708391549416\n",
      "train loss:0.06842954461347009\n",
      "train loss:0.06482155330867437\n",
      "train loss:0.04458851961009639\n",
      "train loss:0.02336797627924673\n",
      "train loss:0.050019835106964415\n",
      "train loss:0.04877591264131924\n",
      "train loss:0.045561976239616844\n",
      "train loss:0.039327956564657957\n",
      "train loss:0.019902534521669936\n",
      "train loss:0.018976829001168597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.030879758921534547\n",
      "train loss:0.03393702867113617\n",
      "train loss:0.025000029715270516\n",
      "train loss:0.048916484324024935\n",
      "train loss:0.06582681995715615\n",
      "train loss:0.007566644478367705\n",
      "train loss:0.04041661220393508\n",
      "train loss:0.06391299409484218\n",
      "train loss:0.03355883357467009\n",
      "train loss:0.0025830377129671036\n",
      "train loss:0.07384688205264674\n",
      "train loss:0.009980884431919707\n",
      "train loss:0.016660158455968414\n",
      "train loss:0.015246131006773527\n",
      "train loss:0.008869960229227392\n",
      "train loss:0.011329534279929616\n",
      "train loss:0.041644665150050234\n",
      "train loss:0.029476005616518308\n",
      "train loss:0.026062085169390056\n",
      "train loss:0.026788833160766377\n",
      "train loss:0.07190487532153586\n",
      "train loss:0.09776767363024881\n",
      "train loss:0.01668106463870767\n",
      "train loss:0.031266088576036764\n",
      "train loss:0.007033774477395793\n",
      "train loss:0.09277755336020163\n",
      "train loss:0.03999082479629415\n",
      "train loss:0.11775364903873456\n",
      "train loss:0.046331624672387405\n",
      "train loss:0.08153594314019105\n",
      "train loss:0.01235545606292708\n",
      "train loss:0.03604292567413138\n",
      "train loss:0.09987927946198621\n",
      "train loss:0.02124698607490446\n",
      "train loss:0.01781675495547469\n",
      "train loss:0.011407856152245058\n",
      "train loss:0.028054768609926426\n",
      "train loss:0.15221783011290868\n",
      "train loss:0.011616857657082053\n",
      "train loss:0.01813712013204498\n",
      "train loss:0.028420858163110223\n",
      "train loss:0.12738890272690406\n",
      "train loss:0.04940749763165152\n",
      "train loss:0.0727900239194253\n",
      "train loss:0.012262105435749716\n",
      "train loss:0.011331479632420429\n",
      "train loss:0.11019853610889249\n",
      "train loss:0.04313582789942539\n",
      "train loss:0.036961162180651964\n",
      "train loss:0.027693054177116188\n",
      "train loss:0.028415965375203217\n",
      "train loss:0.011840467566142251\n",
      "train loss:0.006938974281452034\n",
      "train loss:0.05503057658739222\n",
      "train loss:0.038824114703891556\n",
      "train loss:0.024911108283567688\n",
      "train loss:0.035152919827011374\n",
      "train loss:0.03336881144963045\n",
      "train loss:0.02953565872942971\n",
      "train loss:0.031336924592591395\n",
      "train loss:0.03075804954037713\n",
      "train loss:0.06512483608486935\n",
      "train loss:0.0432037484185063\n",
      "train loss:0.038230048647580527\n",
      "train loss:0.041197428911684104\n",
      "train loss:0.04535829302237222\n",
      "train loss:0.0075823807121163725\n",
      "train loss:0.009291185308698671\n",
      "train loss:0.012280270817316056\n",
      "train loss:0.017314553373334263\n",
      "train loss:0.024124374247877625\n",
      "train loss:0.01822865512534082\n",
      "train loss:0.0385947694838269\n",
      "train loss:0.013692722743634832\n",
      "train loss:0.07122297330980867\n",
      "train loss:0.040447733669120085\n",
      "train loss:0.023706546189729876\n",
      "train loss:0.04657430231665989\n",
      "train loss:0.020774719346036016\n",
      "train loss:0.03952414799632305\n",
      "train loss:0.0570856511651721\n",
      "train loss:0.055975084153765994\n",
      "train loss:0.14212107931320603\n",
      "train loss:0.056936144374221793\n",
      "train loss:0.011698670823204287\n",
      "train loss:0.06421922442335401\n",
      "train loss:0.03993670015471266\n",
      "train loss:0.0376152687060388\n",
      "train loss:0.02568251493289348\n",
      "train loss:0.06725979201401723\n",
      "train loss:0.057497836464346676\n",
      "train loss:0.05368774033820368\n",
      "train loss:0.028105089594098583\n",
      "train loss:0.008106955350942722\n",
      "train loss:0.06328751405062114\n",
      "train loss:0.024813333020679083\n",
      "train loss:0.03456571659216219\n",
      "train loss:0.018144171725945877\n",
      "train loss:0.11468424278721762\n",
      "train loss:0.02742393957404946\n",
      "train loss:0.048060128438426136\n",
      "train loss:0.016800254755207515\n",
      "train loss:0.009533054388885869\n",
      "train loss:0.023223112138660525\n",
      "train loss:0.027602375799388734\n",
      "train loss:0.013087061175280554\n",
      "train loss:0.005770140218339608\n",
      "train loss:0.0820313505303432\n",
      "train loss:0.0474104313886019\n",
      "train loss:0.037707225231106804\n",
      "train loss:0.013081406867531726\n",
      "train loss:0.015532195275155399\n",
      "train loss:0.05080557426225901\n",
      "train loss:0.02251760738834756\n",
      "train loss:0.06557768592935681\n",
      "train loss:0.08662222862138519\n",
      "train loss:0.0169294207586911\n",
      "train loss:0.031353880958146334\n",
      "train loss:0.06184485586682488\n",
      "train loss:0.071108800242883\n",
      "train loss:0.03984233565626523\n",
      "train loss:0.01429428240159735\n",
      "train loss:0.006283134766257935\n",
      "train loss:0.01914949928581415\n",
      "train loss:0.08005282281321287\n",
      "train loss:0.005221578183017109\n",
      "train loss:0.09261035867348692\n",
      "train loss:0.024606828056429237\n",
      "train loss:0.03166067969371727\n",
      "train loss:0.020542779841515466\n",
      "train loss:0.01979037182337043\n",
      "train loss:0.055914746887608585\n",
      "train loss:0.017763937060376128\n",
      "train loss:0.041823999503761346\n",
      "train loss:0.029921690890462004\n",
      "train loss:0.026406922580473946\n",
      "train loss:0.01635396066730783\n",
      "train loss:0.014628621011881065\n",
      "train loss:0.011456526927289452\n",
      "train loss:0.05826584484525752\n",
      "train loss:0.07132080549874988\n",
      "train loss:0.008849931844782681\n",
      "train loss:0.05294292259360346\n",
      "train loss:0.019429667336902507\n",
      "train loss:0.004777231068291564\n",
      "train loss:0.03367168509164612\n",
      "train loss:0.10727216463919163\n",
      "train loss:0.0593485530803896\n",
      "train loss:0.004514317356594267\n",
      "train loss:0.04718053360455013\n",
      "train loss:0.016158008279924845\n",
      "train loss:0.04873553085344108\n",
      "train loss:0.02234110893446227\n",
      "train loss:0.02335178799182755\n",
      "train loss:0.07802668995505142\n",
      "train loss:0.0085730048373871\n",
      "train loss:0.03058556359665865\n",
      "train loss:0.06165929467586878\n",
      "train loss:0.03973110382964642\n",
      "train loss:0.06646023634877371\n",
      "train loss:0.02229968668757673\n",
      "train loss:0.03337505486801475\n",
      "train loss:0.022498871445737575\n",
      "train loss:0.07385368414336647\n",
      "train loss:0.057832911687072074\n",
      "train loss:0.02905634292322144\n",
      "train loss:0.021643728224850868\n",
      "train loss:0.09496401763531877\n",
      "train loss:0.033239122682389356\n",
      "train loss:0.034060013770467294\n",
      "train loss:0.0619618386264788\n",
      "train loss:0.054962969197179835\n",
      "train loss:0.038347989382625\n",
      "train loss:0.05672342163158601\n",
      "train loss:0.009166655629463255\n",
      "train loss:0.04134706812664643\n",
      "train loss:0.04526287697108691\n",
      "train loss:0.01575910403335358\n",
      "train loss:0.03828282965329596\n",
      "train loss:0.03187328973709386\n",
      "train loss:0.02375753424133422\n",
      "train loss:0.023696519893662366\n",
      "train loss:0.016789020695178843\n",
      "train loss:0.050615946164555135\n",
      "train loss:0.054665460525819005\n",
      "train loss:0.06472411324189312\n",
      "train loss:0.07498115491981783\n",
      "train loss:0.015461482425074247\n",
      "train loss:0.024961822996164645\n",
      "train loss:0.004181954011269341\n",
      "train loss:0.00982399191369044\n",
      "train loss:0.011480457430644528\n",
      "train loss:0.03075776042171119\n",
      "train loss:0.11086767719560037\n",
      "train loss:0.018939891770483684\n",
      "train loss:0.03386740497091691\n",
      "train loss:0.03807777541204204\n",
      "train loss:0.04773795017560198\n",
      "train loss:0.0433035366930846\n",
      "train loss:0.05900407953292786\n",
      "train loss:0.09694868413226514\n",
      "train loss:0.06373963748257955\n",
      "train loss:0.01352505820767595\n",
      "train loss:0.0381253797585751\n",
      "train loss:0.021000517890352167\n",
      "train loss:0.01570019850081965\n",
      "train loss:0.015208317934475602\n",
      "train loss:0.09622183396882682\n",
      "train loss:0.009369873957273058\n",
      "train loss:0.07211349110392959\n",
      "train loss:0.007877939366046567\n",
      "train loss:0.02325520139746053\n",
      "=== epoch:4, train acc:0.981, test acc:0.985 ===\n",
      "train loss:0.03391718726733042\n",
      "train loss:0.024326104532252382\n",
      "train loss:0.012457237318148164\n",
      "train loss:0.04057455328842672\n",
      "train loss:0.020509898265767896\n",
      "train loss:0.1348370916653402\n",
      "train loss:0.018549148314521303\n",
      "train loss:0.011748660391527528\n",
      "train loss:0.013683741227890837\n",
      "train loss:0.01591803740488055\n",
      "train loss:0.006362626829335114\n",
      "train loss:0.014135838963768313\n",
      "train loss:0.025574549170814233\n",
      "train loss:0.015959298623266248\n",
      "train loss:0.021902144579982647\n",
      "train loss:0.07527116636759981\n",
      "train loss:0.026556355499537578\n",
      "train loss:0.06856509227002205\n",
      "train loss:0.029233475633963554\n",
      "train loss:0.012330108971877727\n",
      "train loss:0.01358015728255772\n",
      "train loss:0.15159528316996218\n",
      "train loss:0.01166489623763667\n",
      "train loss:0.029052374419919733\n",
      "train loss:0.15224546685710788\n",
      "train loss:0.06183194785614163\n",
      "train loss:0.019729858002017618\n",
      "train loss:0.0701915879282018\n",
      "train loss:0.04879815039039336\n",
      "train loss:0.014435726671372062\n",
      "train loss:0.022150669812348006\n",
      "train loss:0.09500088262504044\n",
      "train loss:0.01232509506009289\n",
      "train loss:0.07685222961726217\n",
      "train loss:0.05742940091208601\n",
      "train loss:0.0215107595646961\n",
      "train loss:0.022891449308478532\n",
      "train loss:0.07391405848188176\n",
      "train loss:0.02935180858959667\n",
      "train loss:0.021514427628969105\n",
      "train loss:0.028609844045081226\n",
      "train loss:0.01539228431133158\n",
      "train loss:0.04234202359512098\n",
      "train loss:0.02777681449332785\n",
      "train loss:0.049056203229428746\n",
      "train loss:0.01567798928235328\n",
      "train loss:0.015091379047036488\n",
      "train loss:0.023807905134784722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.06285922888244387\n",
      "train loss:0.013190773419302575\n",
      "train loss:0.07911062370084378\n",
      "train loss:0.013242232985895701\n",
      "train loss:0.032205848011289506\n",
      "train loss:0.0169773309891902\n",
      "train loss:0.026492616022441066\n",
      "train loss:0.0942982408321838\n",
      "train loss:0.026858204478740388\n",
      "train loss:0.033290130434004576\n",
      "train loss:0.013206005573371513\n",
      "train loss:0.01678248156718247\n",
      "train loss:0.022344848379999255\n",
      "train loss:0.1675378197081196\n",
      "train loss:0.011386017812084339\n",
      "train loss:0.02512458966433916\n",
      "train loss:0.05021248394186787\n",
      "train loss:0.03724553621032287\n",
      "train loss:0.07480770512831054\n",
      "train loss:0.00584724070205734\n",
      "train loss:0.037535085103619234\n",
      "train loss:0.033525997774992586\n",
      "train loss:0.022465057552226872\n",
      "train loss:0.02008873471437165\n",
      "train loss:0.06829271263190764\n",
      "train loss:0.02841518558499644\n",
      "train loss:0.03436797925193538\n",
      "train loss:0.008324948871012743\n",
      "train loss:0.015072678965617665\n",
      "train loss:0.02060502079253085\n",
      "train loss:0.03227308084903871\n",
      "train loss:0.05359640931389948\n",
      "train loss:0.013500352762120258\n",
      "train loss:0.02506593229794518\n",
      "train loss:0.143576166286081\n",
      "train loss:0.057752542860473925\n",
      "train loss:0.03341693850515742\n",
      "train loss:0.02223204602649169\n",
      "train loss:0.09334798358991402\n",
      "train loss:0.037451779913693035\n",
      "train loss:0.14385395529026368\n",
      "train loss:0.013077466813955283\n",
      "train loss:0.09476789059520847\n",
      "train loss:0.021440152624612834\n",
      "train loss:0.08796676423113847\n",
      "train loss:0.025001943778018995\n",
      "train loss:0.018991296353073287\n",
      "train loss:0.13993959360783206\n",
      "train loss:0.03777316296223496\n",
      "train loss:0.043342649965151005\n",
      "train loss:0.013371498131056394\n",
      "train loss:0.015626039012177743\n",
      "train loss:0.022695060394942738\n",
      "train loss:0.013355647669974902\n",
      "train loss:0.019603039950781817\n",
      "train loss:0.029167759220376555\n",
      "train loss:0.028162625104552353\n",
      "train loss:0.026258060170899825\n",
      "train loss:0.05794631384260917\n",
      "train loss:0.03551888242342793\n",
      "train loss:0.01798649599608274\n",
      "train loss:0.06610881277897575\n",
      "train loss:0.07004492954561042\n",
      "train loss:0.029300272351248116\n",
      "train loss:0.039844663226654635\n",
      "train loss:0.013291227228257454\n",
      "train loss:0.010605595841435847\n",
      "train loss:0.020952195209575065\n",
      "train loss:0.03752335904474733\n",
      "train loss:0.02399301485018543\n",
      "train loss:0.03173448246257372\n",
      "train loss:0.013598472617206687\n",
      "train loss:0.008710745472025999\n",
      "train loss:0.01893048190347445\n",
      "train loss:0.0913839711104155\n",
      "train loss:0.11436768477985103\n",
      "train loss:0.009222534942237065\n",
      "train loss:0.024551221037459738\n",
      "train loss:0.009572356372982425\n",
      "train loss:0.015350959955380944\n",
      "train loss:0.04334404452431736\n",
      "train loss:0.016981351268614612\n",
      "train loss:0.05458774084765622\n",
      "train loss:0.04221077946706667\n",
      "train loss:0.00729062396021576\n",
      "train loss:0.0181004359021736\n",
      "train loss:0.05626156210745848\n",
      "train loss:0.050129296536911075\n",
      "train loss:0.02105086226217228\n",
      "train loss:0.024295649155059943\n",
      "train loss:0.08499397444043524\n",
      "train loss:0.021531315244687495\n",
      "train loss:0.3137504839496213\n",
      "train loss:0.017945024056247684\n",
      "train loss:0.04690753680734285\n",
      "train loss:0.021171422525179576\n",
      "train loss:0.036975009884502155\n",
      "train loss:0.021454481964394558\n",
      "train loss:0.027654948600282073\n",
      "train loss:0.05773855361885387\n",
      "train loss:0.020156562382416934\n",
      "train loss:0.06418444078599003\n",
      "train loss:0.018066232526194402\n",
      "train loss:0.023245920807613913\n",
      "train loss:0.00871910834395634\n",
      "train loss:0.03515336940077408\n",
      "train loss:0.03332364590900584\n",
      "train loss:0.07427074301179956\n",
      "train loss:0.04403004275522975\n",
      "train loss:0.14028253207284877\n",
      "train loss:0.0412820096415856\n",
      "train loss:0.01767102455565279\n",
      "train loss:0.04706954064401248\n",
      "train loss:0.020407257964478204\n",
      "train loss:0.017366993135339757\n",
      "train loss:0.05962673539441022\n",
      "train loss:0.010773227635537836\n",
      "train loss:0.07993259261503727\n",
      "train loss:0.04430369277190424\n",
      "train loss:0.022559624187432296\n",
      "train loss:0.01779880019777753\n",
      "train loss:0.027894486565161792\n",
      "train loss:0.006691315422791929\n",
      "train loss:0.01148572505723475\n",
      "train loss:0.049384623539254574\n",
      "train loss:0.025746786681229105\n",
      "train loss:0.015803606502082044\n",
      "train loss:0.06219740852694426\n",
      "train loss:0.010380159334144793\n",
      "train loss:0.039975386140013044\n",
      "train loss:0.034482452492645255\n",
      "train loss:0.03566735051817684\n",
      "train loss:0.020774415859225074\n",
      "train loss:0.04396370522067904\n",
      "train loss:0.038929225749129485\n",
      "train loss:0.01627250336190385\n",
      "train loss:0.00635844299381495\n",
      "train loss:0.00914351283240619\n",
      "train loss:0.028061354222851154\n",
      "train loss:0.026563188364046527\n",
      "train loss:0.012596996790901625\n",
      "train loss:0.016938430133897963\n",
      "train loss:0.004928336621179046\n",
      "train loss:0.00947716491382822\n",
      "train loss:0.04924566957311342\n",
      "train loss:0.022652458716103066\n",
      "train loss:0.020097996812042385\n",
      "train loss:0.04745424418859224\n",
      "train loss:0.01983291896443599\n",
      "train loss:0.012311510584643692\n",
      "train loss:0.0285634144365808\n",
      "train loss:0.023201072480352775\n",
      "train loss:0.007099757697878375\n",
      "train loss:0.046713301767574864\n",
      "train loss:0.014200210859422597\n",
      "train loss:0.012617755913328262\n",
      "train loss:0.10867187369780322\n",
      "train loss:0.04497712927713812\n",
      "train loss:0.015254928585302242\n",
      "train loss:0.020102785484777987\n",
      "train loss:0.016850033498111868\n",
      "train loss:0.023837712523657077\n",
      "train loss:0.04240936723724653\n",
      "train loss:0.019382664392829062\n",
      "train loss:0.003200766650418245\n",
      "train loss:0.007284176752378154\n",
      "train loss:0.07585289741995693\n",
      "train loss:0.007212788439800001\n",
      "train loss:0.007479952808082342\n",
      "train loss:0.015886188403759594\n",
      "train loss:0.015055151174672187\n",
      "train loss:0.03345163708937226\n",
      "train loss:0.03157126618965417\n",
      "train loss:0.009727649267937559\n",
      "train loss:0.06432162463778147\n",
      "train loss:0.06135033985348662\n",
      "train loss:0.02183285129532682\n",
      "train loss:0.04865668475117519\n",
      "train loss:0.025138296254552283\n",
      "train loss:0.048660019222324456\n",
      "train loss:0.01305564416280829\n",
      "train loss:0.024498207640723434\n",
      "train loss:0.015195424131345326\n",
      "train loss:0.037098655755644305\n",
      "train loss:0.006524106713132147\n",
      "train loss:0.021421158795911967\n",
      "train loss:0.019699278316368113\n",
      "train loss:0.036285416677062886\n",
      "train loss:0.014120390565504555\n",
      "train loss:0.09399407164248572\n",
      "train loss:0.005018534391348874\n",
      "train loss:0.01711070136660944\n",
      "train loss:0.01621646971709131\n",
      "train loss:0.019524326491663736\n",
      "train loss:0.0067354868969731595\n",
      "train loss:0.03073823988916564\n",
      "train loss:0.06472488357840027\n",
      "train loss:0.014095823089538894\n",
      "train loss:0.013952177591804197\n",
      "train loss:0.007323834067352415\n",
      "train loss:0.009638143270642247\n",
      "train loss:0.008767461401970596\n",
      "train loss:0.12034979601485159\n",
      "train loss:0.02333583263018476\n",
      "train loss:0.005088649658266533\n",
      "train loss:0.032018251946527715\n",
      "train loss:0.0144760911395972\n",
      "train loss:0.0317684536484579\n",
      "train loss:0.020010612809157623\n",
      "train loss:0.02022216635839892\n",
      "train loss:0.030456097642817467\n",
      "train loss:0.010911602610651676\n",
      "train loss:0.004855454681055525\n",
      "train loss:0.01993055702616194\n",
      "train loss:0.0460756042612916\n",
      "train loss:0.04605571631235485\n",
      "train loss:0.00993869965915014\n",
      "train loss:0.012360699144842214\n",
      "train loss:0.00868165187013652\n",
      "train loss:0.024893666409608373\n",
      "train loss:0.04908355308011378\n",
      "train loss:0.0026363322011350265\n",
      "train loss:0.057207032425666104\n",
      "train loss:0.030031098558429982\n",
      "train loss:0.01361776799153986\n",
      "train loss:0.025950808833132773\n",
      "train loss:0.014994824583912445\n",
      "train loss:0.058178184643173564\n",
      "train loss:0.05104142195821598\n",
      "train loss:0.0797097609201023\n",
      "train loss:0.041349591214645605\n",
      "train loss:0.006122220604230882\n",
      "train loss:0.06410751005189848\n",
      "train loss:0.08085908921501286\n",
      "train loss:0.032780793921236645\n",
      "train loss:0.009002065895940372\n",
      "train loss:0.009990702467673746\n",
      "train loss:0.04499069217208116\n",
      "train loss:0.06119322757969704\n",
      "train loss:0.0385802010366557\n",
      "train loss:0.04592016764336261\n",
      "train loss:0.013698930057185641\n",
      "train loss:0.006459521733329548\n",
      "train loss:0.02649922684677284\n",
      "train loss:0.047855860414952656\n",
      "train loss:0.029912322862673636\n",
      "train loss:0.01870704868773996\n",
      "train loss:0.10846704353047495\n",
      "train loss:0.01610132813164962\n",
      "train loss:0.07729236785106545\n",
      "train loss:0.0014961056250519925\n",
      "train loss:0.014257421459564828\n",
      "train loss:0.02421058184633959\n",
      "train loss:0.03268343931122603\n",
      "train loss:0.01505183607250567\n",
      "train loss:0.05718370657939318\n",
      "train loss:0.035710775058272484\n",
      "train loss:0.04491758041778457\n",
      "train loss:0.08833589420209925\n",
      "train loss:0.03252993901206124\n",
      "train loss:0.04654377270410166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.011912560144454213\n",
      "train loss:0.05704482698118197\n",
      "train loss:0.027905228335023846\n",
      "train loss:0.02737339693489189\n",
      "train loss:0.10849175469456666\n",
      "train loss:0.0905638843136308\n",
      "train loss:0.045908268028790236\n",
      "train loss:0.032737737129637866\n",
      "train loss:0.048213918631201697\n",
      "train loss:0.014852478717237723\n",
      "train loss:0.015196878021315714\n",
      "train loss:0.030360267916122657\n",
      "train loss:0.026041320697151597\n",
      "train loss:0.030924443801127494\n",
      "train loss:0.07412817983253768\n",
      "train loss:0.0201036322924149\n",
      "train loss:0.016164451107378543\n",
      "train loss:0.03960158468952361\n",
      "train loss:0.016717695322855216\n",
      "train loss:0.008071331590729149\n",
      "train loss:0.02265942637009114\n",
      "train loss:0.040317907849080674\n",
      "train loss:0.019504343250615244\n",
      "train loss:0.010220859520098068\n",
      "train loss:0.01312817212199177\n",
      "train loss:0.02151911150793999\n",
      "train loss:0.016742744881932056\n",
      "train loss:0.032136409325655674\n",
      "train loss:0.01891537135739908\n",
      "train loss:0.011964889161775669\n",
      "train loss:0.01961153264164611\n",
      "train loss:0.03876564127117848\n",
      "train loss:0.012748953489923009\n",
      "train loss:0.01079257648045902\n",
      "train loss:0.009509926472178316\n",
      "train loss:0.03523232449891081\n",
      "train loss:0.01953026256754483\n",
      "train loss:0.014538085945809576\n",
      "train loss:0.012154072040310226\n",
      "train loss:0.015041919705799475\n",
      "train loss:0.060304690270796914\n",
      "train loss:0.020573237726663023\n",
      "train loss:0.016586733156697557\n",
      "train loss:0.014950285936604137\n",
      "train loss:0.006066656973279164\n",
      "train loss:0.024298310051248315\n",
      "train loss:0.01542147395513177\n",
      "train loss:0.012810089029748466\n",
      "train loss:0.016011794919380668\n",
      "train loss:0.06924698832168306\n",
      "train loss:0.03768345970529164\n",
      "train loss:0.03948026522148565\n",
      "train loss:0.03290710215470991\n",
      "train loss:0.026842073956125652\n",
      "train loss:0.0035964834313809467\n",
      "train loss:0.02669642613835999\n",
      "train loss:0.009402098961398613\n",
      "train loss:0.0553613268529342\n",
      "train loss:0.028475617535652486\n",
      "train loss:0.021381253415145792\n",
      "train loss:0.03341635267528556\n",
      "train loss:0.009996843306514352\n",
      "train loss:0.005393231827140405\n",
      "train loss:0.022239949199553518\n",
      "train loss:0.05175905736893261\n",
      "train loss:0.006593246723627273\n",
      "train loss:0.09441605863692923\n",
      "train loss:0.02495586427516331\n",
      "train loss:0.0380835277587517\n",
      "train loss:0.009209060922735676\n",
      "train loss:0.013057668505206186\n",
      "train loss:0.01704976349899517\n",
      "train loss:0.01802435653370507\n",
      "train loss:0.030969753657825564\n",
      "train loss:0.03632963359023492\n",
      "train loss:0.0642174098387458\n",
      "train loss:0.011446254054386009\n",
      "train loss:0.014912076925684081\n",
      "train loss:0.008216160642093344\n",
      "train loss:0.07986368654442125\n",
      "train loss:0.026023875789901226\n",
      "train loss:0.014969690358084577\n",
      "train loss:0.07213856653082948\n",
      "train loss:0.047064522705721555\n",
      "train loss:0.008459637547278605\n",
      "train loss:0.01314161970527504\n",
      "train loss:0.016291923848298126\n",
      "train loss:0.03374308842805716\n",
      "train loss:0.0296381794614586\n",
      "train loss:0.012096318321497964\n",
      "train loss:0.02389079865080771\n",
      "train loss:0.01331383914092936\n",
      "train loss:0.08519329615535087\n",
      "train loss:0.017323305451961445\n",
      "train loss:0.023185598771551068\n",
      "train loss:0.021500884811975884\n",
      "train loss:0.015736375541930435\n",
      "train loss:0.0049493745845672985\n",
      "train loss:0.02186685547263575\n",
      "train loss:0.02598820018871597\n",
      "train loss:0.01546951770472966\n",
      "train loss:0.05594633255549351\n",
      "train loss:0.007466932274478978\n",
      "train loss:0.02000986055805346\n",
      "train loss:0.010573354111142148\n",
      "train loss:0.004501549160945194\n",
      "train loss:0.09150980315651078\n",
      "train loss:0.0383637788070143\n",
      "train loss:0.031695141905160436\n",
      "train loss:0.0390215838387288\n",
      "train loss:0.015494650838939061\n",
      "train loss:0.019821148656442397\n",
      "train loss:0.00874665364517082\n",
      "train loss:0.023782448385039866\n",
      "train loss:0.03690783137708455\n",
      "train loss:0.014897455929568424\n",
      "train loss:0.04181149426368655\n",
      "train loss:0.019312193625330754\n",
      "train loss:0.052727737442998766\n",
      "train loss:0.012686994435412966\n",
      "train loss:0.032546974710630075\n",
      "train loss:0.021366901923406522\n",
      "train loss:0.005934613569593949\n",
      "train loss:0.037658816080891694\n",
      "train loss:0.033076246597546065\n",
      "train loss:0.05694842873161134\n",
      "train loss:0.010511622117357764\n",
      "train loss:0.01222640128575049\n",
      "train loss:0.009496322283957493\n",
      "train loss:0.009550661698859429\n",
      "train loss:0.004654770191530824\n",
      "train loss:0.008575478111062088\n",
      "train loss:0.027862761677283066\n",
      "train loss:0.04897963161836028\n",
      "train loss:0.006675874372849924\n",
      "train loss:0.10600137835224194\n",
      "train loss:0.016529237986854316\n",
      "train loss:0.020565144320125874\n",
      "train loss:0.014336262358369442\n",
      "train loss:0.03789993288922297\n",
      "train loss:0.0155609001841154\n",
      "train loss:0.02761447635933719\n",
      "train loss:0.009000254803717274\n",
      "train loss:0.06993348237183901\n",
      "train loss:0.011754477067031683\n",
      "train loss:0.006049552132427455\n",
      "train loss:0.011252587609269086\n",
      "train loss:0.007117773461193216\n",
      "train loss:0.043330614727547685\n",
      "train loss:0.03255593505543684\n",
      "train loss:0.011184822120264025\n",
      "train loss:0.04374314392088793\n",
      "train loss:0.00683956599129834\n",
      "train loss:0.029962104361823654\n",
      "train loss:0.011891437395222824\n",
      "train loss:0.001704545193743199\n",
      "train loss:0.06304210559779708\n",
      "train loss:0.04140332402839003\n",
      "train loss:0.016911489711159463\n",
      "train loss:0.022436967629365076\n",
      "train loss:0.04724454541039951\n",
      "train loss:0.08505261988622653\n",
      "train loss:0.06725330444866084\n",
      "train loss:0.00611970414542227\n",
      "train loss:0.033446371783065594\n",
      "train loss:0.008465443747761618\n",
      "train loss:0.016457038041065894\n",
      "train loss:0.017640400568026036\n",
      "train loss:0.05733782014252376\n",
      "train loss:0.03671677656177099\n",
      "train loss:0.01142980965058391\n",
      "train loss:0.011050414358424082\n",
      "train loss:0.019086525582401835\n",
      "train loss:0.014373956577478109\n",
      "train loss:0.003182417752075538\n",
      "train loss:0.010964797466786184\n",
      "train loss:0.009596126935320557\n",
      "train loss:0.06619725633871798\n",
      "train loss:0.016398077632635434\n",
      "train loss:0.017326463734005322\n",
      "train loss:0.004959015527205079\n",
      "train loss:0.04228129173341576\n",
      "train loss:0.04588606946997289\n",
      "train loss:0.00760480797814249\n",
      "train loss:0.0085247445852856\n",
      "train loss:0.029808522520953774\n",
      "train loss:0.016960883818121865\n",
      "train loss:0.0035236221681530216\n",
      "train loss:0.006366403746949073\n",
      "train loss:0.03503791098482131\n",
      "train loss:0.005883040657075475\n",
      "train loss:0.01911314834793282\n",
      "train loss:0.03038365636749801\n",
      "train loss:0.04654107735902849\n",
      "train loss:0.010410406201996156\n",
      "train loss:0.008365145643229847\n",
      "train loss:0.01644689652730685\n",
      "train loss:0.012028601087194238\n",
      "train loss:0.012009658546217754\n",
      "train loss:0.02308701906484254\n",
      "train loss:0.016738327328613623\n",
      "train loss:0.013526596199109724\n",
      "train loss:0.01561127157849374\n",
      "train loss:0.015500908465612095\n",
      "train loss:0.011919469299373563\n",
      "train loss:0.008371983648118201\n",
      "train loss:0.0034263689897849935\n",
      "train loss:0.011206024655214999\n",
      "train loss:0.030989436997263996\n",
      "train loss:0.060253160050091174\n",
      "train loss:0.004369768943835624\n",
      "train loss:0.01844976013797348\n",
      "train loss:0.018946383577481192\n",
      "train loss:0.028755272120609993\n",
      "train loss:0.018897154814904275\n",
      "train loss:0.04499810155336226\n",
      "train loss:0.018807070392640207\n",
      "train loss:0.02042878176020188\n",
      "train loss:0.02166338336949637\n",
      "train loss:0.006699450889708787\n",
      "train loss:0.04362593221701856\n",
      "train loss:0.050128474262825115\n",
      "train loss:0.004765359834280226\n",
      "train loss:0.0017162856448875402\n",
      "train loss:0.023396010459146573\n",
      "train loss:0.015057560273580843\n",
      "train loss:0.04468344107323677\n",
      "train loss:0.09204163169479555\n",
      "train loss:0.003725908703395249\n",
      "train loss:0.018926480884320317\n",
      "train loss:0.026816334846086306\n",
      "train loss:0.01352733337528766\n",
      "train loss:0.13581892408956595\n",
      "train loss:0.017685751340485594\n",
      "train loss:0.03479969715007456\n",
      "train loss:0.02526935863668319\n",
      "train loss:0.011720844583753205\n",
      "train loss:0.026301962647149832\n",
      "train loss:0.01606458868537965\n",
      "train loss:0.0036838079659869856\n",
      "train loss:0.005020523221743131\n",
      "train loss:0.08761365566164984\n",
      "train loss:0.00951705832012179\n",
      "train loss:0.01611003942053576\n",
      "train loss:0.05738888965948354\n",
      "train loss:0.03194905049826663\n",
      "train loss:0.016682358331537347\n",
      "train loss:0.006024638976645496\n",
      "train loss:0.02489756800657699\n",
      "train loss:0.007854556229307435\n",
      "train loss:0.015845664090764773\n",
      "train loss:0.009849958550984129\n",
      "train loss:0.021836089144927164\n",
      "train loss:0.01223529674372356\n",
      "train loss:0.0977871600405174\n",
      "train loss:0.0157655786155337\n",
      "train loss:0.15399869848077982\n",
      "train loss:0.03673579210199075\n",
      "train loss:0.04479456273626165\n",
      "train loss:0.05784416644610704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.01977777458040335\n",
      "train loss:0.024126533032961067\n",
      "train loss:0.02055015614443123\n",
      "train loss:0.0032354815815152913\n",
      "train loss:0.039645991231812845\n",
      "train loss:0.03018733724866598\n",
      "train loss:0.05884443591117839\n",
      "train loss:0.055416366531319164\n",
      "train loss:0.0052876168392925185\n",
      "train loss:0.006845905077869478\n",
      "train loss:0.009555609633133087\n",
      "train loss:0.015277572060161875\n",
      "train loss:0.014717685012565453\n",
      "train loss:0.01240855239987943\n",
      "train loss:0.018556827961938954\n",
      "train loss:0.02200363778903553\n",
      "train loss:0.007003280230216434\n",
      "train loss:0.06850448331994538\n",
      "train loss:0.00863356606210717\n",
      "train loss:0.009447278518643016\n",
      "train loss:0.1289796787345236\n",
      "train loss:0.016763027668335925\n",
      "train loss:0.006891771830494446\n",
      "train loss:0.015814230601567943\n",
      "train loss:0.044489272821905235\n",
      "train loss:0.018921132013924938\n",
      "train loss:0.012396521479159548\n",
      "train loss:0.023349462205585602\n",
      "train loss:0.024357084307664353\n",
      "train loss:0.005747882590486783\n",
      "train loss:0.01074782598655739\n",
      "=== epoch:5, train acc:0.984, test acc:0.985 ===\n",
      "train loss:0.028515768317790253\n",
      "train loss:0.01131848665267774\n",
      "train loss:0.014005732401409277\n",
      "train loss:0.00511017727686263\n",
      "train loss:0.0026719641968477093\n",
      "train loss:0.03853674934804932\n",
      "train loss:0.01031106970302205\n",
      "train loss:0.018397011150731626\n",
      "train loss:0.013539507232623805\n",
      "train loss:0.011177966731610835\n",
      "train loss:0.008510774932872618\n",
      "train loss:0.04417536366112162\n",
      "train loss:0.07425843385394987\n",
      "train loss:0.021703095333279826\n",
      "train loss:0.007725039577920483\n",
      "train loss:0.14033975234045343\n",
      "train loss:0.021291551738459768\n",
      "train loss:0.01006437442194345\n",
      "train loss:0.03087736388586989\n",
      "train loss:0.04921843986056976\n",
      "train loss:0.05258658569448972\n",
      "train loss:0.017707233078705243\n",
      "train loss:0.026359631113087933\n",
      "train loss:0.03750539345712502\n",
      "train loss:0.023028654784508485\n",
      "train loss:0.01610297642660651\n",
      "train loss:0.08519308492702526\n",
      "train loss:0.021539653230811875\n",
      "train loss:0.036024518873311255\n",
      "train loss:0.038087375407323924\n",
      "train loss:0.01061923864588298\n",
      "train loss:0.0052307865790214535\n",
      "train loss:0.04489189899372556\n",
      "train loss:0.012386743459285882\n",
      "train loss:0.02415415248999886\n",
      "train loss:0.010855321864797884\n",
      "train loss:0.04795955005815502\n",
      "train loss:0.0026583549982886574\n",
      "train loss:0.02208358142811011\n",
      "train loss:0.02764433457823382\n",
      "train loss:0.01267886987649075\n",
      "train loss:0.05371857177387157\n",
      "train loss:0.005323222189849638\n",
      "train loss:0.02212570035406712\n",
      "train loss:0.05639734259772298\n",
      "train loss:0.07855018709472038\n",
      "train loss:0.008967154550358816\n",
      "train loss:0.009764818280571042\n",
      "train loss:0.024858420031076665\n",
      "train loss:0.021643341864335107\n",
      "train loss:0.027347924822159076\n",
      "train loss:0.027585686190766786\n",
      "train loss:0.004206806264423949\n",
      "train loss:0.04331454718212016\n",
      "train loss:0.022973912890989344\n",
      "train loss:0.016658625673969682\n",
      "train loss:0.06068781241543571\n",
      "train loss:0.030877799617790216\n",
      "train loss:0.013209489731691846\n",
      "train loss:0.005590702424081602\n",
      "train loss:0.010811184433109846\n",
      "train loss:0.04122305027341142\n",
      "train loss:0.008129956752800992\n",
      "train loss:0.03914112635559674\n",
      "train loss:0.0080073380959189\n",
      "train loss:0.024205539581446153\n",
      "train loss:0.002668560405449165\n",
      "train loss:0.006325496510812425\n",
      "train loss:0.01835004178800394\n",
      "train loss:0.0082900753994976\n",
      "train loss:0.03963550340272926\n",
      "train loss:0.024225627313022798\n",
      "train loss:0.04020003723449376\n",
      "train loss:0.07044074802026587\n",
      "train loss:0.01860725380153382\n",
      "train loss:0.029593035800266416\n",
      "train loss:0.021354866574169726\n",
      "train loss:0.007102045148636188\n",
      "train loss:0.02502834973118888\n",
      "train loss:0.007116940611366701\n",
      "train loss:0.09552270447251654\n",
      "train loss:0.026554188968532854\n",
      "train loss:0.01629172428352518\n",
      "train loss:0.009923087207125158\n",
      "train loss:0.059504350575059206\n",
      "train loss:0.017433099426376896\n",
      "train loss:0.05319541401628162\n",
      "train loss:0.010667759433484754\n",
      "train loss:0.01457602634102988\n",
      "train loss:0.003908968698033446\n",
      "train loss:0.045703427284543616\n",
      "train loss:0.02085344050045511\n",
      "train loss:0.011388920906567339\n",
      "train loss:0.011041296208746386\n",
      "train loss:0.023131928654936244\n",
      "train loss:0.04858625337397893\n",
      "train loss:0.0357908878412941\n",
      "train loss:0.010353901189607082\n",
      "train loss:0.013810413095818758\n",
      "train loss:0.10027677653285778\n",
      "train loss:0.04135486324782062\n",
      "train loss:0.009788655695173065\n",
      "train loss:0.06711261193247768\n",
      "train loss:0.010057755374466196\n",
      "train loss:0.02994266373475922\n",
      "train loss:0.006640083501307128\n",
      "train loss:0.00840607904584659\n",
      "train loss:0.015608099505242907\n",
      "train loss:0.011576891733339265\n",
      "train loss:0.05531043702658227\n",
      "train loss:0.014457093929477367\n",
      "train loss:0.006731297202028914\n",
      "train loss:0.020304704072226763\n",
      "train loss:0.00818432884541842\n",
      "train loss:0.0050056616561257325\n",
      "train loss:0.011368844750201194\n",
      "train loss:0.007637551377057853\n",
      "train loss:0.026312323908843797\n",
      "train loss:0.004952972404115476\n",
      "train loss:0.02232419498072364\n",
      "train loss:0.008595621631902521\n",
      "train loss:0.01638060526929024\n",
      "train loss:0.016720462969836477\n",
      "train loss:0.012644968104437142\n",
      "train loss:0.007155983392129858\n",
      "train loss:0.09295574561886977\n",
      "train loss:0.016613434570550936\n",
      "train loss:0.05620469497084163\n",
      "train loss:0.005995321921226065\n",
      "train loss:0.004525345337900261\n",
      "train loss:0.04888785092980913\n",
      "train loss:0.03409308765953353\n",
      "train loss:0.052296487206753436\n",
      "train loss:0.014531757948423197\n",
      "train loss:0.012971787884818307\n",
      "train loss:0.010534717372241393\n",
      "train loss:0.01649894217493996\n",
      "train loss:0.0236716584172031\n",
      "train loss:0.018309742708172917\n",
      "train loss:0.0356554313846564\n",
      "train loss:0.02929478235260798\n",
      "train loss:0.03308129573131311\n",
      "train loss:0.03341569465634869\n",
      "train loss:0.04221522363903632\n",
      "train loss:0.007267681518715693\n",
      "train loss:0.01190844729453958\n",
      "train loss:0.03740382460861035\n",
      "train loss:0.025172567050672435\n",
      "train loss:0.019334114051658448\n",
      "train loss:0.0160850883005227\n",
      "train loss:0.0313547912351108\n",
      "train loss:0.012019552534381972\n",
      "train loss:0.023681010473120212\n",
      "train loss:0.012139079126980402\n",
      "train loss:0.009189978122057303\n",
      "train loss:0.011892068757479517\n",
      "train loss:0.024335330950739375\n",
      "train loss:0.011759733131273457\n",
      "train loss:0.029486533625758208\n",
      "train loss:0.05679274227514163\n",
      "train loss:0.01415894547872948\n",
      "train loss:0.010655161190826679\n",
      "train loss:0.00924250271590269\n",
      "train loss:0.027586417302417815\n",
      "train loss:0.020506961900086093\n",
      "train loss:0.014879564408429975\n",
      "train loss:0.03993522598041293\n",
      "train loss:0.023329859783684005\n",
      "train loss:0.017937151292422748\n",
      "train loss:0.014212986565325705\n",
      "train loss:0.01190330291727622\n",
      "train loss:0.013846725851462564\n",
      "train loss:0.07583483491325191\n",
      "train loss:0.02882156493963252\n",
      "train loss:0.03385665202442963\n",
      "train loss:0.01698326717229961\n",
      "train loss:0.025804615808086503\n",
      "train loss:0.010182873835939694\n",
      "train loss:0.03526805098851264\n",
      "train loss:0.004309440930351334\n",
      "train loss:0.07311358373580731\n",
      "train loss:0.007943143578642621\n",
      "train loss:0.020021680410345036\n",
      "train loss:0.011263630661565011\n",
      "train loss:0.01041685456316943\n",
      "train loss:0.01958814192072758\n",
      "train loss:0.005289008572399417\n",
      "train loss:0.02282565251276852\n",
      "train loss:0.020140227910641243\n",
      "train loss:0.06575754944852565\n",
      "train loss:0.07986480779148723\n",
      "train loss:0.029301147492989302\n",
      "train loss:0.007079010144848438\n",
      "train loss:0.015308361255712697\n",
      "train loss:0.017253769213192372\n",
      "train loss:0.009765062859334248\n",
      "train loss:0.025906186711226114\n",
      "train loss:0.015469188336762252\n",
      "train loss:0.013570711202960972\n",
      "train loss:0.012198024869074151\n",
      "train loss:0.004755122392896481\n",
      "train loss:0.08200524415219618\n",
      "train loss:0.018670249764332382\n",
      "train loss:0.006904820389596937\n",
      "train loss:0.001000317922061876\n",
      "train loss:0.025116841776691223\n",
      "train loss:0.002651405627152157\n",
      "train loss:0.029539045981368788\n",
      "train loss:0.06919621224763231\n",
      "train loss:0.01942734390414191\n",
      "train loss:0.014117160003107919\n",
      "train loss:0.015358251837951855\n",
      "train loss:0.034629162754423855\n",
      "train loss:0.007657386026227926\n",
      "train loss:0.04092965709615679\n",
      "train loss:0.02556703383064488\n",
      "train loss:0.06506971865372373\n",
      "train loss:0.01868268754840506\n",
      "train loss:0.001997519791368282\n",
      "train loss:0.07727874752825616\n",
      "train loss:0.03227537843961324\n",
      "train loss:0.002213651091524475\n",
      "train loss:0.014069466823770561\n",
      "train loss:0.04645072506446844\n",
      "train loss:0.017363242813471392\n",
      "train loss:0.019657332568943895\n",
      "train loss:0.01359661772746938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.006853887696556046\n",
      "train loss:0.00912037525891835\n",
      "train loss:0.013406521793242641\n",
      "train loss:0.006755193823201737\n",
      "train loss:0.03816857657535632\n",
      "train loss:0.006771275479631283\n",
      "train loss:0.042078081257673\n",
      "train loss:0.010481869480335056\n",
      "train loss:0.007754373510133858\n",
      "train loss:0.0035870649424119207\n",
      "train loss:0.026325434625380763\n",
      "train loss:0.017806020108880365\n",
      "train loss:0.015907130046193062\n",
      "train loss:0.050674603514849155\n",
      "train loss:0.006753375042218121\n",
      "train loss:0.007013752693159462\n",
      "train loss:0.01636940617512936\n",
      "train loss:0.02530508174011284\n",
      "train loss:0.004030653172431824\n",
      "train loss:0.00425991630519989\n",
      "train loss:0.008915130350812164\n",
      "train loss:0.014657946910255903\n",
      "train loss:0.11952686612149915\n",
      "train loss:0.01052601592017152\n",
      "train loss:0.031199761730759196\n",
      "train loss:0.03497440640583329\n",
      "train loss:0.03673697803171896\n",
      "train loss:0.010656691549010644\n",
      "train loss:0.0183026023067561\n",
      "train loss:0.034214031939112724\n",
      "train loss:0.010603535424485612\n",
      "train loss:0.010343272803650612\n",
      "train loss:0.010428979095915001\n",
      "train loss:0.03650018669874001\n",
      "train loss:0.04405091693356696\n",
      "train loss:0.0031164457945807907\n",
      "train loss:0.06482412522430489\n",
      "train loss:0.019144657362893988\n",
      "train loss:0.027497233564832455\n",
      "train loss:0.011103435597359294\n",
      "train loss:0.0036426366243443886\n",
      "train loss:0.021278646191221356\n",
      "train loss:0.012283228155302885\n",
      "train loss:0.03964856148477116\n",
      "train loss:0.018717687931146716\n",
      "train loss:0.015390908761723531\n",
      "train loss:0.013245695590578926\n",
      "train loss:0.05119920106086115\n",
      "train loss:0.016559040455944234\n",
      "train loss:0.03243081026748633\n",
      "train loss:0.04144337007850133\n",
      "train loss:0.010066126650356545\n",
      "train loss:0.013691327221394519\n",
      "train loss:0.0030769442561905964\n",
      "train loss:0.02698668724360255\n",
      "train loss:0.014202919154055062\n",
      "train loss:0.01707498987568101\n",
      "train loss:0.020471270387628245\n",
      "train loss:0.041461787928638694\n",
      "train loss:0.013030836919342315\n",
      "train loss:0.009345979451466691\n",
      "train loss:0.027325890930609845\n",
      "train loss:0.04115351952819332\n",
      "train loss:0.0495087324288869\n",
      "train loss:0.026051938168120254\n",
      "train loss:0.01468501690577768\n",
      "train loss:0.049009577031332345\n",
      "train loss:0.026140444733325184\n",
      "train loss:0.015654249166598678\n",
      "train loss:0.00530347413953882\n",
      "train loss:0.05438675095371021\n",
      "train loss:0.02422726205035887\n",
      "train loss:0.01054386715658248\n",
      "train loss:0.08066176187035666\n",
      "train loss:0.015434511685855096\n",
      "train loss:0.01357826540523642\n",
      "train loss:0.0360684993474897\n",
      "train loss:0.023511011340348012\n",
      "train loss:0.03181761508729454\n",
      "train loss:0.005931233087482006\n",
      "train loss:0.014433053131242707\n",
      "train loss:0.011485713338860814\n",
      "train loss:0.013960555918381532\n",
      "train loss:0.013930088815987547\n",
      "train loss:0.07493485290517415\n",
      "train loss:0.014551470146428993\n",
      "train loss:0.014595519086291633\n",
      "train loss:0.038915674884818056\n",
      "train loss:0.02390063748394665\n",
      "train loss:0.019241220135195566\n",
      "train loss:0.04987879919911297\n",
      "train loss:0.010327268159249175\n",
      "train loss:0.09666918739144276\n",
      "train loss:0.02192193427630197\n",
      "train loss:0.03880865128178326\n",
      "train loss:0.08708986139774064\n",
      "train loss:0.03557140598035636\n",
      "train loss:0.04739879694868863\n",
      "train loss:0.01440592499571516\n",
      "train loss:0.0074344928852583085\n",
      "train loss:0.006883081640681566\n",
      "train loss:0.007852571054304543\n",
      "train loss:0.037941140824334744\n",
      "train loss:0.012303573504103939\n",
      "train loss:0.005573065357255703\n",
      "train loss:0.05117509283723742\n",
      "train loss:0.020461783930859602\n",
      "train loss:0.02063675815029016\n",
      "train loss:0.027483435495980455\n",
      "train loss:0.041619519541770886\n",
      "train loss:0.015676960021830347\n",
      "train loss:0.036680751235571615\n",
      "train loss:0.011296016960830862\n",
      "train loss:0.05166914899070847\n",
      "train loss:0.011367122119835087\n",
      "train loss:0.01844433348159316\n",
      "train loss:0.008399109894253689\n",
      "train loss:0.018701864348366695\n",
      "train loss:0.0036235950369550795\n",
      "train loss:0.0228841745074867\n",
      "train loss:0.040841367192049664\n",
      "train loss:0.00936499191954447\n",
      "train loss:0.012028790669999137\n",
      "train loss:0.021722593233839738\n",
      "train loss:0.031680408788594724\n",
      "train loss:0.03264550825785989\n",
      "train loss:0.019039828447183806\n",
      "train loss:0.006451714668828113\n",
      "train loss:0.0363082318244405\n",
      "train loss:0.0050007087984265\n",
      "train loss:0.014509260504470605\n",
      "train loss:0.00730744248374788\n",
      "train loss:0.011887323449258221\n",
      "train loss:0.016630427216447096\n",
      "train loss:0.009336810931554463\n",
      "train loss:0.057892688516068824\n",
      "train loss:0.08443573515634728\n",
      "train loss:0.12098138916965093\n",
      "train loss:0.007418107515117527\n",
      "train loss:0.01480150006764549\n",
      "train loss:0.0416229267434402\n",
      "train loss:0.02422039113481149\n",
      "train loss:0.019498332078480875\n",
      "train loss:0.011768621849703764\n",
      "train loss:0.006851523450176803\n",
      "train loss:0.03472861417428232\n",
      "train loss:0.006610387179679551\n",
      "train loss:0.020674731588411804\n",
      "train loss:0.005982912808617008\n",
      "train loss:0.003859586570473159\n",
      "train loss:0.006842967787723528\n",
      "train loss:0.0008291357223223143\n",
      "train loss:0.004738183431708583\n",
      "train loss:0.022981856459488307\n",
      "train loss:0.009046421918011983\n",
      "train loss:0.002815407187334539\n",
      "train loss:0.0037258327294021083\n",
      "train loss:0.015599729430281118\n",
      "train loss:0.00865468474053936\n",
      "train loss:0.0009792060654673718\n",
      "train loss:0.005426735941762433\n",
      "train loss:0.009791675609508358\n",
      "train loss:0.018482906107869734\n",
      "train loss:0.021791556151225584\n",
      "train loss:0.019750940297776864\n",
      "train loss:0.0014000174514406166\n",
      "train loss:0.03834907135992326\n",
      "train loss:0.015415209636643994\n",
      "train loss:0.015478753302276431\n",
      "train loss:0.04767007505998328\n",
      "train loss:0.0014722044274200997\n",
      "train loss:0.012515671943328985\n",
      "train loss:0.01702520293488338\n",
      "train loss:0.00797607877696248\n",
      "train loss:0.016348950673624973\n",
      "train loss:0.27282510246921843\n",
      "train loss:0.03052119448059755\n",
      "train loss:0.0077705654394935484\n",
      "train loss:0.0033225349392107207\n",
      "train loss:0.027120100695756794\n",
      "train loss:0.012976139777001652\n",
      "train loss:0.03729138809828747\n",
      "train loss:0.03230978661464148\n",
      "train loss:0.010862927747996624\n",
      "train loss:0.006561456624361965\n",
      "train loss:0.022627854149829107\n",
      "train loss:0.027424071075357204\n",
      "train loss:0.005620088130541173\n",
      "train loss:0.07742917309484527\n",
      "train loss:0.044715551356802725\n",
      "train loss:0.0301217021554747\n",
      "train loss:0.005241279175935764\n",
      "train loss:0.018667526724105078\n",
      "train loss:0.034862761507960485\n",
      "train loss:0.012688128264314648\n",
      "train loss:0.009250995231852183\n",
      "train loss:0.03505805080457816\n",
      "train loss:0.01114898829887943\n",
      "train loss:0.012390503890880408\n",
      "train loss:0.011550684890914528\n",
      "train loss:0.009980212437651272\n",
      "train loss:0.044490060175071386\n",
      "train loss:0.005028895622778164\n",
      "train loss:0.01474633487354999\n",
      "train loss:0.047259007070206686\n",
      "train loss:0.005112652222189237\n",
      "train loss:0.028052906194506653\n",
      "train loss:0.02463905238079613\n",
      "train loss:0.029917661963626184\n",
      "train loss:0.02638086780654561\n",
      "train loss:0.04884808400876068\n",
      "train loss:0.09462453535071166\n",
      "train loss:0.011759711061523166\n",
      "train loss:0.05703617388276233\n",
      "train loss:0.014195506004256932\n",
      "train loss:0.003939572851576031\n",
      "train loss:0.02031244032331046\n",
      "train loss:0.0075056832624185894\n",
      "train loss:0.10051908819165982\n",
      "train loss:0.028839649646940288\n",
      "train loss:0.012572805949443178\n",
      "train loss:0.0073594304605220226\n",
      "train loss:0.007373681655134298\n",
      "train loss:0.03215931462425837\n",
      "train loss:0.017267510271597907\n",
      "train loss:0.013049467758723779\n",
      "train loss:0.007039682718319549\n",
      "train loss:0.02672708577821028\n",
      "train loss:0.019219976255943155\n",
      "train loss:0.003336905573615142\n",
      "train loss:0.011139561813727408\n",
      "train loss:0.023458777587406218\n",
      "train loss:0.037929532668473015\n",
      "train loss:0.031496397399007094\n",
      "train loss:0.004740355264721309\n",
      "train loss:0.014721607334145802\n",
      "train loss:0.02168241333680392\n",
      "train loss:0.02167105975393665\n",
      "train loss:0.009288581910540678\n",
      "train loss:0.010162730721793108\n",
      "train loss:0.05870189736509634\n",
      "train loss:0.07749037936287491\n",
      "train loss:0.019277635765500303\n",
      "train loss:0.02230804598983548\n",
      "train loss:0.013102070183098714\n",
      "train loss:0.005903727753566401\n",
      "train loss:0.025635527735178187\n",
      "train loss:0.011910627464655927\n",
      "train loss:0.019583521920035384\n",
      "train loss:0.007630771319158747\n",
      "train loss:0.0027704891467608206\n",
      "train loss:0.008938840276345481\n",
      "train loss:0.04287555157125125\n",
      "train loss:0.014195954049155062\n",
      "train loss:0.03200928653748923\n",
      "train loss:0.05516244335399849\n",
      "train loss:0.017569109160630435\n",
      "train loss:0.020271788455142426\n",
      "train loss:0.005212068869903921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.004621952366548593\n",
      "train loss:0.04369374773453532\n",
      "train loss:0.017993323973701857\n",
      "train loss:0.0044032081668684215\n",
      "train loss:0.01199090793706401\n",
      "train loss:0.024107016709474776\n",
      "train loss:0.009669662400279326\n",
      "train loss:0.02301684885955003\n",
      "train loss:0.002085521583420509\n",
      "train loss:0.010597014743461648\n",
      "train loss:0.026512852008175103\n",
      "train loss:0.027579459389864867\n",
      "train loss:0.023010543989163884\n",
      "train loss:0.017454203086905092\n",
      "train loss:0.005549769313872027\n",
      "train loss:0.027130005110127496\n",
      "train loss:0.035918881177332364\n",
      "train loss:0.011515841387495132\n",
      "train loss:0.0015938068747791223\n",
      "train loss:0.01946007559431666\n",
      "train loss:0.004653008992751237\n",
      "train loss:0.005659511092294978\n",
      "train loss:0.03208709512329646\n",
      "train loss:0.05235590121569876\n",
      "train loss:0.050486443373066264\n",
      "train loss:0.0029293839327874586\n",
      "train loss:0.00798960982366787\n",
      "train loss:0.005563714931167988\n",
      "train loss:0.028531800242643696\n",
      "train loss:0.03718089348255826\n",
      "train loss:0.012896573588489436\n",
      "train loss:0.07048516711821863\n",
      "train loss:0.009342553259701188\n",
      "train loss:0.006466505008096054\n",
      "train loss:0.006134058423336341\n",
      "train loss:0.008956052357149967\n",
      "train loss:0.025915501298883193\n",
      "train loss:0.03468227396921109\n",
      "train loss:0.14650908559305972\n",
      "train loss:0.050481966627520754\n",
      "train loss:0.008346977432744119\n",
      "train loss:0.01088732373351324\n",
      "train loss:0.01226200261526949\n",
      "train loss:0.005990545299537175\n",
      "train loss:0.008375638604412856\n",
      "train loss:0.01656003829680191\n",
      "train loss:0.029598211985999677\n",
      "train loss:0.007556418314164027\n",
      "train loss:0.013907156470768844\n",
      "train loss:0.05333431278673644\n",
      "train loss:0.015184201446983124\n",
      "train loss:0.020613048264442924\n",
      "train loss:0.018384428249674893\n",
      "train loss:0.011876212370118076\n",
      "train loss:0.03691676442455036\n",
      "train loss:0.020577156252658414\n",
      "train loss:0.07882791387319181\n",
      "train loss:0.01940293752999021\n",
      "train loss:0.008659129105218377\n",
      "train loss:0.014189693812881805\n",
      "train loss:0.06417219426301451\n",
      "train loss:0.03295868500092895\n",
      "train loss:0.012742405328904895\n",
      "train loss:0.010867023265979278\n",
      "train loss:0.00848892665745129\n",
      "train loss:0.0042969037351504695\n",
      "train loss:0.017762983802882028\n",
      "train loss:0.013560431425223366\n",
      "train loss:0.011663853397336658\n",
      "train loss:0.040831283974284496\n",
      "train loss:0.006223999873786362\n",
      "train loss:0.01279116405616901\n",
      "train loss:0.012394948958469926\n",
      "train loss:0.02553898811714209\n",
      "train loss:0.012113180220391746\n",
      "train loss:0.009303826423788387\n",
      "train loss:0.01672850002614397\n",
      "train loss:0.03541090746924085\n",
      "train loss:0.010168675090362083\n",
      "train loss:0.002890597835343029\n",
      "train loss:0.005218068855002056\n",
      "train loss:0.022397039083658105\n",
      "train loss:0.0039363801288959535\n",
      "train loss:0.020636376666575886\n",
      "train loss:0.004849824944433898\n",
      "train loss:0.037273311486057274\n",
      "train loss:0.010021388710850287\n",
      "train loss:0.04308891987104538\n",
      "train loss:0.007715553850386438\n",
      "train loss:0.013000709049373348\n",
      "train loss:0.008379555814622233\n",
      "train loss:0.0035788048506339193\n",
      "train loss:0.026114620830171898\n",
      "train loss:0.04828442705958936\n",
      "train loss:0.02288999316149273\n",
      "train loss:0.009525790457223973\n",
      "train loss:0.054131488842854\n",
      "train loss:0.01921700402894459\n",
      "train loss:0.0024764404170411446\n",
      "train loss:0.07241406106728572\n",
      "train loss:0.003091173107390882\n",
      "train loss:0.006285832626853977\n",
      "train loss:0.0014583525590355795\n",
      "train loss:0.008505803439981918\n",
      "train loss:0.005266541091441605\n",
      "train loss:0.011122089108146351\n",
      "train loss:0.007743179979418657\n",
      "train loss:0.016466654314861443\n",
      "train loss:0.01855627422063035\n",
      "train loss:0.033867474294523275\n",
      "train loss:0.009961528895711199\n",
      "train loss:0.002420220595532405\n",
      "train loss:0.04398880307503785\n",
      "train loss:0.02619901413160901\n",
      "=== epoch:6, train acc:0.991, test acc:0.984 ===\n",
      "train loss:0.0048568074632893\n",
      "train loss:0.008645860547472635\n",
      "train loss:0.006427550509597906\n",
      "train loss:0.07210262712110363\n",
      "train loss:0.04082492672611893\n",
      "train loss:0.013897990831987107\n",
      "train loss:0.019285006217742706\n",
      "train loss:0.020121075223818927\n",
      "train loss:0.027608576846389125\n",
      "train loss:0.00800751524433949\n",
      "train loss:0.016943582311493347\n",
      "train loss:0.005423009972418485\n",
      "train loss:0.011414058105327808\n",
      "train loss:0.05980541472238201\n",
      "train loss:0.010285033666024006\n",
      "train loss:0.008051639773097256\n",
      "train loss:0.00742545351635\n",
      "train loss:0.015038344760223239\n",
      "train loss:0.012589706051912423\n",
      "train loss:0.021008503792167968\n",
      "train loss:0.03932291086386561\n",
      "train loss:0.008741098665714948\n",
      "train loss:0.010084779999513366\n",
      "train loss:0.11259245989183769\n",
      "train loss:0.03967083908180999\n",
      "train loss:0.021832253526248575\n",
      "train loss:0.013757801653184765\n",
      "train loss:0.007229867526792161\n",
      "train loss:0.03515853452687778\n",
      "train loss:0.022890207747304797\n",
      "train loss:0.021487239481003183\n",
      "train loss:0.019884485645364908\n",
      "train loss:0.026268646647987337\n",
      "train loss:0.20518217741251\n",
      "train loss:0.006862526477855222\n",
      "train loss:0.019808986462698745\n",
      "train loss:0.013924319878839015\n",
      "train loss:0.01799037667685531\n",
      "train loss:0.05796595255531564\n",
      "train loss:0.020292981033575172\n",
      "train loss:0.0958927039912863\n",
      "train loss:0.01286011642595001\n",
      "train loss:0.003352113909649401\n",
      "train loss:0.004137582717196394\n",
      "train loss:0.006262881771799646\n",
      "train loss:0.0018802050325989212\n",
      "train loss:0.012171976709714745\n",
      "train loss:0.008617584189135614\n",
      "train loss:0.025083362639535284\n",
      "train loss:0.0027291945964594017\n",
      "train loss:0.044107050435422755\n",
      "train loss:0.01639736877291875\n",
      "train loss:0.009222051929351414\n",
      "train loss:0.04674382181649949\n",
      "train loss:0.006566632847597283\n",
      "train loss:0.03918325518031474\n",
      "train loss:0.09299955391708904\n",
      "train loss:0.01742896245911608\n",
      "train loss:0.007663891130507189\n",
      "train loss:0.026317994312126575\n",
      "train loss:0.006636781673847182\n",
      "train loss:0.023482486412117608\n",
      "train loss:0.006432039167589908\n",
      "train loss:0.01150258714023567\n",
      "train loss:0.006518200213634017\n",
      "train loss:0.014381823593673964\n",
      "train loss:0.049198359215779544\n",
      "train loss:0.01641902487971606\n",
      "train loss:0.029775666523332994\n",
      "train loss:0.028741543155690327\n",
      "train loss:0.0032628683231843906\n",
      "train loss:0.002313318030983767\n",
      "train loss:0.05983694669636231\n",
      "train loss:0.03234882234832171\n",
      "train loss:0.019640106534933106\n",
      "train loss:0.04207725021583489\n",
      "train loss:0.0062586716266030615\n",
      "train loss:0.0022583267102590393\n",
      "train loss:0.027211846720739178\n",
      "train loss:0.033996163369338674\n",
      "train loss:0.00845733408574826\n",
      "train loss:0.013113663780655068\n",
      "train loss:0.006289407907203016\n",
      "train loss:0.040730390781879275\n",
      "train loss:0.00400362405231951\n",
      "train loss:0.006266887004535362\n",
      "train loss:0.029111021576786796\n",
      "train loss:0.02752925089360728\n",
      "train loss:0.013541524532185802\n",
      "train loss:0.005474586539819351\n",
      "train loss:0.010981423446783387\n",
      "train loss:0.009957442046031887\n",
      "train loss:0.008497515807877727\n",
      "train loss:0.012619391005370401\n",
      "train loss:0.007266542654754914\n",
      "train loss:0.0028892586583126032\n",
      "train loss:0.0024889918301943164\n",
      "train loss:0.005738141867396001\n",
      "train loss:0.010547061034673719\n",
      "train loss:0.06739388899533959\n",
      "train loss:0.012044882922393207\n",
      "train loss:0.009847494851122242\n",
      "train loss:0.04834129069917004\n",
      "train loss:0.04414054604233713\n",
      "train loss:0.002695503156618308\n",
      "train loss:0.007854099938502015\n",
      "train loss:0.007373937289432285\n",
      "train loss:0.015522773929702416\n",
      "train loss:0.00337026241728462\n",
      "train loss:0.01689821108850374\n",
      "train loss:0.012203744454838888\n",
      "train loss:0.015528130416501171\n",
      "train loss:0.006249802112322488\n",
      "train loss:0.024115004723753587\n",
      "train loss:0.01651026613000512\n",
      "train loss:0.02360595399306036\n",
      "train loss:0.015852343276921185\n",
      "train loss:0.009027326131067406\n",
      "train loss:0.012478558625632748\n",
      "train loss:0.01136276145744166\n",
      "train loss:0.032208918684876894\n",
      "train loss:0.026425171046340547\n",
      "train loss:0.008008802707941732\n",
      "train loss:0.010567560448220176\n",
      "train loss:0.03333391985542155\n",
      "train loss:0.00827884527040876\n",
      "train loss:0.006419186489700935\n",
      "train loss:0.02372154649326465\n",
      "train loss:0.019648405919058688\n",
      "train loss:0.05870868941489395\n",
      "train loss:0.005505799855945961\n",
      "train loss:0.007464250668449054\n",
      "train loss:0.0036256966710469306\n",
      "train loss:0.011665266527809377\n",
      "train loss:0.0013330368591178735\n",
      "train loss:0.014934051038395567\n",
      "train loss:0.008878362340942934\n",
      "train loss:0.06266364851369358\n",
      "train loss:0.008400698615657025\n",
      "train loss:0.0032798939542076997\n",
      "train loss:0.017043959820716294\n",
      "train loss:0.004985122533043106\n",
      "train loss:0.02474445910725923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.02083786962683244\n",
      "train loss:0.009580532799434436\n",
      "train loss:0.047223623727138876\n",
      "train loss:0.02276894279085739\n",
      "train loss:0.048835671871785696\n",
      "train loss:0.007234999456325743\n",
      "train loss:0.013668027698700751\n",
      "train loss:0.004707040099902801\n",
      "train loss:0.004404218654688785\n",
      "train loss:0.011856973213449373\n",
      "train loss:0.03435010949307979\n",
      "train loss:0.0049226741242077935\n",
      "train loss:0.0226697448751482\n",
      "train loss:0.005419230177109792\n",
      "train loss:0.0534212002720931\n",
      "train loss:0.02054667470262545\n",
      "train loss:0.012109781559433788\n",
      "train loss:0.10224753422302943\n",
      "train loss:0.003405494953228469\n",
      "train loss:0.009446884040570388\n",
      "train loss:0.02039748007438328\n",
      "train loss:0.05084638334486933\n",
      "train loss:0.06667709594542154\n",
      "train loss:0.015787281445855224\n",
      "train loss:0.04327018671932756\n",
      "train loss:0.035915858036283096\n",
      "train loss:0.03598764578448286\n",
      "train loss:0.05919360129860939\n",
      "train loss:0.019422555896898036\n",
      "train loss:0.008225943688426815\n",
      "train loss:0.021699815075181532\n",
      "train loss:0.008079135121342644\n",
      "train loss:0.0006690664938539384\n",
      "train loss:0.01791544910285612\n",
      "train loss:0.02228630673612917\n",
      "train loss:0.0029385926373076948\n",
      "train loss:0.01695673574788205\n",
      "train loss:0.01023754958168287\n",
      "train loss:0.024562279287098684\n",
      "train loss:0.005441338256875532\n",
      "train loss:0.0071857373592539575\n",
      "train loss:0.02631181859779879\n",
      "train loss:0.012708608786584136\n",
      "train loss:0.010470348155948404\n",
      "train loss:0.006206005156063742\n",
      "train loss:0.02257792742539932\n",
      "train loss:0.024013711219729818\n",
      "train loss:0.021094884261274118\n",
      "train loss:0.01966774655208049\n",
      "train loss:0.018171403247964273\n",
      "train loss:0.004005440556385354\n",
      "train loss:0.004315844330614801\n",
      "train loss:0.02750918812942444\n",
      "train loss:0.0054541273361262835\n",
      "train loss:0.025314423580173807\n",
      "train loss:0.011582258218077927\n",
      "train loss:0.028052216834567646\n",
      "train loss:0.007951306679677863\n",
      "train loss:0.012035793188170893\n",
      "train loss:0.03997532109811897\n",
      "train loss:0.0439189330495334\n",
      "train loss:0.00835821277734639\n",
      "train loss:0.006907187195496969\n",
      "train loss:0.04030947697976634\n",
      "train loss:0.008964788295525638\n",
      "train loss:0.030634729680630624\n",
      "train loss:0.0757582793900254\n",
      "train loss:0.0055290388228917174\n",
      "train loss:0.018370962502218816\n",
      "train loss:0.015026831162391077\n",
      "train loss:0.009276977508845428\n",
      "train loss:0.02734300695115648\n",
      "train loss:0.0057533274542184674\n",
      "train loss:0.004650230256612187\n",
      "train loss:0.018761230994140815\n",
      "train loss:0.0029227512769666826\n",
      "train loss:0.03284904749324076\n",
      "train loss:0.010597008012731324\n",
      "train loss:0.005556325019723009\n",
      "train loss:0.015739850149989985\n",
      "train loss:0.004345437800757506\n",
      "train loss:0.00487173182455649\n",
      "train loss:0.022326727366383463\n",
      "train loss:0.03347587682718725\n",
      "train loss:0.12571813288214467\n",
      "train loss:0.008911322788454724\n",
      "train loss:0.012588310445754973\n",
      "train loss:0.026490578685927364\n",
      "train loss:0.002687745467142064\n",
      "train loss:0.0010409148926768359\n",
      "train loss:0.006673140145985987\n",
      "train loss:0.0038474127751915482\n",
      "train loss:0.018550510519372714\n",
      "train loss:0.009950527121695179\n",
      "train loss:0.0473490810856108\n",
      "train loss:0.015364670995269433\n",
      "train loss:0.018320164752227065\n",
      "train loss:0.003773980386893607\n",
      "train loss:0.023998692148654636\n",
      "train loss:0.005832189927103939\n",
      "train loss:0.0026821683328569403\n",
      "train loss:0.03594519098569796\n",
      "train loss:0.008166867243617353\n",
      "train loss:0.0084252473804129\n",
      "train loss:0.017506221515685066\n",
      "train loss:0.0036920326498861146\n",
      "train loss:0.004316074568297782\n",
      "train loss:0.029825087688005437\n",
      "train loss:0.0010317520587139555\n",
      "train loss:0.0074940943417670615\n",
      "train loss:0.0026613526233107824\n",
      "train loss:0.024388529063088745\n",
      "train loss:0.006823723335161938\n",
      "train loss:0.0021411455908003636\n",
      "train loss:0.009726258864445335\n",
      "train loss:0.006061129765717221\n",
      "train loss:0.01281744453092369\n",
      "train loss:0.03466390415031963\n",
      "train loss:0.018299000190592357\n",
      "train loss:0.01504015809804757\n",
      "train loss:0.033814179364419834\n",
      "train loss:0.0010449914752099256\n",
      "train loss:0.026769408826506882\n",
      "train loss:0.05338741285226261\n",
      "train loss:0.004411925385288415\n",
      "train loss:0.023620094449777813\n",
      "train loss:0.024055886156883127\n",
      "train loss:0.10880088458297386\n",
      "train loss:0.006999468840440968\n",
      "train loss:0.015026528136127744\n",
      "train loss:0.006462091959860147\n",
      "train loss:0.01596834305635852\n",
      "train loss:0.013336380205959493\n",
      "train loss:0.06477743082046619\n",
      "train loss:0.003281705817927244\n",
      "train loss:0.009712218338731293\n",
      "train loss:0.04308552536978045\n",
      "train loss:0.0055249005704462485\n",
      "train loss:0.011417364675710235\n",
      "train loss:0.015939952106356917\n",
      "train loss:0.027030889223402875\n",
      "train loss:0.002682545417932852\n",
      "train loss:0.005967581841371311\n",
      "train loss:0.024178576598289373\n",
      "train loss:0.02766905280548325\n",
      "train loss:0.03043680834834775\n",
      "train loss:0.009535872134416421\n",
      "train loss:0.002985794146913471\n",
      "train loss:0.01310998187103242\n",
      "train loss:0.011786669171520318\n",
      "train loss:0.008715247290807412\n",
      "train loss:0.0408031604969594\n",
      "train loss:0.031154929040035455\n",
      "train loss:0.005784712695607228\n",
      "train loss:0.010447607276053174\n",
      "train loss:0.0828515667933765\n",
      "train loss:0.0038140374416678557\n",
      "train loss:0.006247776035133339\n",
      "train loss:0.062234309249794666\n",
      "train loss:0.0010302832727998607\n",
      "train loss:0.011330915021021928\n",
      "train loss:0.012070376042038528\n",
      "train loss:0.011905666720071081\n",
      "train loss:0.010703539782418083\n",
      "train loss:0.007363581708235983\n",
      "train loss:0.02884380990570019\n",
      "train loss:0.0028235611472024903\n",
      "train loss:0.044280449524919475\n",
      "train loss:0.018075903250905447\n",
      "train loss:0.011056118114142241\n",
      "train loss:0.028900820169409706\n",
      "train loss:0.007330231464350867\n",
      "train loss:0.05707407087354941\n",
      "train loss:0.0075161040362684475\n",
      "train loss:0.00838128348367891\n",
      "train loss:0.002753160051094093\n",
      "train loss:0.007464403684908667\n",
      "train loss:0.01914736459995037\n",
      "train loss:0.018667480611380204\n",
      "train loss:0.008820620071601703\n",
      "train loss:0.1035624887382403\n",
      "train loss:0.06089783515566424\n",
      "train loss:0.006593813234946851\n",
      "train loss:0.01179896501699966\n",
      "train loss:0.007528903447964403\n",
      "train loss:0.005002865466736679\n",
      "train loss:0.014732500872956017\n",
      "train loss:0.13224179375104084\n",
      "train loss:0.00926221747114057\n",
      "train loss:0.010212958135155098\n",
      "train loss:0.002179999741958575\n",
      "train loss:0.009529394055283772\n",
      "train loss:0.0381028269979519\n",
      "train loss:0.0108159958147858\n",
      "train loss:0.0015892257516278829\n",
      "train loss:0.035621058286732635\n",
      "train loss:0.010917722844419046\n",
      "train loss:0.008098761334051614\n",
      "train loss:0.034629703258593304\n",
      "train loss:0.005193580799047268\n",
      "train loss:0.02214757500927509\n",
      "train loss:0.0037267924332841447\n",
      "train loss:0.04331169050248246\n",
      "train loss:0.012454534857212587\n",
      "train loss:0.025070884140806035\n",
      "train loss:0.013586935931212112\n",
      "train loss:0.015232866029672802\n",
      "train loss:0.017586880258355482\n",
      "train loss:0.03192762190531116\n",
      "train loss:0.00789184849398903\n",
      "train loss:0.008154779864964674\n",
      "train loss:0.033740403102619096\n",
      "train loss:0.012356534531459828\n",
      "train loss:0.008279454871912012\n",
      "train loss:0.004996889791815024\n",
      "train loss:0.002905977050920986\n",
      "train loss:0.010142264335678353\n",
      "train loss:0.007075429075377505\n",
      "train loss:0.0036631334206152434\n",
      "train loss:0.04429950588720949\n",
      "train loss:0.02425733475204634\n",
      "train loss:0.035200060074983074\n",
      "train loss:0.01703098165290399\n",
      "train loss:0.01683195468667136\n",
      "train loss:0.022351058543119397\n",
      "train loss:0.002177259301137123\n",
      "train loss:0.021551671142894267\n",
      "train loss:0.05451117111351232\n",
      "train loss:0.00542063542765154\n",
      "train loss:0.011265071431578844\n",
      "train loss:0.01690738339662956\n",
      "train loss:0.03264717177855219\n",
      "train loss:0.009550529842099328\n",
      "train loss:0.021709080357832193\n",
      "train loss:0.019586805815710526\n",
      "train loss:0.011028821546524853\n",
      "train loss:0.01947439978851883\n",
      "train loss:0.01868350851543813\n",
      "train loss:0.003678136912191325\n",
      "train loss:0.014242027622845162\n",
      "train loss:0.031112732351986652\n",
      "train loss:0.005269716156272602\n",
      "train loss:0.0053197671008730915\n",
      "train loss:0.008561719800927226\n",
      "train loss:0.0022912012951484813\n",
      "train loss:0.0030473196081072327\n",
      "train loss:0.007659646893008305\n",
      "train loss:0.0041854821144108245\n",
      "train loss:0.09641076105538332\n",
      "train loss:0.0013924699225070435\n",
      "train loss:0.006016433803251818\n",
      "train loss:0.027555743121930775\n",
      "train loss:0.11030492716687432\n",
      "train loss:0.021648468976811004\n",
      "train loss:0.002158539502677364\n",
      "train loss:0.009428722128167285\n",
      "train loss:0.005220235451875091\n",
      "train loss:0.017911580271915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.022765416485082113\n",
      "train loss:0.005984245761776256\n",
      "train loss:0.011761493630344096\n",
      "train loss:0.00336962398444819\n",
      "train loss:0.022774295064506674\n",
      "train loss:0.053441644096973884\n",
      "train loss:0.03709782971393629\n",
      "train loss:0.01366705166608499\n",
      "train loss:0.01872976406286376\n",
      "train loss:0.0040113750297538844\n",
      "train loss:0.0023376771062272677\n",
      "train loss:0.07005492981889781\n",
      "train loss:0.03789943969812723\n",
      "train loss:0.0073678987428605\n",
      "train loss:0.008702748261666065\n",
      "train loss:0.014055969391455163\n",
      "train loss:0.0012505089425801433\n",
      "train loss:0.005328988505641719\n",
      "train loss:0.044878019856012585\n",
      "train loss:0.005185884098991111\n",
      "train loss:0.009367007818052431\n",
      "train loss:0.00611895020201903\n",
      "train loss:0.019251260975245696\n",
      "train loss:0.011248615089618547\n",
      "train loss:0.0052884865144458\n",
      "train loss:0.009158866047985327\n",
      "train loss:0.004073568845968208\n",
      "train loss:0.0011055214170491254\n",
      "train loss:0.0223478662958162\n",
      "train loss:0.008706210533134064\n",
      "train loss:0.007506012896116513\n",
      "train loss:0.006399357859020782\n",
      "train loss:0.024870819017896343\n",
      "train loss:0.025926037192169033\n",
      "train loss:0.01867583939666058\n",
      "train loss:0.01632130182288412\n",
      "train loss:0.013186797362458074\n",
      "train loss:0.004188461111166033\n",
      "train loss:0.02319063226216139\n",
      "train loss:0.015072805455372587\n",
      "train loss:0.026118233623577774\n",
      "train loss:0.008121834791691358\n",
      "train loss:0.007839038667027634\n",
      "train loss:0.007357263250081922\n",
      "train loss:0.054388290955296716\n",
      "train loss:0.022062485248016996\n",
      "train loss:0.003332793328069633\n",
      "train loss:0.06895021834355003\n",
      "train loss:0.06172779874622698\n",
      "train loss:0.009941953378810297\n",
      "train loss:0.05124053960279678\n",
      "train loss:0.013084777605699892\n",
      "train loss:0.012986083327879327\n",
      "train loss:0.026734845313403404\n",
      "train loss:0.02977238371010362\n",
      "train loss:0.00561297745400475\n",
      "train loss:0.04597938630012974\n",
      "train loss:0.012306926137694527\n",
      "train loss:0.01562227212063298\n",
      "train loss:0.0018222452332721606\n",
      "train loss:0.015250773141769845\n",
      "train loss:0.019763470028052815\n",
      "train loss:0.056199386257841114\n",
      "train loss:0.006368482166631419\n",
      "train loss:0.014511895790978946\n",
      "train loss:0.004913132022089533\n",
      "train loss:0.01136242816300985\n",
      "train loss:0.0009989892376669094\n",
      "train loss:0.013586818806814019\n",
      "train loss:0.012914904295359382\n",
      "train loss:0.00254932175610147\n",
      "train loss:0.0057752976164628265\n",
      "train loss:0.027852254745026127\n",
      "train loss:0.06197915211050509\n",
      "train loss:0.0054912969444292125\n",
      "train loss:0.006589646952033595\n",
      "train loss:0.02302341904146317\n",
      "train loss:0.01568944432784884\n",
      "train loss:0.004908236622403809\n",
      "train loss:0.017699912629145192\n",
      "train loss:0.003024833534461244\n",
      "train loss:0.003801569152814841\n",
      "train loss:0.02687857787600191\n",
      "train loss:0.03429264784061758\n",
      "train loss:0.016076951977504254\n",
      "train loss:0.01615628902735743\n",
      "train loss:0.02158740934335122\n",
      "train loss:0.07600651756987735\n",
      "train loss:0.0019809793284218235\n",
      "train loss:0.011947001228830675\n",
      "train loss:0.01620951808850547\n",
      "train loss:0.00463098997382721\n",
      "train loss:0.013869907808009502\n",
      "train loss:0.01774296828973085\n",
      "train loss:0.08205975037954931\n",
      "train loss:0.0061843919473349275\n",
      "train loss:0.045240217915605695\n",
      "train loss:0.004503765370059645\n",
      "train loss:0.008550615691867044\n",
      "train loss:0.006069018273213802\n",
      "train loss:0.01078867182804753\n",
      "train loss:0.045698093892365205\n",
      "train loss:0.04300245692193341\n",
      "train loss:0.00323510684935354\n",
      "train loss:0.0066989253730380385\n",
      "train loss:0.017596048550363563\n",
      "train loss:0.03025031141828621\n",
      "train loss:0.0028503270539981225\n",
      "train loss:0.0038906541478280323\n",
      "train loss:0.0008694723283725808\n",
      "train loss:0.03071179806640122\n",
      "train loss:0.014887529476394963\n",
      "train loss:0.006229910638336985\n",
      "train loss:0.0035461763200019232\n",
      "train loss:0.01417595902845779\n",
      "train loss:0.010709863380773533\n",
      "train loss:0.0241164054286522\n",
      "train loss:0.01071871114003005\n",
      "train loss:0.058302668351221464\n",
      "train loss:0.0033312312852152486\n",
      "train loss:0.07795843180696437\n",
      "train loss:0.0066657233610365315\n",
      "train loss:0.007897323095074935\n",
      "train loss:0.0045470528540616445\n",
      "train loss:0.001841233952688886\n",
      "train loss:0.007410476883082916\n",
      "train loss:0.007373607913501025\n",
      "train loss:0.007934593052573246\n",
      "train loss:0.01583589154913565\n",
      "train loss:0.0022504593515955858\n",
      "train loss:0.007480051302239978\n",
      "train loss:0.001801563170281517\n",
      "train loss:0.010848739071536058\n",
      "train loss:0.006777689766527796\n",
      "train loss:0.005460585584505142\n",
      "train loss:0.0029822356757268154\n",
      "train loss:0.01307059649155103\n",
      "train loss:0.0012882233199489293\n",
      "train loss:0.023775865843760853\n",
      "train loss:0.013383202461293637\n",
      "train loss:0.00957943988567746\n",
      "train loss:0.013357802116341749\n",
      "train loss:0.026821398570995334\n",
      "train loss:0.02942102512774683\n",
      "train loss:0.006653860952570453\n",
      "train loss:0.009357171543899008\n",
      "train loss:0.029877171441301583\n",
      "train loss:0.008601295252174176\n",
      "train loss:0.012990226456855372\n",
      "train loss:0.003230726544830926\n",
      "train loss:0.00556522328918736\n",
      "train loss:0.0062262728532296975\n",
      "train loss:0.013121235974261718\n",
      "train loss:0.03453958430347192\n",
      "train loss:0.007953371936994056\n",
      "train loss:0.035445702419894094\n",
      "train loss:0.01698145123617404\n",
      "train loss:0.011255508048232223\n",
      "train loss:0.006769962162321241\n",
      "train loss:0.005518716651918087\n",
      "train loss:0.00859330831314249\n",
      "train loss:0.0406402361433655\n",
      "train loss:0.005704988979373276\n",
      "train loss:0.015699824988050178\n",
      "train loss:0.0018761097790222262\n",
      "train loss:0.015843674532806846\n",
      "train loss:0.006218256893068323\n",
      "train loss:0.004072961098040714\n",
      "train loss:0.0011431886549764755\n",
      "train loss:0.033186782552811954\n",
      "train loss:0.014611089658155647\n",
      "train loss:0.09258229556680146\n",
      "train loss:0.006733461613299308\n",
      "train loss:0.0017590307912429967\n",
      "train loss:0.013763950675995486\n",
      "train loss:0.006142274089836769\n",
      "train loss:0.014584799620027924\n",
      "train loss:0.0036688216914493275\n",
      "train loss:0.033444675217879354\n",
      "train loss:0.007066933203212562\n",
      "train loss:0.003789486027029044\n",
      "train loss:0.03749770100188951\n",
      "train loss:0.0037552069368513435\n",
      "train loss:0.015935368097510016\n",
      "train loss:0.045355878633186616\n",
      "train loss:0.005552404857303064\n",
      "train loss:0.02218688377041509\n",
      "train loss:0.002323832893110941\n",
      "train loss:0.0027538841160506915\n",
      "train loss:0.005543797525277048\n",
      "train loss:0.01719932498245416\n",
      "train loss:0.0065748860831291445\n",
      "train loss:0.011271226353033586\n",
      "train loss:0.010674415238416645\n",
      "train loss:0.002789488877571875\n",
      "train loss:0.02613608807957762\n",
      "train loss:0.02065236568693177\n",
      "train loss:0.05110407132291205\n",
      "train loss:0.0034055973727320833\n",
      "=== epoch:7, train acc:0.991, test acc:0.987 ===\n",
      "train loss:0.028415688936661684\n",
      "train loss:0.010838036155345588\n",
      "train loss:0.005499980571069137\n",
      "train loss:0.00608692984544695\n",
      "train loss:0.012112524532271437\n",
      "train loss:0.015560023395402342\n",
      "train loss:0.002737385806705204\n",
      "train loss:0.025298073545779082\n",
      "train loss:0.008199813833333555\n",
      "train loss:0.006958936545397947\n",
      "train loss:0.0072048236129709205\n",
      "train loss:0.005424684808032092\n",
      "train loss:0.0030305744839463876\n",
      "train loss:0.004766688991903825\n",
      "train loss:0.037900684757972335\n",
      "train loss:0.030499737589092826\n",
      "train loss:0.002065485123336732\n",
      "train loss:0.012805832552870664\n",
      "train loss:0.006984414286905982\n",
      "train loss:0.008117403838743683\n",
      "train loss:0.02437007222336888\n",
      "train loss:0.02703485254380332\n",
      "train loss:0.015991644937819\n",
      "train loss:0.06915079254290042\n",
      "train loss:0.006585657479397565\n",
      "train loss:0.020767922916957872\n",
      "train loss:0.024397870093794984\n",
      "train loss:0.0033446025439502368\n",
      "train loss:0.0352973864209073\n",
      "train loss:0.005952670643275089\n",
      "train loss:0.014275297941983355\n",
      "train loss:0.015525167497984205\n",
      "train loss:0.017090674387977477\n",
      "train loss:0.0059985368638789975\n",
      "train loss:0.00503571152098492\n",
      "train loss:0.05343240945347949\n",
      "train loss:0.01230619552461326\n",
      "train loss:0.009373378709403028\n",
      "train loss:0.012032798423744294\n",
      "train loss:0.005623283238221543\n",
      "train loss:0.023588128270183042\n",
      "train loss:0.004792881439953476\n",
      "train loss:0.020107213665804716\n",
      "train loss:0.0021465313939379883\n",
      "train loss:0.008490854503423089\n",
      "train loss:0.0032599782523765356\n",
      "train loss:0.004962619396858268\n",
      "train loss:0.01806420008314694\n",
      "train loss:0.05130394016330705\n",
      "train loss:0.013210806751370405\n",
      "train loss:0.10208960045725934\n",
      "train loss:0.010454099571054198\n",
      "train loss:0.04680334175391196\n",
      "train loss:0.009905582548586745\n",
      "train loss:0.03665001954650211\n",
      "train loss:0.010067847993375536\n",
      "train loss:0.023352280926098774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.002034504820429316\n",
      "train loss:0.06469665565647749\n",
      "train loss:0.017211596411396843\n",
      "train loss:0.0292497030857306\n",
      "train loss:0.015528756056850097\n",
      "train loss:0.0023021311825850554\n",
      "train loss:0.10717660057306247\n",
      "train loss:0.009667940998800107\n",
      "train loss:0.02335849903633943\n",
      "train loss:0.003346238762822945\n",
      "train loss:0.04126001694170427\n",
      "train loss:0.00636040941717339\n",
      "train loss:0.033316521531138656\n",
      "train loss:0.006935142926372324\n",
      "train loss:0.00871271475014976\n",
      "train loss:0.002510590113133073\n",
      "train loss:0.007386868717309729\n",
      "train loss:0.012332383152236887\n",
      "train loss:0.005131690669947997\n",
      "train loss:0.0840326330817299\n",
      "train loss:0.021261538379220405\n",
      "train loss:0.003893418002576968\n",
      "train loss:0.029794776665403488\n",
      "train loss:0.022429516109217392\n",
      "train loss:0.01645124347458653\n",
      "train loss:0.003795004023801569\n",
      "train loss:0.00798496524012869\n",
      "train loss:0.009030325332682781\n",
      "train loss:0.013501929367310483\n",
      "train loss:0.006264017799677982\n",
      "train loss:0.0012511273340288936\n",
      "train loss:0.013220349101841328\n",
      "train loss:0.008294862964798729\n",
      "train loss:0.008079866452783045\n",
      "train loss:0.008991062225454833\n",
      "train loss:0.023056898182740496\n",
      "train loss:0.013471607648242092\n",
      "train loss:0.011365013262564947\n",
      "train loss:0.0072246724903808094\n",
      "train loss:0.002986334082748125\n",
      "train loss:0.003550098304470831\n",
      "train loss:0.011975526050037098\n",
      "train loss:0.008729520592818639\n",
      "train loss:0.03481064738915108\n",
      "train loss:0.004522811589324475\n",
      "train loss:0.009919365328606586\n",
      "train loss:0.003886877250168046\n",
      "train loss:0.014063218593634165\n",
      "train loss:0.01231586064219756\n",
      "train loss:0.0019653198560127057\n",
      "train loss:0.0005974542516509843\n",
      "train loss:0.009212355034017662\n",
      "train loss:0.0027867411440123447\n",
      "train loss:0.008516520000697293\n",
      "train loss:0.01832634846408233\n",
      "train loss:0.006553930139091804\n",
      "train loss:0.001477330715033353\n",
      "train loss:0.0036606190582319383\n",
      "train loss:0.04626523912117035\n",
      "train loss:0.0007200804063220666\n",
      "train loss:0.004806722590524693\n",
      "train loss:0.0018547374608613293\n",
      "train loss:0.020805221499877295\n",
      "train loss:0.00886624333879056\n",
      "train loss:0.0016285179106985254\n",
      "train loss:0.0021009048554185023\n",
      "train loss:0.008150898615062718\n",
      "train loss:0.004126535096240636\n",
      "train loss:0.011145495792529617\n",
      "train loss:0.001430266740677157\n",
      "train loss:0.0023521973822440965\n",
      "train loss:0.004426865209052413\n",
      "train loss:0.002267214901263772\n",
      "train loss:0.006198215985972772\n",
      "train loss:0.010332068634790602\n",
      "train loss:0.005499730764024652\n",
      "train loss:0.005359031977905921\n",
      "train loss:0.032839040386112156\n",
      "train loss:0.01291456896847495\n",
      "train loss:0.0036993133036723885\n",
      "train loss:0.006821911160083233\n",
      "train loss:0.00118365131518386\n",
      "train loss:0.005321451511975692\n",
      "train loss:0.061848040230083064\n",
      "train loss:0.0017344876055109559\n",
      "train loss:0.007879614337758247\n",
      "train loss:0.004870838007889774\n",
      "train loss:0.012457858973913036\n",
      "train loss:0.004190873338452385\n",
      "train loss:0.012468407555055301\n",
      "train loss:0.0148218318310615\n",
      "train loss:0.03194991497132185\n",
      "train loss:0.03727218403055524\n",
      "train loss:0.00484567127200969\n",
      "train loss:0.0024269361965600826\n",
      "train loss:0.028666441935563832\n",
      "train loss:0.010434376990731402\n",
      "train loss:0.014267850159213447\n",
      "train loss:0.08238130279479172\n",
      "train loss:0.004287405569962818\n",
      "train loss:0.012712303324619185\n",
      "train loss:0.0014549482970330316\n",
      "train loss:0.005224303048918426\n",
      "train loss:0.0013018378551359615\n",
      "train loss:0.011290219193402031\n",
      "train loss:0.037958917441547085\n",
      "train loss:0.009128292916957916\n",
      "train loss:0.0021215313939897153\n",
      "train loss:0.00651642912071613\n",
      "train loss:0.06380226399399716\n",
      "train loss:0.04911262680507954\n",
      "train loss:0.013158200968322806\n",
      "train loss:0.0027683473384584817\n",
      "train loss:0.0038752882494799963\n",
      "train loss:0.005494228219077225\n",
      "train loss:0.007645819705028495\n",
      "train loss:0.005588818606679836\n",
      "train loss:0.00483124786538449\n",
      "train loss:0.0006040140455301945\n",
      "train loss:0.004802092098564584\n",
      "train loss:0.004946744677320796\n",
      "train loss:0.000951046225164031\n",
      "train loss:0.009442587569849777\n",
      "train loss:0.00664161695785449\n",
      "train loss:0.0014118632175737216\n",
      "train loss:0.008415569304161609\n",
      "train loss:0.011218329318899028\n",
      "train loss:0.00874878992917268\n",
      "train loss:0.03567935445516017\n",
      "train loss:0.0025385652211689217\n",
      "train loss:0.008337329212876112\n",
      "train loss:0.009373681289955006\n",
      "train loss:0.006920482124356163\n",
      "train loss:0.01648740540777106\n",
      "train loss:0.01225692460859615\n",
      "train loss:0.04562335726134179\n",
      "train loss:0.003610274619311166\n",
      "train loss:0.017727060164062763\n",
      "train loss:0.0033665333445629725\n",
      "train loss:0.01610541066235448\n",
      "train loss:0.013175093983271375\n",
      "train loss:0.004293331262395049\n",
      "train loss:0.005904818427765504\n",
      "train loss:0.00495544434698338\n",
      "train loss:0.1105736409233076\n",
      "train loss:0.0034858775462606375\n",
      "train loss:0.019960650237313665\n",
      "train loss:0.003567259240953229\n",
      "train loss:0.05765104448100874\n",
      "train loss:0.008019168663986602\n",
      "train loss:0.02965314986691714\n",
      "train loss:0.002271887038803018\n",
      "train loss:0.008741119230114473\n",
      "train loss:0.05935812627341579\n",
      "train loss:0.04534616702104798\n",
      "train loss:0.009209569649825127\n",
      "train loss:0.02363281690968644\n",
      "train loss:0.007306348706174213\n",
      "train loss:0.005138894342343598\n",
      "train loss:0.018116710550132916\n",
      "train loss:0.013703392875981022\n",
      "train loss:0.0013622359537988921\n",
      "train loss:0.07075359447019604\n",
      "train loss:0.003527447310736733\n",
      "train loss:0.004543124189427988\n",
      "train loss:0.008196896502425503\n",
      "train loss:0.004010310496958498\n",
      "train loss:0.014365816287059352\n",
      "train loss:0.01319871015494684\n",
      "train loss:0.002305490646290107\n",
      "train loss:0.0006887332718216858\n",
      "train loss:0.007286635080570084\n",
      "train loss:0.008323542266871991\n",
      "train loss:0.006257832349955167\n",
      "train loss:0.0937341084992679\n",
      "train loss:0.07297230697555071\n",
      "train loss:0.009599301277021677\n",
      "train loss:0.01009664348435987\n",
      "train loss:0.012842086803704406\n",
      "train loss:0.005212836199948878\n",
      "train loss:0.02070934654809355\n",
      "train loss:0.009486561643810875\n",
      "train loss:0.01049235111376195\n",
      "train loss:0.011070436547652735\n",
      "train loss:0.003269298040028651\n",
      "train loss:0.05444914464529718\n",
      "train loss:0.00645623487501991\n",
      "train loss:0.001296299565546404\n",
      "train loss:0.007088976281145207\n",
      "train loss:0.005077170077193521\n",
      "train loss:0.008704055425500307\n",
      "train loss:0.008199769011309836\n",
      "train loss:0.03525357467896371\n",
      "train loss:0.0159091206382752\n",
      "train loss:0.021817778016131543\n",
      "train loss:0.001363879437314177\n",
      "train loss:0.04189541877319761\n",
      "train loss:0.002535644665493291\n",
      "train loss:0.003308024877469721\n",
      "train loss:0.0014949326887229192\n",
      "train loss:0.004389811606895232\n",
      "train loss:0.010101263021745934\n",
      "train loss:0.005916123988992372\n",
      "train loss:0.003586306623620187\n",
      "train loss:0.0036056564528402406\n",
      "train loss:0.0010534563791837715\n",
      "train loss:0.003898997956963376\n",
      "train loss:0.011905414652031359\n",
      "train loss:0.012033604883714603\n",
      "train loss:0.006647928010131335\n",
      "train loss:0.0034524230381767235\n",
      "train loss:0.0028102307011975158\n",
      "train loss:0.019265511277902207\n",
      "train loss:0.021277656788762617\n",
      "train loss:0.002081769695818447\n",
      "train loss:0.0034303045650616237\n",
      "train loss:0.004199358070523873\n",
      "train loss:0.005874327649632273\n",
      "train loss:0.0240763338492523\n",
      "train loss:0.002336914677337964\n",
      "train loss:0.020955739367602536\n",
      "train loss:0.00280149103112216\n",
      "train loss:0.0017653161954594401\n",
      "train loss:0.013467420931610765\n",
      "train loss:0.005434058114208174\n",
      "train loss:0.00355875657319586\n",
      "train loss:0.008423623601189777\n",
      "train loss:0.008998178738476468\n",
      "train loss:0.0033817864597622256\n",
      "train loss:0.0012845796045228358\n",
      "train loss:0.005086065011182566\n",
      "train loss:0.002520917651719948\n",
      "train loss:0.009483407153377081\n",
      "train loss:0.022906532747451514\n",
      "train loss:0.012117969336841427\n",
      "train loss:0.00585415922420248\n",
      "train loss:0.0056131572116988136\n",
      "train loss:0.0041252283750851555\n",
      "train loss:0.001789886325248393\n",
      "train loss:0.00477256394161497\n",
      "train loss:0.00888664510033241\n",
      "train loss:0.06470007767840232\n",
      "train loss:0.015528077620289817\n",
      "train loss:0.012438639107971794\n",
      "train loss:0.012971340210037818\n",
      "train loss:0.011981216772081867\n",
      "train loss:0.009169067549522528\n",
      "train loss:0.02597241011230337\n",
      "train loss:0.018787684429092757\n",
      "train loss:0.006870951234823477\n",
      "train loss:0.001988009515023151\n",
      "train loss:0.013434359758812013\n",
      "train loss:0.040209171338063655\n",
      "train loss:0.01603119205507789\n",
      "train loss:0.0025432830035832053\n",
      "train loss:0.01800808690379438\n",
      "train loss:0.005464018382954738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.006161885635897497\n",
      "train loss:0.024333201362745686\n",
      "train loss:0.006008413756708655\n",
      "train loss:0.007753171011193391\n",
      "train loss:0.0013165812902586728\n",
      "train loss:0.0621605223066714\n",
      "train loss:0.004011394968220768\n",
      "train loss:0.005937358003539846\n",
      "train loss:0.03849692731491789\n",
      "train loss:0.009632620397823515\n",
      "train loss:0.021690652670068363\n",
      "train loss:0.010949654930390152\n",
      "train loss:0.023372101789612056\n",
      "train loss:0.03885420200371152\n",
      "train loss:0.060368396533313655\n",
      "train loss:0.007432161603785237\n",
      "train loss:0.0020233177794327264\n",
      "train loss:0.011563724354436151\n",
      "train loss:0.036377316314338305\n",
      "train loss:0.018255232368244355\n",
      "train loss:0.013724102941327105\n",
      "train loss:0.0050214927501652795\n",
      "train loss:0.0009194392027521833\n",
      "train loss:0.11880766500670989\n",
      "train loss:0.003277841794691808\n",
      "train loss:0.012172885113119147\n",
      "train loss:0.0205219981689453\n",
      "train loss:0.013895134938313538\n",
      "train loss:0.016988122454672957\n",
      "train loss:0.01660957490503241\n",
      "train loss:0.020844267611597554\n",
      "train loss:0.031698259237729516\n",
      "train loss:0.0038628708829170065\n",
      "train loss:0.015783553285873463\n",
      "train loss:0.024002463619662247\n",
      "train loss:0.004277669716128548\n",
      "train loss:0.08830276654824334\n",
      "train loss:0.006308824229050345\n",
      "train loss:0.009004567263620483\n",
      "train loss:0.002819076434212522\n",
      "train loss:0.003810085349826666\n",
      "train loss:0.006852490578200071\n",
      "train loss:0.028758209724121877\n",
      "train loss:0.005642421171107802\n",
      "train loss:0.006496423016973685\n",
      "train loss:0.018804344808594088\n",
      "train loss:0.0046537114130991005\n",
      "train loss:0.004225972424223886\n",
      "train loss:0.0024846052485056006\n",
      "train loss:0.016435555492767765\n",
      "train loss:0.033222547351995535\n",
      "train loss:0.009360086977651633\n",
      "train loss:0.0023323065427562546\n",
      "train loss:0.01900644086449489\n",
      "train loss:0.01564006950491961\n",
      "train loss:0.0018654430965180544\n",
      "train loss:0.002536726080717942\n",
      "train loss:0.01094867218722254\n",
      "train loss:0.0008347349419291983\n",
      "train loss:0.0041539964078502216\n",
      "train loss:0.004045444695012435\n",
      "train loss:0.002030417988670785\n",
      "train loss:0.00603792576626364\n",
      "train loss:0.005796640362501292\n",
      "train loss:0.0024292796578006507\n",
      "train loss:0.009748450353022162\n",
      "train loss:0.000766575845498898\n",
      "train loss:0.0011233413274806712\n",
      "train loss:0.002995005814237855\n",
      "train loss:0.005185867733077599\n",
      "train loss:0.008225738701484466\n",
      "train loss:0.0036876485650553063\n",
      "train loss:0.005923093612229185\n",
      "train loss:0.004258959859617667\n",
      "train loss:0.0073281539156174556\n",
      "train loss:0.004744289707189309\n",
      "train loss:0.0008632679777457182\n",
      "train loss:0.014253847131007366\n",
      "train loss:0.003678323213563192\n",
      "train loss:0.010747496622434339\n",
      "train loss:0.003757840261504172\n",
      "train loss:0.009134495800486682\n",
      "train loss:0.001648585193968601\n",
      "train loss:0.00648476888714727\n",
      "train loss:0.01652136719454009\n",
      "train loss:0.07607959413320288\n",
      "train loss:0.022114226410649994\n",
      "train loss:0.006226489696547882\n",
      "train loss:0.014647854313088677\n",
      "train loss:0.0038299338540420166\n",
      "train loss:0.022918719486381623\n",
      "train loss:0.002250870074144658\n",
      "train loss:0.006617764818755327\n",
      "train loss:0.007171749180266645\n",
      "train loss:0.002308169256292345\n",
      "train loss:0.0027944486884116126\n",
      "train loss:0.006604247459069714\n",
      "train loss:0.055752265587901494\n",
      "train loss:0.002250701153948054\n",
      "train loss:0.007565036085014062\n",
      "train loss:0.014729707607972108\n",
      "train loss:0.009051294454613477\n",
      "train loss:0.003430294440508642\n",
      "train loss:0.008205805860708909\n",
      "train loss:0.0027049685872545253\n",
      "train loss:0.013736657055910166\n",
      "train loss:0.0053652565447316145\n",
      "train loss:0.00791788634168197\n",
      "train loss:0.04069607333926033\n",
      "train loss:0.004410561001378526\n",
      "train loss:0.0010917783424514555\n",
      "train loss:0.003542891659617241\n",
      "train loss:0.005069170281280023\n",
      "train loss:0.002407090882692435\n",
      "train loss:0.011319982952723302\n",
      "train loss:0.009101179670591458\n",
      "train loss:0.005641562024355521\n",
      "train loss:0.0024696495056165716\n",
      "train loss:0.0036548270306743236\n",
      "train loss:0.07973448625006031\n",
      "train loss:0.004695636491871773\n",
      "train loss:0.0053416225514649005\n",
      "train loss:0.0015358608710828025\n",
      "train loss:0.001934506470977951\n",
      "train loss:0.021532153720366275\n",
      "train loss:0.012532944725258885\n",
      "train loss:0.06407718150385872\n",
      "train loss:0.0020652927853144955\n",
      "train loss:0.00392152573944317\n",
      "train loss:0.00575383004352626\n",
      "train loss:0.0032844950158617205\n",
      "train loss:0.01726899991772063\n",
      "train loss:0.005625441998588488\n",
      "train loss:0.006565728202377178\n",
      "train loss:0.01721668284498484\n",
      "train loss:0.0022744165070123622\n",
      "train loss:0.0032959277398071584\n",
      "train loss:0.002117384729728222\n",
      "train loss:0.023967576488923288\n",
      "train loss:0.0012038435337791108\n",
      "train loss:0.019956831249148568\n",
      "train loss:0.0041738819547853506\n",
      "train loss:0.0015609243849968033\n",
      "train loss:0.002842216620620076\n",
      "train loss:0.012826708343416487\n",
      "train loss:0.012306072621974987\n",
      "train loss:0.0089749306831124\n",
      "train loss:0.0015156757298382573\n",
      "train loss:0.006523129482057505\n",
      "train loss:0.009576116129375298\n",
      "train loss:0.04043310139009524\n",
      "train loss:0.049836848661349256\n",
      "train loss:0.006592261624623731\n",
      "train loss:0.001987482879842028\n",
      "train loss:0.00927646644190068\n",
      "train loss:0.00810577838262801\n",
      "train loss:0.004967346501554509\n",
      "train loss:0.0019369327499910262\n",
      "train loss:0.0073780632224107875\n",
      "train loss:0.0012737516498585592\n",
      "train loss:0.0009373892394745381\n",
      "train loss:0.006150599588947421\n",
      "train loss:0.007071647835303686\n",
      "train loss:0.005288288076153124\n",
      "train loss:0.02731488646279174\n",
      "train loss:0.0062439207331560186\n",
      "train loss:0.04365177151085697\n",
      "train loss:0.003508680623101735\n",
      "train loss:0.005308336067526592\n",
      "train loss:0.005553685747834859\n",
      "train loss:0.0338560966881362\n",
      "train loss:0.01873892752882579\n",
      "train loss:0.0067169393518555585\n",
      "train loss:0.04312740114843924\n",
      "train loss:0.00839107106321458\n",
      "train loss:0.010026540143325905\n",
      "train loss:0.016564261589734917\n",
      "train loss:0.009265677577136774\n",
      "train loss:0.023016342099148362\n",
      "train loss:0.004389930715949733\n",
      "train loss:0.006452643432732525\n",
      "train loss:0.023522611008910854\n",
      "train loss:0.0016230450605760605\n",
      "train loss:0.003164817728778606\n",
      "train loss:0.012831215012612994\n",
      "train loss:0.027744214293608266\n",
      "train loss:0.002943844865762336\n",
      "train loss:0.01667303481230572\n",
      "train loss:0.009014488523228635\n",
      "train loss:0.016096638059148696\n",
      "train loss:0.004876461075888969\n",
      "train loss:0.0037662269642610375\n",
      "train loss:0.014368237447410863\n",
      "train loss:0.00496173234892249\n",
      "train loss:0.007152842226626239\n",
      "train loss:0.04679172584273281\n",
      "train loss:0.00693228328229577\n",
      "train loss:0.056530685003805044\n",
      "train loss:0.0034550641348287617\n",
      "train loss:0.012710626220540507\n",
      "train loss:0.011528500984643739\n",
      "train loss:0.004456427445934248\n",
      "train loss:0.06894618343282272\n",
      "train loss:0.005597823239338516\n",
      "train loss:0.0035625975952766707\n",
      "train loss:0.01597043337709073\n",
      "train loss:0.0038556761557922274\n",
      "train loss:0.0016214427831604521\n",
      "train loss:0.06284094915988686\n",
      "train loss:0.0006447366633809596\n",
      "train loss:0.009615873975110524\n",
      "train loss:0.004313222412164952\n",
      "train loss:0.00471207899500509\n",
      "train loss:0.005452703828182397\n",
      "train loss:0.009369131631204595\n",
      "train loss:0.0020732938803268734\n",
      "train loss:0.021845055823272026\n",
      "train loss:0.0024974393347090332\n",
      "train loss:0.023855779382560723\n",
      "train loss:0.03192980533302966\n",
      "train loss:0.003975113289137665\n",
      "train loss:0.006282757212830125\n",
      "train loss:0.004311947360035619\n",
      "train loss:0.008213616548108822\n",
      "train loss:0.020102454747305636\n",
      "train loss:0.004985513163902188\n",
      "train loss:0.0041940983100107795\n",
      "train loss:0.025236589477857552\n",
      "train loss:0.009463426393924708\n",
      "train loss:0.0022176050447146795\n",
      "train loss:0.0009589942504692816\n",
      "train loss:0.0033674881476782577\n",
      "train loss:0.00941150823084386\n",
      "train loss:0.0018108105109367028\n",
      "train loss:0.005112744735145999\n",
      "train loss:0.004990870654413421\n",
      "train loss:0.015408825737610477\n",
      "train loss:0.002643895146951615\n",
      "train loss:0.00531101080664589\n",
      "train loss:0.009765066555766143\n",
      "train loss:0.0018340261717899498\n",
      "train loss:0.0030882021084313017\n",
      "train loss:0.0030031144737535215\n",
      "train loss:0.007707636496362215\n",
      "train loss:0.011783271893099807\n",
      "train loss:0.007210668891722914\n",
      "train loss:0.04937528003450939\n",
      "train loss:0.01362012860630495\n",
      "train loss:0.0028286175679072627\n",
      "train loss:0.004572904319968124\n",
      "train loss:0.002006927421536906\n",
      "train loss:0.011934929528335641\n",
      "train loss:0.005620373507815774\n",
      "train loss:0.025367998549683496\n",
      "train loss:0.004032496041823828\n",
      "train loss:0.016248556477397466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.02658633109807226\n",
      "train loss:0.02052328788217588\n",
      "train loss:0.0046338765146985515\n",
      "train loss:0.004962448311308451\n",
      "train loss:0.021589054456236087\n",
      "train loss:0.05713705648858859\n",
      "train loss:0.010045811088460744\n",
      "train loss:0.0027800148577301078\n",
      "train loss:0.0022691843286382733\n",
      "train loss:0.002946481366460343\n",
      "train loss:0.0014424626522475089\n",
      "train loss:0.0035418378334462776\n",
      "train loss:0.01240441576371094\n",
      "train loss:0.00859873526801382\n",
      "train loss:0.0010826073705796475\n",
      "train loss:0.003072930910511437\n",
      "train loss:0.0013250772371712477\n",
      "train loss:0.00498428888431885\n",
      "train loss:0.1397925055049966\n",
      "train loss:0.020491766187755882\n",
      "train loss:0.0037420054899788553\n",
      "train loss:0.004147865242101494\n",
      "train loss:0.008737609002927272\n",
      "train loss:0.000698135831221242\n",
      "train loss:0.006910407727007083\n",
      "train loss:0.032860527507748226\n",
      "train loss:0.020752712129246384\n",
      "train loss:0.0077576430104311835\n",
      "train loss:0.005800015031464279\n",
      "train loss:0.0004821093025536998\n",
      "=== epoch:8, train acc:0.989, test acc:0.984 ===\n",
      "train loss:0.006874023185874565\n",
      "train loss:0.007575799331431963\n",
      "train loss:0.003598330382090573\n",
      "train loss:0.0563591861842778\n",
      "train loss:0.02614816356472931\n",
      "train loss:0.011403928399033664\n",
      "train loss:0.0007615590811501471\n",
      "train loss:0.0015255703184330225\n",
      "train loss:0.0033896960891393164\n",
      "train loss:0.004051379542599519\n",
      "train loss:0.0053540548187364515\n",
      "train loss:0.0014265670362877964\n",
      "train loss:0.008627303700525312\n",
      "train loss:0.016262855082993497\n",
      "train loss:0.01912393460830524\n",
      "train loss:0.003802226995221294\n",
      "train loss:0.016085414557788266\n",
      "train loss:0.00842742394391921\n",
      "train loss:0.00880339433399188\n",
      "train loss:0.007717433222892753\n",
      "train loss:0.0044463693780247135\n",
      "train loss:0.0016991895453378034\n",
      "train loss:0.00506385637546778\n",
      "train loss:0.016743500429123183\n",
      "train loss:0.004508288546552763\n",
      "train loss:0.0022968025856993255\n",
      "train loss:0.000818704781271586\n",
      "train loss:0.004087030375630965\n",
      "train loss:0.0005554204956351753\n",
      "train loss:0.0022681062373262075\n",
      "train loss:0.003806116668350511\n",
      "train loss:0.010008716802149405\n",
      "train loss:0.01103876091638261\n",
      "train loss:0.002739778182529824\n",
      "train loss:0.007314455004867125\n",
      "train loss:0.01669637731271584\n",
      "train loss:0.0014611410037558645\n",
      "train loss:0.02960675951004261\n",
      "train loss:0.009988949582995678\n",
      "train loss:0.03515614698852777\n",
      "train loss:0.009016910462028134\n",
      "train loss:0.006437501198851236\n",
      "train loss:0.0006687374472525234\n",
      "train loss:0.006328467423251989\n",
      "train loss:0.003991233941611596\n",
      "train loss:0.007508131807132029\n",
      "train loss:0.005834120868637961\n",
      "train loss:0.003154804314652206\n",
      "train loss:0.005056440855303494\n",
      "train loss:0.029749612641746487\n",
      "train loss:0.006992476577603424\n",
      "train loss:0.0074410863316551525\n",
      "train loss:0.01041193678739677\n",
      "train loss:0.04304560555046416\n",
      "train loss:0.00815294680531762\n",
      "train loss:0.024241782813794074\n",
      "train loss:0.002939082188944156\n",
      "train loss:0.002104922025744911\n",
      "train loss:0.02941602172884247\n",
      "train loss:0.0014816589548365617\n",
      "train loss:0.009174994718037309\n",
      "train loss:0.0022160691787904007\n",
      "train loss:0.0015682991558756084\n",
      "train loss:0.0027095741476827217\n",
      "train loss:0.007970654293609835\n",
      "train loss:0.006431510100587905\n",
      "train loss:0.001814866720886824\n",
      "train loss:0.007636398789494279\n",
      "train loss:0.002735404685743154\n",
      "train loss:0.006294224947715818\n",
      "train loss:0.006915116921545236\n",
      "train loss:0.0061783520458476805\n",
      "train loss:0.005830506127354722\n",
      "train loss:0.003893862971895003\n",
      "train loss:0.0100697628621544\n",
      "train loss:0.03491398361671012\n",
      "train loss:0.017208512588421127\n",
      "train loss:0.011017793906098408\n",
      "train loss:0.0016110941284898186\n",
      "train loss:0.009655434044377268\n",
      "train loss:0.0025349841120927614\n",
      "train loss:0.026267347384801864\n",
      "train loss:0.010298791262860105\n",
      "train loss:0.005591683853040548\n",
      "train loss:0.016749769249041327\n",
      "train loss:0.002703548981966258\n",
      "train loss:0.0016226099292328022\n",
      "train loss:0.004001188243006417\n",
      "train loss:0.0017023332303337654\n",
      "train loss:0.0015978716192292888\n",
      "train loss:0.003586637835977022\n",
      "train loss:0.015262055743752865\n",
      "train loss:0.0206047091629616\n",
      "train loss:0.05111267709573476\n",
      "train loss:0.05863628969480776\n",
      "train loss:0.006814852923372508\n",
      "train loss:0.01169719087506488\n",
      "train loss:0.00562015962229592\n",
      "train loss:0.004748553943435344\n",
      "train loss:0.0048434150520275795\n",
      "train loss:0.014547130132470078\n",
      "train loss:0.008076407508838042\n",
      "train loss:0.007949166865754389\n",
      "train loss:0.013520507492330922\n",
      "train loss:0.005014770238082727\n",
      "train loss:0.008462594355996288\n",
      "train loss:0.005141496998727243\n",
      "train loss:0.0585486733502851\n",
      "train loss:0.001981952389990176\n",
      "train loss:0.019768225211973592\n",
      "train loss:0.004737428350765305\n",
      "train loss:0.024366093226592684\n",
      "train loss:0.003823145800321836\n",
      "train loss:0.03576698107609527\n",
      "train loss:0.005035071201251299\n",
      "train loss:0.04183118901245922\n",
      "train loss:0.011805715054577746\n",
      "train loss:0.008205032682833012\n",
      "train loss:0.003891816848856003\n",
      "train loss:0.014342249208635997\n",
      "train loss:0.0037923583768531303\n",
      "train loss:0.005636586329422048\n",
      "train loss:0.0010098519513088375\n",
      "train loss:0.012835550915367422\n",
      "train loss:0.03608951952863795\n",
      "train loss:0.002297177115662551\n",
      "train loss:0.002823259418190287\n",
      "train loss:0.007394698566592692\n",
      "train loss:0.008875033035894195\n",
      "train loss:0.0018062208476379911\n",
      "train loss:0.014616038129470014\n",
      "train loss:0.008607470318743381\n",
      "train loss:0.004464142868567581\n",
      "train loss:0.00604784554589598\n",
      "train loss:0.00644940822269228\n",
      "train loss:0.0048757073378171666\n",
      "train loss:0.011859181717512723\n",
      "train loss:0.05325147636685453\n",
      "train loss:0.00527225967735896\n",
      "train loss:0.0850829754946316\n",
      "train loss:0.007482821953215693\n",
      "train loss:0.002052509882565988\n",
      "train loss:0.014745930618712275\n",
      "train loss:0.003728893364728158\n",
      "train loss:0.0027290905909432813\n",
      "train loss:0.002888975137920836\n",
      "train loss:0.03739624855688238\n",
      "train loss:0.0009997081001786995\n",
      "train loss:0.001803173521897447\n",
      "train loss:0.07536482570190142\n",
      "train loss:0.002175205300571821\n",
      "train loss:0.03856667368726508\n",
      "train loss:0.026375440293715687\n",
      "train loss:0.00804273888698279\n",
      "train loss:0.002203471205882807\n",
      "train loss:0.025922788899486418\n",
      "train loss:0.0033657015434559122\n",
      "train loss:0.004512670964290916\n",
      "train loss:0.0009743015088149874\n",
      "train loss:0.0009501851301510186\n",
      "train loss:0.0037283630578371744\n",
      "train loss:0.014688163647775991\n",
      "train loss:0.021601628160322562\n",
      "train loss:0.003933639970056408\n",
      "train loss:0.006827120632743769\n",
      "train loss:0.004280413603566124\n",
      "train loss:0.005431187854214929\n",
      "train loss:0.0027916693933921414\n",
      "train loss:0.003601653600493064\n",
      "train loss:0.0028830059479835277\n",
      "train loss:0.001716556587160016\n",
      "train loss:0.0007630499872795143\n",
      "train loss:0.012821056868308112\n",
      "train loss:0.025797651495512545\n",
      "train loss:0.0015342093156918422\n",
      "train loss:0.0062112397407986545\n",
      "train loss:0.004975058784309139\n",
      "train loss:0.04115725243362596\n",
      "train loss:0.0026937747352916543\n",
      "train loss:0.010261407808273696\n",
      "train loss:0.029893931819208817\n",
      "train loss:0.10305288963337537\n",
      "train loss:0.0035715861069468813\n",
      "train loss:0.0028751594444933293\n",
      "train loss:0.004866897623566261\n",
      "train loss:0.005072832081211014\n",
      "train loss:0.0039680368595230615\n",
      "train loss:0.0025402424562548614\n",
      "train loss:0.027962120385700985\n",
      "train loss:0.08273252239571752\n",
      "train loss:0.01594860566425007\n",
      "train loss:0.0024767589689132403\n",
      "train loss:0.0059723863796393006\n",
      "train loss:0.0075810553234587855\n",
      "train loss:0.014085573366111558\n",
      "train loss:0.003748814326288054\n",
      "train loss:0.004563143364155248\n",
      "train loss:0.004679559563963046\n",
      "train loss:0.0012084759763218498\n",
      "train loss:0.0038434389426706735\n",
      "train loss:0.0024060182822043755\n",
      "train loss:0.008478365417993175\n",
      "train loss:0.0028725360393260023\n",
      "train loss:0.0017066593870158082\n",
      "train loss:0.006554399965007159\n",
      "train loss:0.009249304725047976\n",
      "train loss:0.07243917722226256\n",
      "train loss:0.008008336033629973\n",
      "train loss:0.0033538814269997944\n",
      "train loss:0.008254696641631722\n",
      "train loss:0.010612040740243713\n",
      "train loss:0.013058726767503144\n",
      "train loss:0.00787379804679686\n",
      "train loss:0.01863209822435783\n",
      "train loss:0.004385306232892278\n",
      "train loss:0.013765983815274418\n",
      "train loss:0.03526853674898742\n",
      "train loss:0.005522891272567902\n",
      "train loss:0.03248872916511179\n",
      "train loss:0.003943186573323488\n",
      "train loss:0.0031642361285144334\n",
      "train loss:0.005188110030672221\n",
      "train loss:0.01889443114182466\n",
      "train loss:0.008142752645642239\n",
      "train loss:0.016699057820240237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.004189254380639812\n",
      "train loss:0.0075913539771113765\n",
      "train loss:0.008653134896154041\n",
      "train loss:0.037973882075000645\n",
      "train loss:0.003752475779073898\n",
      "train loss:0.007711030060038906\n",
      "train loss:0.0049663302927919935\n",
      "train loss:0.0007233652150517376\n",
      "train loss:0.007450833741209831\n",
      "train loss:0.00709960717875906\n",
      "train loss:0.002577028729058816\n",
      "train loss:0.026240346617919585\n",
      "train loss:0.0028200885855690096\n",
      "train loss:0.0018990828516036904\n",
      "train loss:0.007867104725170548\n",
      "train loss:0.003015880579481633\n",
      "train loss:0.0013925164989738064\n",
      "train loss:0.0024920356870485392\n",
      "train loss:0.001973972365244634\n",
      "train loss:0.007161627914276595\n",
      "train loss:0.002961394906389521\n",
      "train loss:0.011856069199098984\n",
      "train loss:0.01360332793040864\n",
      "train loss:0.003996817746582223\n",
      "train loss:0.009894008546065787\n",
      "train loss:0.005847355439439445\n",
      "train loss:0.018378067949100477\n",
      "train loss:0.007292284463613702\n",
      "train loss:0.008798582564043797\n",
      "train loss:0.0028996868298429316\n",
      "train loss:0.008034285474588558\n",
      "train loss:0.013821727896542606\n",
      "train loss:0.004486086072539537\n",
      "train loss:0.00378320057926589\n",
      "train loss:0.0010288630841063268\n",
      "train loss:0.0068635334328932705\n",
      "train loss:0.002081126053121248\n",
      "train loss:0.0012782143422289506\n",
      "train loss:0.08228808580589617\n",
      "train loss:0.015518623277986929\n",
      "train loss:0.0201653391834266\n",
      "train loss:0.0042932365583832925\n",
      "train loss:0.011430537479271077\n",
      "train loss:0.020099315141806063\n",
      "train loss:0.002519587992733988\n",
      "train loss:0.04749755430080261\n",
      "train loss:0.005055557228841457\n",
      "train loss:0.005518766713426892\n",
      "train loss:0.018238803782918846\n",
      "train loss:0.02987821595133873\n",
      "train loss:0.005156074178115739\n",
      "train loss:0.010062732856729675\n",
      "train loss:0.003974216976013817\n",
      "train loss:0.005982358708518093\n",
      "train loss:0.008112393774295406\n",
      "train loss:0.02523242395281764\n",
      "train loss:0.03693864172897281\n",
      "train loss:0.02004653875339539\n",
      "train loss:0.0012891661590453587\n",
      "train loss:0.0005135230482090375\n",
      "train loss:0.014019994961587744\n",
      "train loss:0.00113407037118393\n",
      "train loss:0.0038256123327570805\n",
      "train loss:0.00773424445401465\n",
      "train loss:0.003528886873103693\n",
      "train loss:0.000683428133626465\n",
      "train loss:0.005423570117515275\n",
      "train loss:0.005041107820316773\n",
      "train loss:0.02233155335497804\n",
      "train loss:0.01832952073823453\n",
      "train loss:0.011232991172422097\n",
      "train loss:0.0008326300615292311\n",
      "train loss:0.002694744867509176\n",
      "train loss:0.003188300267687512\n",
      "train loss:0.017735111759469308\n",
      "train loss:0.03713626854956343\n",
      "train loss:0.009720059903736228\n",
      "train loss:0.010383197428291127\n",
      "train loss:0.03321364844062641\n",
      "train loss:0.0011018544871518156\n",
      "train loss:0.06142258081621768\n",
      "train loss:0.006724900538006982\n",
      "train loss:0.025720936110065713\n",
      "train loss:0.019704141510492598\n",
      "train loss:0.016687819392143782\n",
      "train loss:0.004798503183474645\n",
      "train loss:0.0044881226189769155\n",
      "train loss:0.014051570504895734\n",
      "train loss:0.004682080789403338\n",
      "train loss:0.007816288010070295\n",
      "train loss:0.006113052405573043\n",
      "train loss:0.01961788283515123\n",
      "train loss:0.004691261069594988\n",
      "train loss:0.0030016993316397816\n",
      "train loss:0.002558624830318746\n",
      "train loss:0.014029413854370175\n",
      "train loss:0.017780874244941087\n",
      "train loss:0.004767477583905584\n",
      "train loss:0.003303916738125093\n",
      "train loss:0.0010935127185268795\n",
      "train loss:0.014179996203557017\n",
      "train loss:0.00536284220640369\n",
      "train loss:0.024779052364173212\n",
      "train loss:0.0037588251861294625\n",
      "train loss:0.003421148934088779\n",
      "train loss:0.0018078986652727605\n",
      "train loss:0.00538362553260273\n",
      "train loss:0.005111854493313488\n",
      "train loss:0.007977829630537347\n",
      "train loss:0.004511902624470632\n",
      "train loss:0.002746486362572003\n",
      "train loss:0.001019631591644219\n",
      "train loss:0.007096767154708468\n",
      "train loss:0.004275372791222278\n",
      "train loss:0.0033808513799604163\n",
      "train loss:0.010026409620264663\n",
      "train loss:0.005231129872174964\n",
      "train loss:0.02214499515761803\n",
      "train loss:0.001681810861690674\n",
      "train loss:0.015518006447166699\n",
      "train loss:0.0014124564636378283\n",
      "train loss:0.005361379843547186\n",
      "train loss:0.01716668062581334\n",
      "train loss:0.0011760144197234933\n",
      "train loss:0.005344461226549448\n",
      "train loss:0.04351080385941332\n",
      "train loss:0.016320171818877054\n",
      "train loss:0.003448924541356346\n",
      "train loss:0.0536634248768101\n",
      "train loss:0.018221057689391168\n",
      "train loss:0.01770078496283134\n",
      "train loss:0.0018433001984531556\n",
      "train loss:0.008093864124583166\n",
      "train loss:0.014148415999012771\n",
      "train loss:0.0036648228687507956\n",
      "train loss:0.005037376052382262\n",
      "train loss:0.00133102254649092\n",
      "train loss:0.004034580791215767\n",
      "train loss:0.0033984872296016332\n",
      "train loss:0.004657904723798747\n",
      "train loss:0.029573194211902732\n",
      "train loss:0.009243226638911145\n",
      "train loss:0.0022295663439194\n",
      "train loss:0.004218878421218989\n",
      "train loss:0.027683350124238976\n",
      "train loss:0.018141817565950097\n",
      "train loss:0.00939860465057616\n",
      "train loss:0.0031864957245472643\n",
      "train loss:0.01143541761954588\n",
      "train loss:0.009029422437558646\n",
      "train loss:0.017539515581316053\n",
      "train loss:0.0211942930006831\n",
      "train loss:0.011888700932743518\n",
      "train loss:0.009447724917982061\n",
      "train loss:0.0019343558144862428\n",
      "train loss:0.011386359884056422\n",
      "train loss:0.01797606029734143\n",
      "train loss:0.0026455618187560793\n",
      "train loss:0.015554033804566795\n",
      "train loss:0.007482391747597268\n",
      "train loss:0.004132225037363568\n",
      "train loss:0.005458693619400202\n",
      "train loss:0.014968419390514276\n",
      "train loss:0.024156569466662704\n",
      "train loss:0.005750029856532346\n",
      "train loss:0.0013219612922434293\n",
      "train loss:0.07657500682084657\n",
      "train loss:0.008178278272899355\n",
      "train loss:0.01592731137081474\n",
      "train loss:0.008220892682867522\n",
      "train loss:0.004388090963487998\n",
      "train loss:0.01545286135981824\n",
      "train loss:0.0054603416819807606\n",
      "train loss:0.007926652418042905\n",
      "train loss:0.004836586746077468\n",
      "train loss:0.0069312607227393665\n",
      "train loss:0.002361767058423358\n",
      "train loss:0.0005368838070081837\n",
      "train loss:0.004659844348276158\n",
      "train loss:0.0017589292915193226\n",
      "train loss:0.004803715573545853\n",
      "train loss:0.07097342756139816\n",
      "train loss:0.015465817004365905\n",
      "train loss:0.006929021909063123\n",
      "train loss:0.003272783314023943\n",
      "train loss:0.005762272718399512\n",
      "train loss:0.03134653449897488\n",
      "train loss:0.006200623980007152\n",
      "train loss:0.0027609714462608036\n",
      "train loss:0.003174791527221025\n",
      "train loss:0.003929950155923025\n",
      "train loss:0.0031710925291669194\n",
      "train loss:0.010265263127211939\n",
      "train loss:0.0015543554698163112\n",
      "train loss:0.0058239239410861465\n",
      "train loss:0.0053174261008905425\n",
      "train loss:0.025671451698803093\n",
      "train loss:0.005263465136843465\n",
      "train loss:0.00976877932831169\n",
      "train loss:0.009556712612980467\n",
      "train loss:0.02002491968500805\n",
      "train loss:0.031384267777333524\n",
      "train loss:0.005185547958910855\n",
      "train loss:0.003335822545200874\n",
      "train loss:0.0035539876358734158\n",
      "train loss:0.0025125569424888066\n",
      "train loss:0.0012087849704210292\n",
      "train loss:0.001090326791221485\n",
      "train loss:0.0013623081252454353\n",
      "train loss:0.0025366696652385514\n",
      "train loss:0.035060270255732884\n",
      "train loss:0.0012910493248099193\n",
      "train loss:0.004990882550879099\n",
      "train loss:0.002750081692232457\n",
      "train loss:0.0020679304032255393\n",
      "train loss:0.0022170022001850036\n",
      "train loss:0.006880654317187215\n",
      "train loss:0.006874779456281387\n",
      "train loss:0.002027831444020897\n",
      "train loss:0.01841097918236479\n",
      "train loss:0.0022394583909173312\n",
      "train loss:0.006148274647000748\n",
      "train loss:0.05086319381564757\n",
      "train loss:0.00973563305242079\n",
      "train loss:0.013170471013056953\n",
      "train loss:0.0044260164437145925\n",
      "train loss:0.004468319064465202\n",
      "train loss:0.0016123762693964704\n",
      "train loss:0.010252793923156584\n",
      "train loss:0.0120686940378469\n",
      "train loss:0.008142500126005337\n",
      "train loss:0.006720201164559706\n",
      "train loss:0.0030494605221102877\n",
      "train loss:0.018603024699658098\n",
      "train loss:0.0013160989154162716\n",
      "train loss:0.003104529714242395\n",
      "train loss:0.0038172460167389542\n",
      "train loss:0.008563116852744884\n",
      "train loss:0.10180691521431412\n",
      "train loss:0.010571595030950832\n",
      "train loss:0.006071915426203455\n",
      "train loss:0.011369667688745084\n",
      "train loss:0.021020749239119813\n",
      "train loss:0.003927986182106245\n",
      "train loss:0.0014285979942072378\n",
      "train loss:0.008401467737505103\n",
      "train loss:0.0023951041758427986\n",
      "train loss:0.005949866171169121\n",
      "train loss:0.05118541864335107\n",
      "train loss:0.002739871185818982\n",
      "train loss:0.004125218866604211\n",
      "train loss:0.002938880866993853\n",
      "train loss:0.0003723208171923871\n",
      "train loss:0.0032600421640126844\n",
      "train loss:0.0008518606633525158\n",
      "train loss:0.003073171284330793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0045763707013705395\n",
      "train loss:0.0030127339509679663\n",
      "train loss:0.0028859098968413884\n",
      "train loss:0.0019433116920790274\n",
      "train loss:0.002288346358452606\n",
      "train loss:0.011239975050218273\n",
      "train loss:0.003809557606245158\n",
      "train loss:0.0015615250363376044\n",
      "train loss:0.00205864115226997\n",
      "train loss:0.014477599377503949\n",
      "train loss:0.005742215638657768\n",
      "train loss:0.0023386088691142344\n",
      "train loss:0.009728106021752624\n",
      "train loss:0.0018884479271229788\n",
      "train loss:0.0022204751557045317\n",
      "train loss:0.003391345018855249\n",
      "train loss:0.010204410463902356\n",
      "train loss:0.00652430722097789\n",
      "train loss:0.007633791592252181\n",
      "train loss:0.0053206569557496\n",
      "train loss:0.0008566713125145975\n",
      "train loss:0.0021629348257386147\n",
      "train loss:0.008565330322987497\n",
      "train loss:0.0007294434313559075\n",
      "train loss:0.006712394830790249\n",
      "train loss:0.005445387528657678\n",
      "train loss:0.008730889971458301\n",
      "train loss:0.020119642884642387\n",
      "train loss:0.001479890056714963\n",
      "train loss:0.032116064592529775\n",
      "train loss:0.004181779387080867\n",
      "train loss:0.004877823121706506\n",
      "train loss:0.01677383475692144\n",
      "train loss:0.0496011272523252\n",
      "train loss:0.03922645004142158\n",
      "train loss:0.0011262065542206885\n",
      "train loss:0.00041879537460817073\n",
      "train loss:0.006298781222053927\n",
      "train loss:0.01199874678619104\n",
      "train loss:0.0060226315152029156\n",
      "train loss:0.008780917824964618\n",
      "train loss:0.020333725815492452\n",
      "train loss:0.0050867987870918275\n",
      "train loss:0.008397897251191892\n",
      "train loss:0.004057837601917793\n",
      "train loss:0.026038474155556948\n",
      "train loss:0.0036948498835008363\n",
      "train loss:0.0024379408465391426\n",
      "train loss:0.005571731994704517\n",
      "train loss:0.012398939406910715\n",
      "train loss:0.005033747959189046\n",
      "train loss:0.004939253172627677\n",
      "train loss:0.020340080014505078\n",
      "train loss:0.00868892806136032\n",
      "train loss:0.004740861582129628\n",
      "train loss:0.004633656161045396\n",
      "train loss:0.0037840617243357754\n",
      "train loss:0.002994923693330057\n",
      "train loss:0.01742676891763087\n",
      "train loss:0.010344074700231085\n",
      "train loss:0.017964189630480562\n",
      "train loss:0.01233768123219228\n",
      "train loss:0.011366156502361729\n",
      "train loss:0.0030222572277434907\n",
      "train loss:0.006857517485214415\n",
      "train loss:0.005020639452232309\n",
      "train loss:0.001903606431989\n",
      "train loss:0.004659291145826038\n",
      "train loss:0.0027584072470291953\n",
      "train loss:0.005593744535576019\n",
      "train loss:0.0017288593909145224\n",
      "train loss:0.002304973494966224\n",
      "train loss:0.012340312537632786\n",
      "train loss:0.010978891851538481\n",
      "train loss:0.0018295544131359544\n",
      "train loss:0.006895668335003919\n",
      "train loss:0.014154339862620244\n",
      "train loss:0.0006582154948087491\n",
      "train loss:0.004305245770769001\n",
      "train loss:0.0013019292197539497\n",
      "train loss:0.0031956197571369823\n",
      "train loss:0.0007535671667540239\n",
      "train loss:0.0016572081103353798\n",
      "train loss:0.005789327018338824\n",
      "train loss:0.06608772613355787\n",
      "train loss:0.0023119468236307496\n",
      "train loss:0.008945000618941759\n",
      "train loss:0.0007977306366152744\n",
      "train loss:0.0032410552119858816\n",
      "train loss:0.0057099198106910975\n",
      "train loss:0.002209058117887372\n",
      "train loss:0.010326760218564088\n",
      "train loss:0.0009286436557410452\n",
      "train loss:0.006845928969520908\n",
      "train loss:0.0048877402135354435\n",
      "train loss:0.05578725202689703\n",
      "train loss:0.0011016356788719208\n",
      "train loss:0.0015346985259791404\n",
      "train loss:0.0020428201240850646\n",
      "train loss:0.005871537163318705\n",
      "train loss:0.03204318653461222\n",
      "train loss:0.0009317574471515759\n",
      "train loss:0.0006316116084650014\n",
      "train loss:0.021735023774786786\n",
      "train loss:0.006632606913606746\n",
      "train loss:0.004416987466896954\n",
      "train loss:0.0005961212384900383\n",
      "train loss:0.002071531754646848\n",
      "train loss:0.005390036865488264\n",
      "train loss:0.01979525281477017\n",
      "train loss:0.006795141471691592\n",
      "train loss:0.005898008695247645\n",
      "train loss:0.0016858507621373081\n",
      "train loss:0.0012817245521879622\n",
      "train loss:0.001493390068108911\n",
      "train loss:0.001842176936533657\n",
      "train loss:0.005503650215166304\n",
      "train loss:0.003971752833358466\n",
      "train loss:0.008721671604906708\n",
      "=== epoch:9, train acc:0.998, test acc:0.988 ===\n",
      "train loss:0.021734481363315995\n",
      "train loss:0.04308975654204952\n",
      "train loss:0.004025637335005099\n",
      "train loss:0.0035765567007095623\n",
      "train loss:0.014070433593060776\n",
      "train loss:0.01575410538842835\n",
      "train loss:0.02268315511549339\n",
      "train loss:0.0005951831357416151\n",
      "train loss:0.000908159232009158\n",
      "train loss:0.0015477401257533194\n",
      "train loss:0.005699693016857942\n",
      "train loss:0.002452820756140422\n",
      "train loss:0.0027736270990194505\n",
      "train loss:0.0025565973261267847\n",
      "train loss:0.002715351283862222\n",
      "train loss:0.0026564286305506436\n",
      "train loss:0.002368753940571311\n",
      "train loss:0.003661830178548752\n",
      "train loss:0.017905892165583536\n",
      "train loss:0.01098197545464523\n",
      "train loss:0.0010748807593239952\n",
      "train loss:0.0007124753429899537\n",
      "train loss:0.001676566282790239\n",
      "train loss:0.00878040873650053\n",
      "train loss:0.0038637736359927104\n",
      "train loss:0.002474476481683918\n",
      "train loss:0.01813328010176153\n",
      "train loss:0.030685297483089048\n",
      "train loss:0.005599349853111269\n",
      "train loss:0.0056533020884900346\n",
      "train loss:0.002316961483889752\n",
      "train loss:0.010260137963093587\n",
      "train loss:0.0009491329380147316\n",
      "train loss:0.0011766692521912887\n",
      "train loss:0.004647758837033217\n",
      "train loss:0.0024589996416431678\n",
      "train loss:0.003145832583670002\n",
      "train loss:0.005104382219186164\n",
      "train loss:0.004838508438472529\n",
      "train loss:0.004031923183310237\n",
      "train loss:0.0002536848945508597\n",
      "train loss:0.0035849906096212226\n",
      "train loss:0.0013942853024346497\n",
      "train loss:0.0020917894258701587\n",
      "train loss:0.0028075743023279003\n",
      "train loss:0.0045196030694971915\n",
      "train loss:0.008799142835993095\n",
      "train loss:0.009357355679195467\n",
      "train loss:0.05090441984236783\n",
      "train loss:0.009363061480194182\n",
      "train loss:0.08542667367500095\n",
      "train loss:0.002510488646298481\n",
      "train loss:0.006101660642474983\n",
      "train loss:0.0006820722403633391\n",
      "train loss:0.0013153281907310607\n",
      "train loss:0.0011501334045022108\n",
      "train loss:0.0019664166129059464\n",
      "train loss:0.001694930217704548\n",
      "train loss:0.003261378368021103\n",
      "train loss:0.0028821391648270202\n",
      "train loss:0.00471709289422725\n",
      "train loss:0.006023964801401243\n",
      "train loss:0.0016063265769123932\n",
      "train loss:0.004541355723329396\n",
      "train loss:0.0007556697526348779\n",
      "train loss:0.0015980998227567338\n",
      "train loss:0.0020892892718994876\n",
      "train loss:0.0007260370734888016\n",
      "train loss:0.04495149502574573\n",
      "train loss:0.0026137489395075615\n",
      "train loss:0.004887115358928335\n",
      "train loss:0.012824227391335201\n",
      "train loss:0.0021754730737676025\n",
      "train loss:0.005384504576431565\n",
      "train loss:0.0040409261033303515\n",
      "train loss:0.0037668150025470426\n",
      "train loss:0.015115478636602606\n",
      "train loss:0.005885535365681609\n",
      "train loss:0.0018862615909949118\n",
      "train loss:0.008831904982501488\n",
      "train loss:0.006831832560886037\n",
      "train loss:0.0020683955616871493\n",
      "train loss:0.06073898803397821\n",
      "train loss:0.010095207611171426\n",
      "train loss:0.03189766481062578\n",
      "train loss:0.004713011732302822\n",
      "train loss:0.01672651407235297\n",
      "train loss:0.0030766725625655535\n",
      "train loss:0.0009209683042234616\n",
      "train loss:0.004395777379548956\n",
      "train loss:0.002766698199108047\n",
      "train loss:0.005835117693632431\n",
      "train loss:0.004195849911722374\n",
      "train loss:0.00610162625598844\n",
      "train loss:0.004269142990517325\n",
      "train loss:0.010624262979803532\n",
      "train loss:0.010515073822838639\n",
      "train loss:0.006832616955181228\n",
      "train loss:0.005436278223452546\n",
      "train loss:0.0010090360144240965\n",
      "train loss:0.0018959964723270334\n",
      "train loss:0.0032053694004465284\n",
      "train loss:0.00423341325492888\n",
      "train loss:0.003867773462841659\n",
      "train loss:0.0073142657140921\n",
      "train loss:0.006878446623494378\n",
      "train loss:0.005117930967080591\n",
      "train loss:0.002376255015342394\n",
      "train loss:0.0017524397069286263\n",
      "train loss:0.010296926382189693\n",
      "train loss:0.002365945997189083\n",
      "train loss:0.003433302783510449\n",
      "train loss:0.003518253690321047\n",
      "train loss:0.004936836979459494\n",
      "train loss:0.006983979087017322\n",
      "train loss:0.0017778003307100162\n",
      "train loss:0.0031994898129850437\n",
      "train loss:0.003146084915043885\n",
      "train loss:0.006093507728826802\n",
      "train loss:0.04401689147692041\n",
      "train loss:0.002287313085687464\n",
      "train loss:0.003924211203176694\n",
      "train loss:0.03973117778817486\n",
      "train loss:0.005634518053987476\n",
      "train loss:0.002284479108816254\n",
      "train loss:0.03173480297064697\n",
      "train loss:0.0031015543474589096\n",
      "train loss:0.005838584583559541\n",
      "train loss:0.005273648198827109\n",
      "train loss:0.005075354558869022\n",
      "train loss:0.002621619555731153\n",
      "train loss:0.004448794888399629\n",
      "train loss:0.0005423167628467621\n",
      "train loss:0.006157365509484882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.001669032021480141\n",
      "train loss:0.003591261274501406\n",
      "train loss:0.0050568225372198405\n",
      "train loss:0.0012769942560992972\n",
      "train loss:0.020654044443947063\n",
      "train loss:0.016076394320680136\n",
      "train loss:0.004046118806787514\n",
      "train loss:0.008814286570856115\n",
      "train loss:0.00224923717609764\n",
      "train loss:0.001875079999626251\n",
      "train loss:0.007140874757439626\n",
      "train loss:0.0008929342868564405\n",
      "train loss:0.002124748647346628\n",
      "train loss:0.006696351025596863\n",
      "train loss:0.043125609983612236\n",
      "train loss:0.0017928219382962263\n",
      "train loss:0.005964183008406232\n",
      "train loss:0.00879449604368775\n",
      "train loss:0.015829942249050235\n",
      "train loss:0.0035510351696930974\n",
      "train loss:0.010354895452908177\n",
      "train loss:0.003967695262586198\n",
      "train loss:0.00667211202128979\n",
      "train loss:0.005504977748419233\n",
      "train loss:0.0011879553750336767\n",
      "train loss:0.0005331385649708374\n",
      "train loss:0.005713025067434861\n",
      "train loss:0.00303094637348161\n",
      "train loss:0.013419320682669449\n",
      "train loss:0.01651523393297752\n",
      "train loss:0.0003510922010055364\n",
      "train loss:0.0017016068482832304\n",
      "train loss:0.0067994852701908845\n",
      "train loss:0.002453927154820828\n",
      "train loss:0.0036653199955313043\n",
      "train loss:0.0011695797267596826\n",
      "train loss:0.05446565502256555\n",
      "train loss:0.03331962129577442\n",
      "train loss:0.005495712465940999\n",
      "train loss:0.0055764698654282244\n",
      "train loss:0.0022450327611486394\n",
      "train loss:0.017562763498285602\n",
      "train loss:0.0026551366812396044\n",
      "train loss:0.00017224723214931281\n",
      "train loss:0.0032839232492155503\n",
      "train loss:0.008138886129816073\n",
      "train loss:0.005261249321549802\n",
      "train loss:0.006302850160337491\n",
      "train loss:0.02478729068915746\n",
      "train loss:0.00473426873942875\n",
      "train loss:0.0007540518697258173\n",
      "train loss:0.001622386423117705\n",
      "train loss:0.002574035713010439\n",
      "train loss:0.0012150424589150819\n",
      "train loss:0.03796213303461814\n",
      "train loss:0.017834535098085736\n",
      "train loss:0.01354035047702316\n",
      "train loss:0.08485405762611681\n",
      "train loss:0.002261461294724327\n",
      "train loss:0.006579640015943444\n",
      "train loss:0.005856269859545369\n",
      "train loss:0.010575854609414372\n",
      "train loss:0.0075118062072541516\n",
      "train loss:0.0030484332765943318\n",
      "train loss:0.0025155296127122883\n",
      "train loss:0.0014977940362954482\n",
      "train loss:0.0020176136358091195\n",
      "train loss:0.009101358290915235\n",
      "train loss:0.001452093859252316\n",
      "train loss:0.006821420480552151\n",
      "train loss:0.010155664204430061\n",
      "train loss:0.0017557440228662667\n",
      "train loss:0.009360438664150612\n",
      "train loss:0.0036687248513206034\n",
      "train loss:0.0006380280273617145\n",
      "train loss:0.03593278117295806\n",
      "train loss:0.007498184788037396\n",
      "train loss:0.015141452768258477\n",
      "train loss:0.0030609431154091232\n",
      "train loss:0.01964916062840196\n",
      "train loss:0.0033278197854024537\n",
      "train loss:0.0058838277526442285\n",
      "train loss:0.00903483632020846\n",
      "train loss:0.0026875697575568336\n",
      "train loss:0.004051627555155512\n",
      "train loss:0.008350831779123703\n",
      "train loss:0.009682961656544826\n",
      "train loss:0.0003399527043016367\n",
      "train loss:0.00527999671048272\n",
      "train loss:0.08834223040404517\n",
      "train loss:0.03481029690366554\n",
      "train loss:0.02813643463081499\n",
      "train loss:0.0032671719595466263\n",
      "train loss:0.0022891896075024716\n",
      "train loss:0.009983107514993258\n",
      "train loss:0.01926564417161798\n",
      "train loss:0.012396382692130795\n",
      "train loss:0.004412302233732658\n",
      "train loss:0.019912256303438555\n",
      "train loss:0.010464668782929218\n",
      "train loss:0.007554651727827393\n",
      "train loss:0.0016101092737843248\n",
      "train loss:0.013392504866686115\n",
      "train loss:0.002189479117255653\n",
      "train loss:0.006147571991801619\n",
      "train loss:0.04502177744583658\n",
      "train loss:0.015976666793726026\n",
      "train loss:0.0027266549895298324\n",
      "train loss:0.0007299676877422008\n",
      "train loss:0.005369175436349344\n",
      "train loss:0.0022722618004205196\n",
      "train loss:0.0008423699226843176\n",
      "train loss:0.0012413072010723307\n",
      "train loss:0.015288487939185455\n",
      "train loss:0.0016367854219955453\n",
      "train loss:0.005903310924979197\n",
      "train loss:0.007549710657535029\n",
      "train loss:0.003458203526253908\n",
      "train loss:0.002561704510996622\n",
      "train loss:0.008922981547704649\n",
      "train loss:0.0003462859645856402\n",
      "train loss:0.0016414688292315347\n",
      "train loss:0.0031441534086758356\n",
      "train loss:0.006781161391703079\n",
      "train loss:0.0013319054850049138\n",
      "train loss:0.012661037924791053\n",
      "train loss:0.00021214604812218804\n",
      "train loss:0.0006363956818584869\n",
      "train loss:0.0013147037995927637\n",
      "train loss:0.026414167360401433\n",
      "train loss:0.0006748257107351642\n",
      "train loss:0.017720632690595924\n",
      "train loss:0.006300156749440327\n",
      "train loss:0.004705006405089773\n",
      "train loss:0.009083916031721035\n",
      "train loss:0.004621498318257431\n",
      "train loss:0.015873590118740184\n",
      "train loss:0.011882809896160439\n",
      "train loss:0.00847794690848185\n",
      "train loss:0.010114203694893093\n",
      "train loss:0.0004113158626447138\n",
      "train loss:0.0006528507874519472\n",
      "train loss:0.009463245603897741\n",
      "train loss:0.002874412354848149\n",
      "train loss:0.0010246686817211707\n",
      "train loss:0.0048744275823169045\n",
      "train loss:0.003287203202285934\n",
      "train loss:0.0020909741195275335\n",
      "train loss:0.002037723411279924\n",
      "train loss:0.02467103068043998\n",
      "train loss:0.012562661001216344\n",
      "train loss:0.0015571798527072044\n",
      "train loss:0.005158381133529879\n",
      "train loss:0.003695966545021868\n",
      "train loss:0.00266721219395462\n",
      "train loss:0.005604060191348688\n",
      "train loss:0.029317516499518446\n",
      "train loss:0.004187918112870741\n",
      "train loss:0.005724937045331415\n",
      "train loss:0.006565556220354597\n",
      "train loss:0.0014047944507580871\n",
      "train loss:0.0009247137304754463\n",
      "train loss:0.002567838656781144\n",
      "train loss:0.0021425933848225045\n",
      "train loss:0.025807454051173493\n",
      "train loss:0.00830403556106771\n",
      "train loss:0.01539993269802314\n",
      "train loss:0.00047469826743928585\n",
      "train loss:0.004961697890944026\n",
      "train loss:0.004322337995124721\n",
      "train loss:0.0072454485072520145\n",
      "train loss:0.0017461631098550337\n",
      "train loss:0.0018179111020753403\n",
      "train loss:0.006655721468034996\n",
      "train loss:0.008465881511938294\n",
      "train loss:0.0010289089152359587\n",
      "train loss:0.006069505843025961\n",
      "train loss:0.0031458041468478567\n",
      "train loss:0.0015081496301407287\n",
      "train loss:0.003847449018690495\n",
      "train loss:0.0023591407689116024\n",
      "train loss:0.012962565648522788\n",
      "train loss:0.0027384386135583688\n",
      "train loss:0.030348025146249062\n",
      "train loss:0.014140513647626609\n",
      "train loss:0.0478436908533794\n",
      "train loss:0.0006787346932520192\n",
      "train loss:0.0045676160143308135\n",
      "train loss:0.0031446749081977697\n",
      "train loss:0.0016787155399604692\n",
      "train loss:0.0004297352810627569\n",
      "train loss:0.004602728256855437\n",
      "train loss:0.0032121515631948366\n",
      "train loss:0.0052774361586373705\n",
      "train loss:0.014234542523055529\n",
      "train loss:0.0012581961472449806\n",
      "train loss:0.0035664681700634266\n",
      "train loss:0.010798410525052513\n",
      "train loss:0.01112800073870499\n",
      "train loss:0.01030505147228898\n",
      "train loss:0.0019053977148376304\n",
      "train loss:0.0015953839458427291\n",
      "train loss:0.0032264557200997078\n",
      "train loss:0.013867751214564173\n",
      "train loss:0.005584317301576118\n",
      "train loss:0.006713099722482286\n",
      "train loss:0.007361519072453594\n",
      "train loss:0.007622144748464051\n",
      "train loss:0.00047724237922078103\n",
      "train loss:0.0021226965577995586\n",
      "train loss:0.023552795957129752\n",
      "train loss:0.013109361959687546\n",
      "train loss:0.006701089200544067\n",
      "train loss:0.002900901972871426\n",
      "train loss:0.006980183427068995\n",
      "train loss:0.010656877604635082\n",
      "train loss:0.001866513062093146\n",
      "train loss:0.000956310562846302\n",
      "train loss:0.024487638280746803\n",
      "train loss:0.0277376192522785\n",
      "train loss:0.013664291987491528\n",
      "train loss:0.0007713711328262322\n",
      "train loss:0.009322821167378122\n",
      "train loss:0.001311900086742122\n",
      "train loss:0.0005293424289589736\n",
      "train loss:0.003879019718461336\n",
      "train loss:0.015893776345303326\n",
      "train loss:0.0023187011598089606\n",
      "train loss:0.0034112806471781982\n",
      "train loss:0.0469226080368541\n",
      "train loss:0.005664919815287922\n",
      "train loss:0.00753143018738354\n",
      "train loss:0.0019001330791089943\n",
      "train loss:0.0002360929842963673\n",
      "train loss:0.03460295755283847\n",
      "train loss:0.00702647424272819\n",
      "train loss:0.000592093261367903\n",
      "train loss:0.0014651209875150262\n",
      "train loss:0.003277753973828767\n",
      "train loss:0.09444350817099018\n",
      "train loss:0.02998990802289817\n",
      "train loss:0.007536851943355085\n",
      "train loss:0.000668807746955729\n",
      "train loss:0.026682586116340033\n",
      "train loss:0.0012268736689290175\n",
      "train loss:0.010416158178718393\n",
      "train loss:0.0027794692418528955\n",
      "train loss:0.054170340286668804\n",
      "train loss:0.0033189177857619085\n",
      "train loss:0.0037385735544186614\n",
      "train loss:0.0038251797720565395\n",
      "train loss:0.000820360662755299\n",
      "train loss:0.0016732745465629923\n",
      "train loss:0.011831056831172004\n",
      "train loss:0.011802696656389763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.08536776032160442\n",
      "train loss:0.005258856303077493\n",
      "train loss:0.0038591338506445377\n",
      "train loss:0.013210151320471333\n",
      "train loss:0.002451309266422056\n",
      "train loss:0.0058363135611736025\n",
      "train loss:0.0155863852989074\n",
      "train loss:0.00673926980134635\n",
      "train loss:0.0012160845675291462\n",
      "train loss:0.001966666269404105\n",
      "train loss:0.009207912306737407\n",
      "train loss:0.010963212098429148\n",
      "train loss:0.004235399789193306\n",
      "train loss:0.0009772660677702127\n",
      "train loss:0.005337941621247476\n",
      "train loss:0.0004994314036490428\n",
      "train loss:0.003780552523011289\n",
      "train loss:0.00040855989191385914\n",
      "train loss:0.001927781288484106\n",
      "train loss:0.007376230980353428\n",
      "train loss:0.005962744911571607\n",
      "train loss:0.0005553887823687006\n",
      "train loss:0.013066730790555967\n",
      "train loss:0.007810259976972367\n",
      "train loss:0.0007543362324302617\n",
      "train loss:0.35376916663625196\n",
      "train loss:0.0037448198836432436\n",
      "train loss:0.0018524121177663057\n",
      "train loss:0.002161581582837345\n",
      "train loss:0.01511308742157519\n",
      "train loss:0.0016039170716277188\n",
      "train loss:0.004858898049610831\n",
      "train loss:0.005188718914385393\n",
      "train loss:0.003940871061858777\n",
      "train loss:0.014967876926690986\n",
      "train loss:0.01896104235039716\n",
      "train loss:0.0015066886920951652\n",
      "train loss:0.0026641240102596663\n",
      "train loss:0.0008682664310726433\n",
      "train loss:0.002446978516431959\n",
      "train loss:0.0010613973542343972\n",
      "train loss:0.0075746118844746725\n",
      "train loss:0.0026843443868280526\n",
      "train loss:0.012244701931173439\n",
      "train loss:0.0009311463883223853\n",
      "train loss:0.015765170844581587\n",
      "train loss:0.013016451709383488\n",
      "train loss:0.0016268651820507429\n",
      "train loss:0.013969204920011304\n",
      "train loss:0.0030431158113370066\n",
      "train loss:0.00046516149563385103\n",
      "train loss:0.0016072468851711127\n",
      "train loss:0.011032145129429143\n",
      "train loss:0.003625496180908439\n",
      "train loss:0.001741538143887613\n",
      "train loss:0.0011597668346550176\n",
      "train loss:0.005771291539581188\n",
      "train loss:0.012921022994127753\n",
      "train loss:0.005205136110454861\n",
      "train loss:0.0015717831169258195\n",
      "train loss:0.004502345587677754\n",
      "train loss:0.0005752180183709031\n",
      "train loss:0.00809508946800683\n",
      "train loss:0.013542652831705158\n",
      "train loss:0.0025764293596433493\n",
      "train loss:0.020247759889718404\n",
      "train loss:0.01416521029906191\n",
      "train loss:0.006072391781904651\n",
      "train loss:0.00540221784305974\n",
      "train loss:0.0039024671829196284\n",
      "train loss:0.007729697082840242\n",
      "train loss:0.00910653195314187\n",
      "train loss:0.010288952668026546\n",
      "train loss:0.0010711931952057149\n",
      "train loss:0.0024806356642627083\n",
      "train loss:0.005326553375472681\n",
      "train loss:0.000930361978511323\n",
      "train loss:0.003902157507909987\n",
      "train loss:0.0055338994855046505\n",
      "train loss:0.0013097072784058078\n",
      "train loss:0.04640616453481997\n",
      "train loss:0.004054936676790806\n",
      "train loss:0.0012690860545318487\n",
      "train loss:0.0027563865431248676\n",
      "train loss:0.010836165463848697\n",
      "train loss:0.005666973029240506\n",
      "train loss:0.0020396884438180518\n",
      "train loss:0.0018812582296208779\n",
      "train loss:0.004800964132742452\n",
      "train loss:0.0009260385580635943\n",
      "train loss:0.0016900743102535847\n",
      "train loss:0.005296741487584339\n",
      "train loss:0.020474666469845287\n",
      "train loss:0.00709087899286853\n",
      "train loss:0.0003441511042299468\n",
      "train loss:0.0005906526995711274\n",
      "train loss:0.0017912139863921401\n",
      "train loss:0.006588439947586554\n",
      "train loss:0.016089588514172602\n",
      "train loss:0.005311578832367865\n",
      "train loss:0.0030579533740612485\n",
      "train loss:0.018317731544381977\n",
      "train loss:0.003657021176865108\n",
      "train loss:0.00489424537841384\n",
      "train loss:0.005538011999187628\n",
      "train loss:0.008333185533287215\n",
      "train loss:0.0010565754810301546\n",
      "train loss:0.0020297451323216618\n",
      "train loss:0.0008178770385406706\n",
      "train loss:0.007909557703054135\n",
      "train loss:0.007526310042732008\n",
      "train loss:0.003113593876159561\n",
      "train loss:0.004973128079889122\n",
      "train loss:0.0021542449233149046\n",
      "train loss:0.0008390543382015294\n",
      "train loss:0.0052418785235964615\n",
      "train loss:0.005066908587212598\n",
      "train loss:0.0072442917651306315\n",
      "train loss:0.0008459795836188179\n",
      "train loss:0.0018640030440617162\n",
      "train loss:0.007273011472759582\n",
      "train loss:0.08393114892355612\n",
      "train loss:0.0014396204168370921\n",
      "train loss:0.0007396163891640948\n",
      "train loss:0.0020173750421968637\n",
      "train loss:0.010005914097189402\n",
      "train loss:0.11548309652030998\n",
      "train loss:0.001975887949773439\n",
      "train loss:0.0032573249472479977\n",
      "train loss:0.005719326019166503\n",
      "train loss:0.0035685729001426274\n",
      "train loss:0.004835618959670449\n",
      "train loss:0.0009681785625239372\n",
      "train loss:0.0010700299121611\n",
      "train loss:0.009530028343190505\n",
      "train loss:0.020522226980500623\n",
      "train loss:0.0009676713405088273\n",
      "train loss:0.007690784975091812\n",
      "train loss:0.00204362319472192\n",
      "train loss:0.0018386149951536228\n",
      "train loss:0.000418293944304148\n",
      "train loss:0.037217195882769276\n",
      "train loss:0.001847503788812061\n",
      "train loss:0.006907809462204084\n",
      "train loss:0.006748721725257711\n",
      "train loss:0.0025590484911196454\n",
      "train loss:0.003451723117019107\n",
      "train loss:0.0007084084446623792\n",
      "train loss:0.04751653050068298\n",
      "train loss:0.0013658480048860519\n",
      "train loss:0.0026154820219770353\n",
      "train loss:0.007837836252753489\n",
      "train loss:0.01278314863065578\n",
      "train loss:0.0163745565949436\n",
      "train loss:0.0016689194384838465\n",
      "train loss:0.0010014927020777374\n",
      "train loss:0.0003903683922620008\n",
      "train loss:0.008047172311622086\n",
      "train loss:0.0036798954626251295\n",
      "train loss:0.0011111757168956974\n",
      "train loss:0.0006993746897722378\n",
      "train loss:0.008738953833592609\n",
      "train loss:0.0033980109459258293\n",
      "train loss:0.0037784826878490803\n",
      "train loss:0.012082793683119257\n",
      "train loss:0.008515080978744555\n",
      "train loss:0.005593412868407077\n",
      "train loss:0.004410661774968446\n",
      "train loss:0.014330682459709383\n",
      "train loss:0.002796708128954736\n",
      "train loss:0.0031386083473211195\n",
      "train loss:0.005221208371126168\n",
      "train loss:0.00892237867070514\n",
      "train loss:0.0012219455601965809\n",
      "train loss:0.02275267102034444\n",
      "train loss:0.003015704411676618\n",
      "train loss:0.0014754976987365806\n",
      "train loss:0.005617485862397638\n",
      "train loss:0.0006735134931748579\n",
      "train loss:0.028729764120312664\n",
      "train loss:0.0004794836934941937\n",
      "train loss:0.013172851648323705\n",
      "train loss:0.00554686187209936\n",
      "train loss:0.0015529465003726731\n",
      "train loss:0.009448285429788903\n",
      "train loss:0.0013505887854676965\n",
      "train loss:0.005890593425051249\n",
      "train loss:0.004390593554550456\n",
      "train loss:0.0005135491583280908\n",
      "train loss:0.0021199513824773724\n",
      "train loss:0.0011699946774390338\n",
      "train loss:0.0006142476047130055\n",
      "train loss:0.0012361605911918934\n",
      "train loss:0.0021067444993197013\n",
      "train loss:0.005590416467835497\n",
      "train loss:0.045829137858645765\n",
      "train loss:0.014226294357975724\n",
      "train loss:0.0007811832569046719\n",
      "train loss:0.004186261621990876\n",
      "train loss:0.00038631840667546844\n",
      "train loss:0.009372199064248023\n",
      "train loss:0.0030181176232830158\n",
      "train loss:0.0019836362966372755\n",
      "train loss:0.0019574893444036\n",
      "train loss:0.0010331405030725397\n",
      "train loss:0.0011846117962516934\n",
      "train loss:0.0017535312769950161\n",
      "train loss:0.0025647843811295373\n",
      "train loss:0.01447725509602122\n",
      "train loss:0.018891578854812175\n",
      "train loss:0.006537039419882842\n",
      "=== epoch:10, train acc:0.995, test acc:0.987 ===\n",
      "train loss:0.0073806983447700725\n",
      "train loss:0.009103005585539352\n",
      "train loss:0.0019403914132841453\n",
      "train loss:0.010359014712091313\n",
      "train loss:0.005148211805818232\n",
      "train loss:0.01319769815425358\n",
      "train loss:0.002665899965365767\n",
      "train loss:0.036109472781004494\n",
      "train loss:0.0012122678353223006\n",
      "train loss:0.0033796195949562036\n",
      "train loss:0.00022277098966632495\n",
      "train loss:0.0014631953574968939\n",
      "train loss:0.001994576632265529\n",
      "train loss:0.008178552808313654\n",
      "train loss:0.135669615639725\n",
      "train loss:0.008610708053072853\n",
      "train loss:0.04335643686458084\n",
      "train loss:0.00441181902241475\n",
      "train loss:0.0037537938929017895\n",
      "train loss:0.009376999399571036\n",
      "train loss:0.0018699161270428755\n",
      "train loss:0.0029184748860133438\n",
      "train loss:0.014531822445431757\n",
      "train loss:0.007779820861550758\n",
      "train loss:0.001091450479371716\n",
      "train loss:0.021396669590672962\n",
      "train loss:0.01435723702747055\n",
      "train loss:0.0057134275624969874\n",
      "train loss:0.043111397756679624\n",
      "train loss:0.001634885753619001\n",
      "train loss:0.002999637558474077\n",
      "train loss:0.003264091790582119\n",
      "train loss:0.03093402552413585\n",
      "train loss:0.010770464747119407\n",
      "train loss:0.0014498574584071704\n",
      "train loss:0.012698641482176698\n",
      "train loss:0.005856942738440074\n",
      "train loss:0.008690135501392327\n",
      "train loss:0.0010632599637027155\n",
      "train loss:0.005226317718047114\n",
      "train loss:0.0063123267906488615\n",
      "train loss:0.0024199988663398063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0034589478850762597\n",
      "train loss:0.001269317591871777\n",
      "train loss:0.014399955643219633\n",
      "train loss:0.022284898259585045\n",
      "train loss:0.004353702361157617\n",
      "train loss:0.00342421680091415\n",
      "train loss:0.0007113815687010387\n",
      "train loss:0.0009827299198793779\n",
      "train loss:0.008900855989587241\n",
      "train loss:0.010883812901133579\n",
      "train loss:0.003628207478835436\n",
      "train loss:0.008176724753463609\n",
      "train loss:0.002531939997410484\n",
      "train loss:0.018569630230627906\n",
      "train loss:0.0035082390377625906\n",
      "train loss:0.02567492646151638\n",
      "train loss:0.06215878107000389\n",
      "train loss:0.002197022063683511\n",
      "train loss:0.010046688709856387\n",
      "train loss:0.001458376681969252\n",
      "train loss:0.003820718497900362\n",
      "train loss:0.004773945763607748\n",
      "train loss:0.004647416118065074\n",
      "train loss:0.009495126772784647\n",
      "train loss:0.001698155895781533\n",
      "train loss:0.0022952368888169853\n",
      "train loss:0.0005587160195312142\n",
      "train loss:0.003922738861233442\n",
      "train loss:0.0016504312290907717\n",
      "train loss:0.0004883002266920845\n",
      "train loss:0.0034928867987477743\n",
      "train loss:0.0014592350395101039\n",
      "train loss:0.004102477570476168\n",
      "train loss:0.0009729127466117407\n",
      "train loss:0.0047442166851136\n",
      "train loss:0.003782369915528574\n",
      "train loss:0.055642946211464\n",
      "train loss:0.003957956368372209\n",
      "train loss:0.001267833894445482\n",
      "train loss:0.006317620495243639\n",
      "train loss:0.009373693224540975\n",
      "train loss:0.007054486505086403\n",
      "train loss:0.004359064705186119\n",
      "train loss:0.00144086318341184\n",
      "train loss:0.0036772223573847394\n",
      "train loss:0.011216900326842923\n",
      "train loss:0.000955660167872059\n",
      "train loss:0.003766771074263581\n",
      "train loss:0.0007362575829383489\n",
      "train loss:0.06263285152091058\n",
      "train loss:0.002207687808031897\n",
      "train loss:0.0032806176234791633\n",
      "train loss:0.011110072584455679\n",
      "train loss:0.001307753788312737\n",
      "train loss:0.0019746434351318775\n",
      "train loss:0.001027176708688162\n",
      "train loss:0.008051611409648732\n",
      "train loss:0.002528767791268183\n",
      "train loss:0.04343119380855548\n",
      "train loss:0.046642587022197333\n",
      "train loss:0.027199540271054833\n",
      "train loss:0.05697169216280212\n",
      "train loss:0.05330875962528611\n",
      "train loss:0.002253731182834539\n",
      "train loss:0.0037747181325856497\n",
      "train loss:0.0017274445796827836\n",
      "train loss:0.002749005952798469\n",
      "train loss:0.04011827643409652\n",
      "train loss:0.00966714185313865\n",
      "train loss:0.0009652339087936454\n",
      "train loss:0.0008607669538616855\n",
      "train loss:0.0035613377641830735\n",
      "train loss:0.008233459791607233\n",
      "train loss:0.002912999365105855\n",
      "train loss:0.007850110924802357\n",
      "train loss:0.002697616553330566\n",
      "train loss:0.050117685877840615\n",
      "train loss:0.004950069906879045\n",
      "train loss:0.010278413781984914\n",
      "train loss:0.0011453333381207557\n",
      "train loss:0.011017914360053685\n",
      "train loss:0.005757125709715055\n",
      "train loss:0.003726664356541533\n",
      "train loss:0.0038720553307744167\n",
      "train loss:0.0020877978970721\n",
      "train loss:0.012524116191666485\n",
      "train loss:0.010347556403975446\n",
      "train loss:0.0028256346717167925\n",
      "train loss:0.0030308421550592794\n",
      "train loss:0.015144824926761875\n",
      "train loss:0.011920989197669884\n",
      "train loss:0.007525842035922337\n",
      "train loss:0.0028364306979652993\n",
      "train loss:0.00769548188019726\n",
      "train loss:0.002486913469622428\n",
      "train loss:0.004145807088941862\n",
      "train loss:0.0011708111202420482\n",
      "train loss:0.00035530503780157284\n",
      "train loss:0.0012602819919278424\n",
      "train loss:0.0020248861852930154\n",
      "train loss:0.012661721176004028\n",
      "train loss:0.0027498992322488786\n",
      "train loss:0.01552622246725178\n",
      "train loss:0.001733211008961082\n",
      "train loss:0.010615827445037133\n",
      "train loss:0.01690177497621011\n",
      "train loss:0.004858396398070739\n",
      "train loss:0.0013813634653787278\n",
      "train loss:0.0009302630702554171\n",
      "train loss:0.001822552531252398\n",
      "train loss:0.0016631716126626296\n",
      "train loss:0.007515607271429528\n",
      "train loss:0.0037575498654533923\n",
      "train loss:0.0031238553291526605\n",
      "train loss:0.0005448206426009273\n",
      "train loss:0.00019761066897766864\n",
      "train loss:0.0013677899254901957\n",
      "train loss:0.0035089469862925288\n",
      "train loss:0.00027240217403309977\n",
      "train loss:0.001728447541284452\n",
      "train loss:0.008622100606176887\n",
      "train loss:0.004036246279304816\n",
      "train loss:0.0019158606686293577\n",
      "train loss:0.04875263155363993\n",
      "train loss:0.001253596830703732\n",
      "train loss:0.0018226174915330037\n",
      "train loss:0.002016608396440273\n",
      "train loss:0.0023266979601924264\n",
      "train loss:0.003968542293325945\n",
      "train loss:0.0035980078939638694\n",
      "train loss:0.002532877378426717\n",
      "train loss:0.0016240402897043974\n",
      "train loss:0.0009896122725440864\n",
      "train loss:0.00043191322508580673\n",
      "train loss:0.003301724224632192\n",
      "train loss:0.00298071119565913\n",
      "train loss:0.0012195964289561882\n",
      "train loss:0.008889275755310774\n",
      "train loss:0.0013505212328604722\n",
      "train loss:0.00059528836633062\n",
      "train loss:0.027238594768206337\n",
      "train loss:0.0014568636819518482\n",
      "train loss:0.0007990792673832794\n",
      "train loss:0.0014908241338936856\n",
      "train loss:0.00027069316806915757\n",
      "train loss:0.0015075567223189023\n",
      "train loss:0.0011568120243674862\n",
      "train loss:0.008702917326015696\n",
      "train loss:0.005042902199552398\n",
      "train loss:0.009736412240708254\n",
      "train loss:0.0006478670739709091\n",
      "train loss:0.005174216832769284\n",
      "train loss:0.0024827358553171836\n",
      "train loss:0.0011103733364856636\n",
      "train loss:0.01476984669981648\n",
      "train loss:0.005969779909576788\n",
      "train loss:0.006049001770993062\n",
      "train loss:0.022173990439112067\n",
      "train loss:0.0017109942759890007\n",
      "train loss:0.0017146709792550368\n",
      "train loss:0.0006049898091736904\n",
      "train loss:0.0006781291317665881\n",
      "train loss:0.003908898862157919\n",
      "train loss:0.0004814342192132489\n",
      "train loss:0.0021732631154357373\n",
      "train loss:0.016911778450128437\n",
      "train loss:0.003130715800186344\n",
      "train loss:0.003512250180144777\n",
      "train loss:0.0006732692237323275\n",
      "train loss:0.006398245770120251\n",
      "train loss:0.0031656264198808167\n",
      "train loss:0.01704279498671881\n",
      "train loss:0.02693223800971234\n",
      "train loss:0.003013259119818089\n",
      "train loss:0.008716769854100567\n",
      "train loss:0.00618994582512681\n",
      "train loss:0.010974133507846347\n",
      "train loss:0.0007611080813943176\n",
      "train loss:0.002667518660914732\n",
      "train loss:0.0036252898231352738\n",
      "train loss:0.0022133170823005646\n",
      "train loss:0.013351296452078198\n",
      "train loss:0.022512743117203294\n",
      "train loss:0.0008484973282462765\n",
      "train loss:0.0011250688336277895\n",
      "train loss:0.006241294991731399\n",
      "train loss:0.005292001143072231\n",
      "train loss:0.018501890104478884\n",
      "train loss:0.0021325440019102884\n",
      "train loss:0.04798140439470886\n",
      "train loss:0.0012860548607980346\n",
      "train loss:0.00902301922169014\n",
      "train loss:0.0010212345900369674\n",
      "train loss:0.00405646137478093\n",
      "train loss:0.0014450356931354194\n",
      "train loss:0.005927731560058728\n",
      "train loss:0.0037058192327056034\n",
      "train loss:0.025402795172323005\n",
      "train loss:0.004597934570743787\n",
      "train loss:0.0007666589423424902\n",
      "train loss:0.010732060156609857\n",
      "train loss:0.0010260101400747955\n",
      "train loss:0.0018981383533405788\n",
      "train loss:0.004782647113330908\n",
      "train loss:0.0012433085245811188\n",
      "train loss:0.003186720316405931\n",
      "train loss:0.003988300297286448\n",
      "train loss:0.001071145874859736\n",
      "train loss:0.0005378635757282139\n",
      "train loss:0.002592970990453257\n",
      "train loss:0.005101973909507316\n",
      "train loss:0.0014097142681218939\n",
      "train loss:0.004419252936690796\n",
      "train loss:0.00246474573543754\n",
      "train loss:0.015970417948993457\n",
      "train loss:0.0027874748978751833\n",
      "train loss:0.018841763978737363\n",
      "train loss:0.001521876299643293\n",
      "train loss:0.008891369551713425\n",
      "train loss:0.00043005356942853683\n",
      "train loss:0.005076366760774144\n",
      "train loss:0.0032475362718652045\n",
      "train loss:0.0025252968909301827\n",
      "train loss:0.005843085509605962\n",
      "train loss:0.0031123039108972013\n",
      "train loss:0.008802690526491219\n",
      "train loss:0.004184452360794621\n",
      "train loss:0.003979521914793197\n",
      "train loss:0.023647422251826505\n",
      "train loss:0.010718115190484916\n",
      "train loss:0.0010887215277254891\n",
      "train loss:0.0056727894629261455\n",
      "train loss:0.0015526141966054378\n",
      "train loss:0.0013894166361062468\n",
      "train loss:0.0017473052868589903\n",
      "train loss:0.006463945770293047\n",
      "train loss:0.0018490922285743776\n",
      "train loss:0.004965102677991059\n",
      "train loss:0.0006256432457695751\n",
      "train loss:0.002011195759985461\n",
      "train loss:0.003863364654051442\n",
      "train loss:0.026891920305140843\n",
      "train loss:0.022913037606264196\n",
      "train loss:0.02062631147369227\n",
      "train loss:0.00598094469792255\n",
      "train loss:0.0024968725799060926\n",
      "train loss:0.006465127336983849\n",
      "train loss:0.003394696611263291\n",
      "train loss:0.004418838105321526\n",
      "train loss:0.0023499113892427098\n",
      "train loss:0.004539313380469436\n",
      "train loss:0.0035327494262082705\n",
      "train loss:0.005410467904738543\n",
      "train loss:0.00699275825539406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0021623389290942123\n",
      "train loss:0.007624008620407158\n",
      "train loss:0.010224389180579673\n",
      "train loss:0.0003455712976469353\n",
      "train loss:0.000930852438251613\n",
      "train loss:0.008764126355834831\n",
      "train loss:0.008631806311983143\n",
      "train loss:0.00051452491018459\n",
      "train loss:0.0009827880585652246\n",
      "train loss:0.0016584538514639528\n",
      "train loss:0.005107353999271965\n",
      "train loss:0.0005419125797282597\n",
      "train loss:0.0012743781611732371\n",
      "train loss:0.015870358962023084\n",
      "train loss:0.0027582506162888315\n",
      "train loss:0.0023548437386075862\n",
      "train loss:0.004868262426340627\n",
      "train loss:0.011648096901209644\n",
      "train loss:0.002977249172363749\n",
      "train loss:0.0029913192184384428\n",
      "train loss:0.008426456865865645\n",
      "train loss:0.0013093981159920628\n",
      "train loss:0.004117562455100523\n",
      "train loss:0.0013630952538357902\n",
      "train loss:0.003954319963754145\n",
      "train loss:0.015090919728636696\n",
      "train loss:0.0035422050474305626\n",
      "train loss:0.03004223855926558\n",
      "train loss:0.007749077992553921\n",
      "train loss:0.0007175153845546134\n",
      "train loss:0.0005049721758656399\n",
      "train loss:0.004989393142185148\n",
      "train loss:0.013622501455769643\n",
      "train loss:0.0034442541938418296\n",
      "train loss:0.0002532848671346822\n",
      "train loss:0.006548945127891944\n",
      "train loss:0.003056869351716188\n",
      "train loss:0.00133091238694164\n",
      "train loss:0.01153754310851111\n",
      "train loss:0.0007235302393856981\n",
      "train loss:0.0004393780653651272\n",
      "train loss:0.000291232976709899\n",
      "train loss:0.008003149438037748\n",
      "train loss:0.003456493745922609\n",
      "train loss:0.005981230584113279\n",
      "train loss:0.0044817420063190385\n",
      "train loss:0.04539628259211125\n",
      "train loss:0.001920783098328341\n",
      "train loss:0.0049098756666771035\n",
      "train loss:0.0015684597620608273\n",
      "train loss:0.0013398699283798607\n",
      "train loss:0.0015502262661472664\n",
      "train loss:0.0010252974690745636\n",
      "train loss:0.006039823622001237\n",
      "train loss:0.004171567551813853\n",
      "train loss:0.009625386583468778\n",
      "train loss:0.0010763271621278853\n",
      "train loss:0.0033216857999533106\n",
      "train loss:0.004709491935341686\n",
      "train loss:0.0006227375921175877\n",
      "train loss:0.002399017373661283\n",
      "train loss:0.010751656162822101\n",
      "train loss:0.0016473015548376707\n",
      "train loss:0.015260996290122461\n",
      "train loss:0.0016833051452711139\n",
      "train loss:0.001986568040204041\n",
      "train loss:0.0016208615601199176\n",
      "train loss:0.0019629500302126386\n",
      "train loss:0.0002034801764196718\n",
      "train loss:0.0021604200765469386\n",
      "train loss:0.013440837555326164\n",
      "train loss:0.00201312083726265\n",
      "train loss:0.0015765973646000454\n",
      "train loss:0.002239820261898498\n",
      "train loss:0.0005609511166461106\n",
      "train loss:0.0031193932528530924\n",
      "train loss:0.002062881562512284\n",
      "train loss:0.0006181781195464794\n",
      "train loss:0.0019579221657019465\n",
      "train loss:0.008167509137876892\n",
      "train loss:0.0033012132140434395\n",
      "train loss:0.0007773973343712383\n",
      "train loss:0.001549109906361248\n",
      "train loss:0.0014867887770478227\n",
      "train loss:0.003818720360701524\n",
      "train loss:0.001253237639221766\n",
      "train loss:0.007438607046790027\n",
      "train loss:0.005054748952545208\n",
      "train loss:0.0008701956907619811\n",
      "train loss:0.01776317520861402\n",
      "train loss:0.00010786869681463589\n",
      "train loss:0.001795973492921928\n",
      "train loss:0.0019449245960521946\n",
      "train loss:0.00021084167018406162\n",
      "train loss:0.001648867923846018\n",
      "train loss:0.011269338160356156\n",
      "train loss:0.0015263966310859137\n",
      "train loss:5.272481948121204e-05\n",
      "train loss:0.0023736689332500625\n",
      "train loss:0.0021164924159254002\n",
      "train loss:0.0029193746330102243\n",
      "train loss:0.0055690353035159675\n",
      "train loss:0.005999351891007604\n",
      "train loss:0.010310225303038685\n",
      "train loss:0.0013664609220483462\n",
      "train loss:0.0007995327795060904\n",
      "train loss:0.061305967918249854\n",
      "train loss:0.0021962797314273867\n",
      "train loss:0.003759094004027353\n",
      "train loss:0.003971031453510599\n",
      "train loss:0.006187553547143011\n",
      "train loss:0.004844154603769747\n",
      "train loss:0.030992752162318175\n",
      "train loss:0.0012208866503050143\n",
      "train loss:0.014228866516365285\n",
      "train loss:0.014513631135938107\n",
      "train loss:0.0016546188543994811\n",
      "train loss:0.0017071718974863317\n",
      "train loss:0.038283424919708264\n",
      "train loss:0.007240050595329648\n",
      "train loss:0.00038603696361740714\n",
      "train loss:0.024533701942863445\n",
      "train loss:0.005203346477259341\n",
      "train loss:0.00515545585400955\n",
      "train loss:0.023936441766848286\n",
      "train loss:0.011813538918565712\n",
      "train loss:0.029060278441172\n",
      "train loss:0.001545954155735388\n",
      "train loss:0.007314348265003256\n",
      "train loss:0.002840364553465639\n",
      "train loss:0.0019483896944091587\n",
      "train loss:0.06978550795964626\n",
      "train loss:0.005624072725396779\n",
      "train loss:0.006918443081605491\n",
      "train loss:0.0006902162990322032\n",
      "train loss:0.0020672323696716123\n",
      "train loss:0.004474119741333195\n",
      "train loss:0.0036324969601676266\n",
      "train loss:0.0023165609319347473\n",
      "train loss:0.010611528135428767\n",
      "train loss:0.006551628654704536\n",
      "train loss:0.0004498125087801011\n",
      "train loss:0.002449495124301887\n",
      "train loss:0.0003412065250027996\n",
      "train loss:0.012629321012795427\n",
      "train loss:0.0015088612676704824\n",
      "train loss:0.001293038543186265\n",
      "train loss:0.006325502160440948\n",
      "train loss:0.0019145462135614025\n",
      "train loss:0.0032410109927859553\n",
      "train loss:0.0066608572807549124\n",
      "train loss:0.0006239220416222201\n",
      "train loss:0.002774874414809481\n",
      "train loss:0.006335321293640712\n",
      "train loss:0.005410237259580901\n",
      "train loss:0.014576323923931172\n",
      "train loss:0.0017065681039806224\n",
      "train loss:0.03137245098544624\n",
      "train loss:0.0030140839663547053\n",
      "train loss:0.0031627513453870125\n",
      "train loss:0.001615718065489552\n",
      "train loss:0.0002546824267353305\n",
      "train loss:0.0033067855911679844\n",
      "train loss:0.0006529136640342\n",
      "train loss:0.004566224235030873\n",
      "train loss:0.004995012958536892\n",
      "train loss:0.0022993949616314613\n",
      "train loss:0.04078345858375498\n",
      "train loss:0.0019887708413594864\n",
      "train loss:0.001444855242600165\n",
      "train loss:0.0005648317060664217\n",
      "train loss:0.007578303144647459\n",
      "train loss:0.0001749726602255023\n",
      "train loss:0.004000996426332667\n",
      "train loss:0.0035124506812618177\n",
      "train loss:0.26457376333045696\n",
      "train loss:0.0018438511694797623\n",
      "train loss:0.008185223390343863\n",
      "train loss:0.003775437001423531\n",
      "train loss:0.007493904132511113\n",
      "train loss:0.0023182045185804433\n",
      "train loss:0.0006340604584872367\n",
      "train loss:0.0016790395758412764\n",
      "train loss:0.0032322522372632036\n",
      "train loss:0.0030239735725195505\n",
      "train loss:0.002423342505587138\n",
      "train loss:0.0035681581857495083\n",
      "train loss:0.009578707094531395\n",
      "train loss:0.0022451281846749974\n",
      "train loss:0.006665291174845672\n",
      "train loss:0.0073800507313124515\n",
      "train loss:0.0009226954255062523\n",
      "train loss:0.0010408173229144125\n",
      "train loss:0.017182421810401666\n",
      "train loss:0.0019153855398956732\n",
      "train loss:0.0017389366789042212\n",
      "train loss:0.011311689917038244\n",
      "train loss:0.0019759018687157616\n",
      "train loss:0.0009937207019372477\n",
      "train loss:0.002569047789456567\n",
      "train loss:0.005568535669287873\n",
      "train loss:0.002114846907615607\n",
      "train loss:0.022199156776200258\n",
      "train loss:0.03228310712200291\n",
      "train loss:0.0008739583672290115\n",
      "train loss:0.01496409141453161\n",
      "train loss:0.0022951092865333847\n",
      "train loss:0.007613926773477085\n",
      "train loss:0.016296256491505928\n",
      "train loss:0.00043287434634371616\n",
      "train loss:0.0003308581043784634\n",
      "train loss:0.0034378332398560477\n",
      "train loss:0.0026463030739919714\n",
      "train loss:0.0047477982348047365\n",
      "train loss:0.013546610873865474\n",
      "train loss:0.0035791919469254268\n",
      "train loss:0.0053478741679428765\n",
      "train loss:0.005342633631022497\n",
      "train loss:0.011618782326576624\n",
      "train loss:0.004931641098821299\n",
      "train loss:0.0059897260941940865\n",
      "train loss:0.0014334838193272614\n",
      "train loss:0.0027649290416417154\n",
      "train loss:0.007634951656413888\n",
      "train loss:0.018121884539305504\n",
      "train loss:0.002298906908666483\n",
      "train loss:0.0003834177750008175\n",
      "train loss:0.002212067263212977\n",
      "train loss:0.0005917950295925235\n",
      "train loss:0.007449980046966302\n",
      "train loss:0.0004416146359239211\n",
      "train loss:0.005192821548590219\n",
      "train loss:0.02780350157199812\n",
      "train loss:0.0014127610488116407\n",
      "train loss:8.787382746597042e-05\n",
      "train loss:0.001862795673922604\n",
      "train loss:0.002342363523055076\n",
      "train loss:0.0009950147934724624\n",
      "train loss:0.009186149083214551\n",
      "train loss:0.0003990480534444555\n",
      "train loss:0.008016214850577748\n",
      "train loss:0.002097847014385907\n",
      "train loss:0.003566509789740463\n",
      "train loss:0.00248566740624401\n",
      "train loss:0.0012619960503301505\n",
      "train loss:0.0016174162477747445\n",
      "train loss:0.006256617067406476\n",
      "train loss:0.025150011499780148\n",
      "train loss:0.01230951730868348\n",
      "train loss:0.0007257205971178657\n",
      "train loss:0.007373776144916439\n",
      "train loss:0.0004767855477139466\n",
      "train loss:0.0003030627624024425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.006692005805148666\n",
      "train loss:0.0041973423915882295\n",
      "train loss:0.002578541431125343\n",
      "train loss:0.0009848471406791591\n",
      "train loss:0.0030888830025838174\n",
      "train loss:0.0034865203372205916\n",
      "train loss:0.0007429481512220456\n",
      "train loss:0.059267802243602216\n",
      "train loss:0.0006341318918108622\n",
      "train loss:0.003217001557444717\n",
      "train loss:0.012003935941956425\n",
      "train loss:0.0035702613282105142\n",
      "train loss:0.0011014471946106878\n",
      "train loss:0.0008068070750690832\n",
      "train loss:0.014586450692783724\n",
      "train loss:0.002659800030467735\n",
      "train loss:0.00645732937291995\n",
      "train loss:0.002384225261527674\n",
      "train loss:0.016505786956006373\n",
      "train loss:0.0013822482242340241\n",
      "train loss:0.002182521862664543\n",
      "train loss:0.0017991633694930257\n",
      "train loss:0.004491211344810399\n",
      "train loss:0.0013280727082591328\n",
      "train loss:0.0015326449921047786\n",
      "train loss:0.0008074294042012441\n",
      "train loss:0.004265447682344846\n",
      "train loss:0.0006599587118418449\n",
      "train loss:0.0047094174079775186\n",
      "train loss:0.0025200161391210245\n",
      "train loss:0.0018761690208852167\n",
      "train loss:0.03440703024655022\n",
      "train loss:0.03296469947066647\n",
      "train loss:0.00039472012953963796\n",
      "train loss:0.0004920023423809842\n",
      "train loss:0.0005602677770799805\n",
      "train loss:0.025222043123985297\n",
      "train loss:0.09910902729964233\n",
      "train loss:0.003365978782305746\n",
      "train loss:0.009942905402052667\n",
      "train loss:0.0020913368639469674\n",
      "train loss:0.0014098826734149836\n",
      "train loss:0.0008290473778385817\n",
      "train loss:0.02959900370882545\n",
      "train loss:0.02385312528497633\n",
      "train loss:0.0027033395321467302\n",
      "train loss:0.005056514234235489\n",
      "train loss:0.003201583836908186\n",
      "train loss:0.001403734321827624\n",
      "train loss:0.010259853642789363\n",
      "train loss:0.007187542452836262\n",
      "=== epoch:11, train acc:0.997, test acc:0.985 ===\n",
      "train loss:0.016776812713357653\n",
      "train loss:0.0035414482635966727\n",
      "train loss:0.01652819488122628\n",
      "train loss:0.0027613059649246397\n",
      "train loss:0.005589573821555254\n",
      "train loss:0.0003904687533212823\n",
      "train loss:0.0009387926843780929\n",
      "train loss:0.0015379389191002813\n",
      "train loss:0.0011166048394529895\n",
      "train loss:0.0013979319831174326\n",
      "train loss:0.017278631095084804\n",
      "train loss:0.0028793473102418092\n",
      "train loss:0.0024926329008359335\n",
      "train loss:0.010984843840356709\n",
      "train loss:0.007654872347244129\n",
      "train loss:0.0012760737618031947\n",
      "train loss:0.0008706226899475272\n",
      "train loss:0.002500073044002748\n",
      "train loss:0.0005992246569352352\n",
      "train loss:0.0004925358588733955\n",
      "train loss:0.0022606064508871727\n",
      "train loss:0.00046036623298831444\n",
      "train loss:0.0018241480812571209\n",
      "train loss:0.08626649889541045\n",
      "train loss:0.0029274416575978085\n",
      "train loss:0.007149229277461253\n",
      "train loss:0.007194045034997929\n",
      "train loss:0.0016090599091032071\n",
      "train loss:0.023180170323706915\n",
      "train loss:0.0011733530313436347\n",
      "train loss:0.0021347011327004224\n",
      "train loss:0.0009770499400180612\n",
      "train loss:0.0023504467459052776\n",
      "train loss:0.000793640611650241\n",
      "train loss:0.009960031008953924\n",
      "train loss:0.005580492400700299\n",
      "train loss:0.01765569691198368\n",
      "train loss:0.0019626230731948184\n",
      "train loss:0.0019424399428172499\n",
      "train loss:0.003618553143164067\n",
      "train loss:0.0008547692140692474\n",
      "train loss:0.0074079048060549865\n",
      "train loss:0.0021750244304222103\n",
      "train loss:0.004452593529522411\n",
      "train loss:0.005729650191406073\n",
      "train loss:0.0014973265953834728\n",
      "train loss:0.0037126769536065447\n",
      "train loss:0.001578409296753906\n",
      "train loss:0.00016041722891135227\n",
      "train loss:0.0010231676152237888\n",
      "train loss:0.0037747154753940567\n",
      "train loss:0.0013591327848923404\n",
      "train loss:0.0034350526713022352\n",
      "train loss:0.008085261342955181\n",
      "train loss:0.001367851690451516\n",
      "train loss:0.0014511205417657533\n",
      "train loss:0.0003311620217626822\n",
      "train loss:0.00161868024806471\n",
      "train loss:0.0039738094468007\n",
      "train loss:0.02666216151315128\n",
      "train loss:0.0031659551070803388\n",
      "train loss:0.0018423784947348368\n",
      "train loss:0.019914795887714234\n",
      "train loss:0.0017790374777048282\n",
      "train loss:0.0032626682161721082\n",
      "train loss:0.004564795502287225\n",
      "train loss:0.0008117120588475565\n",
      "train loss:0.017415266952350635\n",
      "train loss:0.005385152834783693\n",
      "train loss:0.000556270007169891\n",
      "train loss:0.006313705717660279\n",
      "train loss:0.005494822260950271\n",
      "train loss:0.008867455855535736\n",
      "train loss:0.0033102268886141643\n",
      "train loss:0.028551048462239464\n",
      "train loss:0.002622210283904553\n",
      "train loss:0.0032750971188453236\n",
      "train loss:0.01563586851276127\n",
      "train loss:0.0015472666639904107\n",
      "train loss:0.004387707364067253\n",
      "train loss:0.002318518668431836\n",
      "train loss:0.0022271935442353184\n",
      "train loss:0.006132114098569801\n",
      "train loss:0.001960457067413042\n",
      "train loss:0.0018318468530034062\n",
      "train loss:0.001407305007264397\n",
      "train loss:0.002168042626582937\n",
      "train loss:0.00996158616902828\n",
      "train loss:0.0009011452548536818\n",
      "train loss:0.0026306377867114728\n",
      "train loss:0.0026940250759256556\n",
      "train loss:0.00600131167034011\n",
      "train loss:0.0005426105289794409\n",
      "train loss:0.0003428328292744071\n",
      "train loss:0.004881268726571665\n",
      "train loss:0.04030194337163254\n",
      "train loss:0.015993337792430003\n",
      "train loss:0.0009669567839705655\n",
      "train loss:0.0037733516713861536\n",
      "train loss:0.010253909863120778\n",
      "train loss:0.0021543549805590847\n",
      "train loss:0.007090586505674955\n",
      "train loss:0.0015707558595766675\n",
      "train loss:0.039058516591466315\n",
      "train loss:0.02620658204802996\n",
      "train loss:0.0029430188185564934\n",
      "train loss:0.004792933838651096\n",
      "train loss:0.0037504760122671166\n",
      "train loss:0.006247534420326566\n",
      "train loss:0.0020825692607739843\n",
      "train loss:0.013024781422698266\n",
      "train loss:0.0059286119326457274\n",
      "train loss:0.0014538530947430328\n",
      "train loss:0.01756127656531897\n",
      "train loss:0.0029570307949905363\n",
      "train loss:0.002115714632155968\n",
      "train loss:0.012427521642028363\n",
      "train loss:0.0011074837884189049\n",
      "train loss:0.008299323881780835\n",
      "train loss:0.008417809108637223\n",
      "train loss:0.0033368245210136337\n",
      "train loss:0.0026990785269320663\n",
      "train loss:0.0021335618284044487\n",
      "train loss:0.0013259026179177089\n",
      "train loss:0.005855569103977606\n",
      "train loss:0.02533045871331282\n",
      "train loss:0.006980181093066782\n",
      "train loss:0.0035188118291373683\n",
      "train loss:0.0017513006031781792\n",
      "train loss:0.0008581502008849823\n",
      "train loss:0.0006057556465018835\n",
      "train loss:0.0009134903593176856\n",
      "train loss:0.0010938466638046255\n",
      "train loss:0.0007373432648670069\n",
      "train loss:0.06683470628056605\n",
      "train loss:0.004911333742132636\n",
      "train loss:0.002909988714529672\n",
      "train loss:0.004693180052950463\n",
      "train loss:0.002710696549407026\n",
      "train loss:0.017186129670512005\n",
      "train loss:0.005507478139783807\n",
      "train loss:0.0009148710273948509\n",
      "train loss:0.0005664913100112259\n",
      "train loss:0.0007569843528915497\n",
      "train loss:0.0025101998448103986\n",
      "train loss:0.007216066303689345\n",
      "train loss:0.0056231188156779\n",
      "train loss:0.0026900710505520175\n",
      "train loss:0.0026786469245369377\n",
      "train loss:0.016095292088413534\n",
      "train loss:0.005484853512237646\n",
      "train loss:0.004229767117413349\n",
      "train loss:0.002802392920136944\n",
      "train loss:0.00106690968380229\n",
      "train loss:0.0009638973062383091\n",
      "train loss:0.012182725280623261\n",
      "train loss:0.00043636212213933964\n",
      "train loss:0.0006828136535508689\n",
      "train loss:0.001268734323244753\n",
      "train loss:0.00520127886819054\n",
      "train loss:0.004451061024686615\n",
      "train loss:0.002615570382032521\n",
      "train loss:0.0038560696311135774\n",
      "train loss:0.002639827174846381\n",
      "train loss:0.0057818725300066605\n",
      "train loss:0.07054354854405083\n",
      "train loss:0.0007491713233351248\n",
      "train loss:0.0030041075690408407\n",
      "train loss:0.0022929819714689616\n",
      "train loss:0.008776387544614605\n",
      "train loss:0.007099140154885966\n",
      "train loss:0.0011711148587787543\n",
      "train loss:0.001156903097741971\n",
      "train loss:0.0013667726135695763\n",
      "train loss:0.0008209146727887848\n",
      "train loss:0.0007985379945575394\n",
      "train loss:0.006830874737709411\n",
      "train loss:0.0019512497300074475\n",
      "train loss:0.004229379325132296\n",
      "train loss:0.001836769086198093\n",
      "train loss:0.000722197071629126\n",
      "train loss:0.015502364231139577\n",
      "train loss:0.004507363324307763\n",
      "train loss:0.0010208821747779578\n",
      "train loss:0.001215870926985654\n",
      "train loss:0.0011180193260469414\n",
      "train loss:0.0006898570654196549\n",
      "train loss:0.0015961463128238062\n",
      "train loss:0.0018408782048045865\n",
      "train loss:0.0002486523216392723\n",
      "train loss:0.00975388288384768\n",
      "train loss:0.001447162066911297\n",
      "train loss:0.004035537162700662\n",
      "train loss:0.014921752754921051\n",
      "train loss:0.009301477693112875\n",
      "train loss:0.0017430713477429988\n",
      "train loss:0.0008550027026678368\n",
      "train loss:0.0001143121403293027\n",
      "train loss:0.0013627733229160117\n",
      "train loss:0.00710622625735701\n",
      "train loss:0.0014165066081146302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0015487281876077424\n",
      "train loss:0.0023706301777557635\n",
      "train loss:0.00241216265193882\n",
      "train loss:0.0004338790571833095\n",
      "train loss:0.009809384066260638\n",
      "train loss:0.0019450511187003353\n",
      "train loss:0.0032469293076168356\n",
      "train loss:0.002490075198127317\n",
      "train loss:0.004383348532703121\n",
      "train loss:0.01680631749668347\n",
      "train loss:0.0021255487464866673\n",
      "train loss:0.0017303305300194053\n",
      "train loss:0.0014276849510834544\n",
      "train loss:0.002643698081160167\n",
      "train loss:0.0013930664561096304\n",
      "train loss:0.004213888011115042\n",
      "train loss:0.0034122037663685827\n",
      "train loss:0.006241896943727418\n",
      "train loss:0.0115445913363585\n",
      "train loss:0.015756100707914\n",
      "train loss:0.0007716658168976003\n",
      "train loss:0.0025708870075000527\n",
      "train loss:0.0022044058868146564\n",
      "train loss:0.0011359781815003139\n",
      "train loss:0.011109584672625551\n",
      "train loss:0.0007546795397203958\n",
      "train loss:0.004925830780832774\n",
      "train loss:0.011041975136872778\n",
      "train loss:0.011065088040354956\n",
      "train loss:0.00038937749800467493\n",
      "train loss:0.004355699043304548\n",
      "train loss:0.0028591955397404036\n",
      "train loss:0.004390412848568699\n",
      "train loss:0.0037764019496621636\n",
      "train loss:0.002081657800276406\n",
      "train loss:0.006634350607074608\n",
      "train loss:0.010443991899045895\n",
      "train loss:0.007696921602819115\n",
      "train loss:0.0005401488233097325\n",
      "train loss:0.005352321968189344\n",
      "train loss:0.0015613712719841249\n",
      "train loss:0.005485243449013394\n",
      "train loss:0.005082885490482364\n",
      "train loss:0.00011508574928711724\n",
      "train loss:0.0027832689607867967\n",
      "train loss:0.00120349161049647\n",
      "train loss:0.0010955577505485084\n",
      "train loss:0.005969350124768497\n",
      "train loss:0.0018248222884965875\n",
      "train loss:0.003572689228933812\n",
      "train loss:0.005814085469340278\n",
      "train loss:0.003614935608299408\n",
      "train loss:0.009637081678653382\n",
      "train loss:0.0041733458610137865\n",
      "train loss:0.004173435994257747\n",
      "train loss:0.00018811708320980392\n",
      "train loss:0.017961503875255604\n",
      "train loss:0.003339053191430604\n",
      "train loss:0.00015076434399376837\n",
      "train loss:0.03233901734869161\n",
      "train loss:0.0006724315056040601\n",
      "train loss:0.0017532368145936337\n",
      "train loss:7.521808648879881e-05\n",
      "train loss:0.001721040864048674\n",
      "train loss:0.003955041656232821\n",
      "train loss:0.00046272862084782316\n",
      "train loss:0.0007315316594783391\n",
      "train loss:0.0022606734699281926\n",
      "train loss:0.0009335116469983576\n",
      "train loss:0.001368127446783035\n",
      "train loss:0.004182588466096634\n",
      "train loss:0.0015393521596953935\n",
      "train loss:0.03733925287050862\n",
      "train loss:0.0035274343108436714\n",
      "train loss:0.001060021104025388\n",
      "train loss:0.002247543584967803\n",
      "train loss:0.005678886946682573\n",
      "train loss:0.002916394295727728\n",
      "train loss:0.0013823699925999092\n",
      "train loss:0.007650511332544248\n",
      "train loss:0.0007172186246062815\n",
      "train loss:0.004229463125309248\n",
      "train loss:0.001735453742969563\n",
      "train loss:0.0007097464659834407\n",
      "train loss:0.0396373341693232\n",
      "train loss:0.004793587803114793\n",
      "train loss:0.005831252791419419\n",
      "train loss:0.0042748139923991695\n",
      "train loss:0.00012422352259559453\n",
      "train loss:0.007633538958473733\n",
      "train loss:0.005258404163835644\n",
      "train loss:0.0008719325353580232\n",
      "train loss:0.004511941601001685\n",
      "train loss:0.01649618324785042\n",
      "train loss:0.003091447392797356\n",
      "train loss:0.003880453490970399\n",
      "train loss:0.013277226528837944\n",
      "train loss:0.00229984393487307\n",
      "train loss:0.00020473453205070109\n",
      "train loss:0.004226877350548262\n",
      "train loss:0.000335300540915659\n",
      "train loss:0.001878594524454299\n",
      "train loss:0.001900374300645808\n",
      "train loss:0.0010336079074151712\n",
      "train loss:0.00852328386558524\n",
      "train loss:0.004882290176985937\n",
      "train loss:0.012161403108759787\n",
      "train loss:0.002021762702421373\n",
      "train loss:0.0034469573938710795\n",
      "train loss:0.0030830704789331985\n",
      "train loss:0.0049035588906784855\n",
      "train loss:0.009619007693843756\n",
      "train loss:0.0003704349259501518\n",
      "train loss:0.00018757175714133282\n",
      "train loss:0.0014084403526389874\n",
      "train loss:0.0014328152610675149\n",
      "train loss:0.0007489087926393807\n",
      "train loss:0.002943929779492943\n",
      "train loss:0.0016415886777195472\n",
      "train loss:0.0007119198277210454\n",
      "train loss:0.005935166500081639\n",
      "train loss:0.00019390648770518996\n",
      "train loss:0.007214419657185777\n",
      "train loss:0.005451437566902929\n",
      "train loss:0.002784338860067334\n",
      "train loss:0.001187560233153481\n",
      "train loss:0.011657086667688136\n",
      "train loss:0.021630249951624294\n",
      "train loss:0.0019681826668642003\n",
      "train loss:0.0008919692757135094\n",
      "train loss:0.00162289563345634\n",
      "train loss:0.0016608412626523419\n",
      "train loss:0.006222628723420701\n",
      "train loss:0.0033752992189227814\n",
      "train loss:0.007460063085303449\n",
      "train loss:0.002761908458991634\n",
      "train loss:0.005611019321009045\n",
      "train loss:0.011469369364393222\n",
      "train loss:0.0005018469702418174\n",
      "train loss:0.0023760831729791832\n",
      "train loss:0.0009069107517437796\n",
      "train loss:0.0003301838396783504\n",
      "train loss:0.0064049576431185916\n",
      "train loss:0.010646466797991942\n",
      "train loss:0.001647184572794662\n",
      "train loss:0.0036140475488817747\n",
      "train loss:0.003924666906069793\n",
      "train loss:0.0018343659949311367\n",
      "train loss:0.0011443814974882844\n",
      "train loss:0.0030427241756362366\n",
      "train loss:0.0024921788298243037\n",
      "train loss:0.005468433538317409\n",
      "train loss:0.012106605741650862\n",
      "train loss:0.0037917994959963063\n",
      "train loss:0.001782961370194671\n",
      "train loss:0.0009097680850973466\n",
      "train loss:0.009069797376916143\n",
      "train loss:0.001921835856693676\n",
      "train loss:0.004089142714722979\n",
      "train loss:0.0016292308676954148\n",
      "train loss:0.00643795185309874\n",
      "train loss:0.010441325581467297\n",
      "train loss:0.002392191325904683\n",
      "train loss:0.0009597075252215396\n",
      "train loss:0.0011146764166906902\n",
      "train loss:0.005989067744292191\n",
      "train loss:0.004598989138923012\n",
      "train loss:0.002230294047228463\n",
      "train loss:0.007753232065585507\n",
      "train loss:0.21606175112915818\n",
      "train loss:0.007859262345051195\n",
      "train loss:0.0012536309900363187\n",
      "train loss:0.001395293334507395\n",
      "train loss:0.002357626152196959\n",
      "train loss:0.0013369911693312884\n",
      "train loss:0.001938459042086295\n",
      "train loss:0.0014604469850013507\n",
      "train loss:0.010337360965169316\n",
      "train loss:0.007062324712586198\n",
      "train loss:0.0008523228367725989\n",
      "train loss:0.003094509053842652\n",
      "train loss:0.00044559403687668004\n",
      "train loss:0.003611946523576451\n",
      "train loss:0.004391571710642747\n",
      "train loss:0.0007793752837094626\n",
      "train loss:0.0071305728819702294\n",
      "train loss:0.002198265538598549\n",
      "train loss:0.004541673861387256\n",
      "train loss:0.0008557022891063663\n",
      "train loss:0.0011731403928647033\n",
      "train loss:0.002610717736745713\n",
      "train loss:0.006665529156138694\n",
      "train loss:0.002420495276343412\n",
      "train loss:0.0008653231873938595\n",
      "train loss:0.00037133720271707\n",
      "train loss:0.0004441588183282992\n",
      "train loss:0.00032494252178118054\n",
      "train loss:0.00020780933007088573\n",
      "train loss:0.0015655735789721662\n",
      "train loss:0.001223148761791561\n",
      "train loss:0.000468604086814698\n",
      "train loss:0.0012931350333873234\n",
      "train loss:0.0017948032595112423\n",
      "train loss:0.0022636709177800245\n",
      "train loss:0.011611430784926687\n",
      "train loss:0.0011635321517755573\n",
      "train loss:0.00368965248525011\n",
      "train loss:0.004811438524274919\n",
      "train loss:0.00036202081731446205\n",
      "train loss:0.004548818093293185\n",
      "train loss:0.001437969804618999\n",
      "train loss:0.00042034854662626397\n",
      "train loss:0.0012979802510897128\n",
      "train loss:0.00123235670354541\n",
      "train loss:0.00342893196078312\n",
      "train loss:0.0011319200972780175\n",
      "train loss:0.0047392801610059675\n",
      "train loss:0.0018582884967931193\n",
      "train loss:0.00120750980975927\n",
      "train loss:0.006113100996563762\n",
      "train loss:0.0025535216584857494\n",
      "train loss:0.0009953594834958774\n",
      "train loss:0.005506833254698711\n",
      "train loss:0.0005957846501755586\n",
      "train loss:0.006511840203362865\n",
      "train loss:0.0008570424690958296\n",
      "train loss:0.00573644817548194\n",
      "train loss:0.016963619460287443\n",
      "train loss:0.00041158909241062843\n",
      "train loss:0.059107039245880635\n",
      "train loss:0.005574783272342726\n",
      "train loss:0.003300458829683207\n",
      "train loss:0.0015750675250541363\n",
      "train loss:0.0007645127998424333\n",
      "train loss:0.004224490451986317\n",
      "train loss:0.006600638970144511\n",
      "train loss:0.004612215773744144\n",
      "train loss:0.00413290906397168\n",
      "train loss:0.0031472081154954247\n",
      "train loss:0.0015242491617201675\n",
      "train loss:0.006550361469515736\n",
      "train loss:0.020586910908224124\n",
      "train loss:0.003457800396343324\n",
      "train loss:0.014025549576117893\n",
      "train loss:0.0013828927514897105\n",
      "train loss:0.0017591975889638884\n",
      "train loss:0.0006647946671977176\n",
      "train loss:0.00151281041900242\n",
      "train loss:0.025304413353085187\n",
      "train loss:0.001718131649115064\n",
      "train loss:0.009988383889571247\n",
      "train loss:0.0004987318273197899\n",
      "train loss:0.008225085900790319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.004879698495615311\n",
      "train loss:0.017692233086543246\n",
      "train loss:0.0017950190571554556\n",
      "train loss:0.000326848679074707\n",
      "train loss:0.00042603558194689856\n",
      "train loss:0.001444120023193063\n",
      "train loss:0.016445568309834978\n",
      "train loss:0.0032213664503516815\n",
      "train loss:0.0034880813569114315\n",
      "train loss:0.02436774422600418\n",
      "train loss:0.0008648721729007139\n",
      "train loss:0.0033757241945058425\n",
      "train loss:0.004593626506637341\n",
      "train loss:0.002545312599921848\n",
      "train loss:0.000260355389584227\n",
      "train loss:0.007601920470668508\n",
      "train loss:0.0019121161002356294\n",
      "train loss:0.0027341969921295312\n",
      "train loss:0.00032505794095298547\n",
      "train loss:0.0026550666437547494\n",
      "train loss:0.0016360864115579272\n",
      "train loss:0.0011140737919213735\n",
      "train loss:0.0027455089306545727\n",
      "train loss:0.0023858311292365027\n",
      "train loss:0.0038887182071556687\n",
      "train loss:0.0006937182004973189\n",
      "train loss:0.010756065739445875\n",
      "train loss:0.005642529056556652\n",
      "train loss:0.0006770029406405455\n",
      "train loss:0.005454362270748407\n",
      "train loss:0.002669855348306266\n",
      "train loss:0.002487452896865328\n",
      "train loss:0.0008126442441840788\n",
      "train loss:0.06466825672125377\n",
      "train loss:0.0037273574857399354\n",
      "train loss:0.016123317434380988\n",
      "train loss:0.0013040003032240616\n",
      "train loss:0.0018824097550766553\n",
      "train loss:0.0002367831172571012\n",
      "train loss:0.0005556849041796406\n",
      "train loss:0.005376405383244672\n",
      "train loss:0.0030602385403936355\n",
      "train loss:0.005484087775261406\n",
      "train loss:0.005508371975489407\n",
      "train loss:0.002420821646909531\n",
      "train loss:0.0059928793113780765\n",
      "train loss:0.0009715864433376184\n",
      "train loss:0.00516745059839522\n",
      "train loss:0.0017105345939548717\n",
      "train loss:0.038507009009884616\n",
      "train loss:0.001999635127160973\n",
      "train loss:0.0035634768251118294\n",
      "train loss:0.001233903624473602\n",
      "train loss:9.000313084890545e-05\n",
      "train loss:0.0024927143394254726\n",
      "train loss:0.0025070254326858515\n",
      "train loss:0.004664897563746408\n",
      "train loss:0.0009343800991337746\n",
      "train loss:0.0010607978266615385\n",
      "train loss:0.002780750326681199\n",
      "train loss:0.0022137775652242495\n",
      "train loss:0.0023082384290314143\n",
      "train loss:0.0064820863469709136\n",
      "train loss:0.0010753029855299966\n",
      "train loss:0.0016182825045737822\n",
      "train loss:0.02990561124753453\n",
      "train loss:0.0053277516310341185\n",
      "train loss:0.0017634309372246953\n",
      "train loss:0.0021279316757218613\n",
      "train loss:0.001522768858700751\n",
      "train loss:0.007824594590990224\n",
      "train loss:0.000821527040812529\n",
      "train loss:0.004584298502310704\n",
      "train loss:0.0037351399237686824\n",
      "train loss:0.03210251302960579\n",
      "train loss:0.0034588076258365024\n",
      "train loss:0.0016977836581746986\n",
      "train loss:0.006179306331483776\n",
      "train loss:0.001802407368718523\n",
      "train loss:0.003493757314477906\n",
      "train loss:0.0071633835461324\n",
      "train loss:0.006532841783129271\n",
      "train loss:0.005134945989047787\n",
      "train loss:0.0015612463094019772\n",
      "train loss:0.0012093993502739057\n",
      "train loss:0.0006003944761368801\n",
      "train loss:0.002018420993266807\n",
      "train loss:0.0021277613991882364\n",
      "train loss:0.000636854795001912\n",
      "train loss:0.0015057899662384833\n",
      "train loss:0.0016773366490369544\n",
      "train loss:0.010625027261040712\n",
      "train loss:0.0005578860695878508\n",
      "train loss:0.0050481752491038645\n",
      "train loss:0.0017887063155250834\n",
      "train loss:0.0019260204638016382\n",
      "train loss:0.0013983029761307458\n",
      "train loss:0.0012380066714481376\n",
      "train loss:0.009161949204512078\n",
      "train loss:0.001280863740690305\n",
      "train loss:0.007314310455437856\n",
      "train loss:0.06026623850007251\n",
      "train loss:0.0001905918841108035\n",
      "train loss:0.002161749198922947\n",
      "train loss:0.0011526281139835897\n",
      "train loss:0.006400151586421075\n",
      "train loss:0.00022784225722632267\n",
      "train loss:0.010415253017536107\n",
      "train loss:0.0026231224127656414\n",
      "train loss:0.0006444298197320593\n",
      "train loss:0.00613123133453859\n",
      "train loss:0.006037065378587393\n",
      "train loss:0.012536876474163237\n",
      "train loss:0.001819092330104517\n",
      "train loss:0.0005619837647223756\n",
      "train loss:0.00574508090783322\n",
      "train loss:0.0009694106022213565\n",
      "train loss:0.0007761730884325782\n",
      "train loss:0.00038296501027280103\n",
      "train loss:0.00035485792358903515\n",
      "train loss:0.0021299472276865863\n",
      "train loss:0.0022491516092624673\n",
      "train loss:0.00883079288939889\n",
      "train loss:0.0021118346956973478\n",
      "train loss:0.001022066719500568\n",
      "train loss:0.0010659026012542281\n",
      "train loss:0.0020723815929134487\n",
      "train loss:0.0024272336166825705\n",
      "train loss:0.0028430105068837346\n",
      "train loss:0.0026877691929229177\n",
      "train loss:0.0014779636159680614\n",
      "train loss:0.00843274440140363\n",
      "train loss:0.00966666036343496\n",
      "train loss:0.0007440196790747795\n",
      "train loss:0.009114690648087542\n",
      "train loss:0.0005350709124161694\n",
      "train loss:0.006147813696946872\n",
      "train loss:0.0029179219658382327\n",
      "train loss:0.0022074544927699977\n",
      "train loss:0.023338020331018217\n",
      "train loss:0.04361895037288846\n",
      "train loss:0.006485769431286517\n",
      "train loss:0.0629857269074867\n",
      "train loss:0.03842219655817315\n",
      "train loss:0.02201794659160044\n",
      "train loss:0.0037856000382659235\n",
      "=== epoch:12, train acc:0.997, test acc:0.986 ===\n",
      "train loss:0.0013441915173874406\n",
      "train loss:0.001563856868279698\n",
      "train loss:0.0026604087641300205\n",
      "train loss:0.002383572235474405\n",
      "train loss:0.008473468602595798\n",
      "train loss:0.008398858584639507\n",
      "train loss:0.004059286054849996\n",
      "train loss:0.006891470685664509\n",
      "train loss:0.0026964407075948343\n",
      "train loss:0.0020927339872505537\n",
      "train loss:0.0013649076831916578\n",
      "train loss:0.004386681210484542\n",
      "train loss:0.0033860812369855475\n",
      "train loss:0.0023141976612900492\n",
      "train loss:0.0007400448148267815\n",
      "train loss:0.002699439816882262\n",
      "train loss:0.00545676449444919\n",
      "train loss:0.020177699279631197\n",
      "train loss:0.005716342429911945\n",
      "train loss:0.00020747067386686342\n",
      "train loss:0.002102687100770995\n",
      "train loss:0.006796141393166917\n",
      "train loss:0.0011769075149989944\n",
      "train loss:0.0017891208303581962\n",
      "train loss:0.0035635933480017757\n",
      "train loss:0.00017476972113606256\n",
      "train loss:0.06904427744737551\n",
      "train loss:0.0013794385209723447\n",
      "train loss:0.002305409179796646\n",
      "train loss:0.00019619497945080463\n",
      "train loss:0.0025162406526620124\n",
      "train loss:0.007807565115957879\n",
      "train loss:0.0026050499669871725\n",
      "train loss:0.003557847479469085\n",
      "train loss:0.0040786713751787855\n",
      "train loss:0.0007305003360373598\n",
      "train loss:0.019687828897957265\n",
      "train loss:0.0005040938460389036\n",
      "train loss:0.004936046553078255\n",
      "train loss:0.0022237105276552613\n",
      "train loss:0.0016277292303271366\n",
      "train loss:0.020610738010917354\n",
      "train loss:0.0010947632574319118\n",
      "train loss:0.0005647008966948428\n",
      "train loss:0.00389874668532003\n",
      "train loss:0.0036858819936848998\n",
      "train loss:0.00010335255932753934\n",
      "train loss:0.0020019756367848237\n",
      "train loss:0.0033001254325260286\n",
      "train loss:0.0009406855286762622\n",
      "train loss:0.004422819451496179\n",
      "train loss:0.001780805166841393\n",
      "train loss:0.0003331988728003819\n",
      "train loss:0.000944859655121317\n",
      "train loss:0.001868420496447056\n",
      "train loss:0.002184982862009816\n",
      "train loss:0.0010393796350276112\n",
      "train loss:0.006833458904879415\n",
      "train loss:0.0020754655103406585\n",
      "train loss:0.0011752145115169349\n",
      "train loss:0.0010182282510427828\n",
      "train loss:0.001839046933854336\n",
      "train loss:0.0020512379422231945\n",
      "train loss:0.00035905421536988147\n",
      "train loss:0.0009459438900487896\n",
      "train loss:0.0005629167340679733\n",
      "train loss:0.0007130535475258702\n",
      "train loss:0.0014486034882015286\n",
      "train loss:0.0056819672370833265\n",
      "train loss:0.0024569475623060933\n",
      "train loss:0.0006936945458576708\n",
      "train loss:0.0006725962270809961\n",
      "train loss:0.0006741478999808333\n",
      "train loss:0.0018331406728326945\n",
      "train loss:0.004439820576481767\n",
      "train loss:0.0026281345737462465\n",
      "train loss:0.0006399962322327885\n",
      "train loss:0.013538051221446792\n",
      "train loss:0.0005205524104504341\n",
      "train loss:0.00314062116967133\n",
      "train loss:0.002217646698572386\n",
      "train loss:0.0023194764757388337\n",
      "train loss:0.004915531197294954\n",
      "train loss:0.0006460777148183201\n",
      "train loss:0.0023514466040248485\n",
      "train loss:0.09540496468719616\n",
      "train loss:0.002613504259814916\n",
      "train loss:0.000371899774522171\n",
      "train loss:0.0005013927334388421\n",
      "train loss:0.038801493239855905\n",
      "train loss:0.0007261021298966779\n",
      "train loss:0.00043807168957712083\n",
      "train loss:7.366561271466722e-05\n",
      "train loss:0.00021668675821860057\n",
      "train loss:0.0017593104321349407\n",
      "train loss:0.00033628011661063516\n",
      "train loss:0.0018036318402880828\n",
      "train loss:0.010913550579925648\n",
      "train loss:0.00026023651243611183\n",
      "train loss:0.0021769793654427627\n",
      "train loss:0.0044547323543342844\n",
      "train loss:0.0028959761706495585\n",
      "train loss:0.0007201017419945733\n",
      "train loss:0.0022821814965130315\n",
      "train loss:0.005618394975606143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.002981513058611303\n",
      "train loss:0.006656208470577456\n",
      "train loss:0.00025497290066901166\n",
      "train loss:0.001236384409190514\n",
      "train loss:0.0005340343839953472\n",
      "train loss:0.007862032756210948\n",
      "train loss:0.00015488480112688119\n",
      "train loss:0.0009335558135606548\n",
      "train loss:0.0008327430727283921\n",
      "train loss:0.004642417705835748\n",
      "train loss:0.0007316524315808997\n",
      "train loss:0.0018269436120823602\n",
      "train loss:0.0032578595893507545\n",
      "train loss:0.00047412594498276824\n",
      "train loss:0.0005685345977816809\n",
      "train loss:0.0030360774386420045\n",
      "train loss:0.0008521996537243836\n",
      "train loss:0.003876700624388467\n",
      "train loss:0.0007960129159825182\n",
      "train loss:0.0016636937928987089\n",
      "train loss:0.0015604883417277705\n",
      "train loss:0.002261675076581077\n",
      "train loss:0.020752809158100582\n",
      "train loss:0.0015837725369300382\n",
      "train loss:0.00409281804273737\n",
      "train loss:0.0006388618632834426\n",
      "train loss:0.0018452477047643768\n",
      "train loss:0.004213799709091141\n",
      "train loss:0.0027341093435543163\n",
      "train loss:0.001178781549644093\n",
      "train loss:0.001420472838618603\n",
      "train loss:0.00024382980480064565\n",
      "train loss:0.0028519599699598935\n",
      "train loss:0.002038128447084952\n",
      "train loss:0.0012799784210346266\n",
      "train loss:0.001722787396711045\n",
      "train loss:0.0032638375346896514\n",
      "train loss:0.0009463130035930006\n",
      "train loss:0.0004003059556089185\n",
      "train loss:0.000598616150711611\n",
      "train loss:0.002393372806869702\n",
      "train loss:0.00033730454157037564\n",
      "train loss:0.0016163299650111358\n",
      "train loss:0.0008078857145228153\n",
      "train loss:0.0003544319720550984\n",
      "train loss:0.009499201621643527\n",
      "train loss:0.00032581810715643283\n",
      "train loss:0.004972043019284413\n",
      "train loss:0.001508275946239168\n",
      "train loss:0.0005006354564855162\n",
      "train loss:0.0008398653175284149\n",
      "train loss:0.005511887631426793\n",
      "train loss:0.011594213646512228\n",
      "train loss:0.00016211873438066936\n",
      "train loss:0.002697890457037745\n",
      "train loss:0.0008188516768884494\n",
      "train loss:0.001045865080971242\n",
      "train loss:5.912798237249969e-05\n",
      "train loss:0.00725743091848851\n",
      "train loss:0.00045821693752085674\n",
      "train loss:0.0032531986906226733\n",
      "train loss:0.0002366483816628492\n",
      "train loss:0.00090525158063474\n",
      "train loss:0.0009181630989793219\n",
      "train loss:0.0038045288384087102\n",
      "train loss:0.00048080581767536915\n",
      "train loss:0.0006382049389519548\n",
      "train loss:0.0006648678716492686\n",
      "train loss:0.00872436403994857\n",
      "train loss:0.003142885469879739\n",
      "train loss:0.0014729129625218626\n",
      "train loss:0.0029713850679305228\n",
      "train loss:0.0013509768965989514\n",
      "train loss:0.005465260866381194\n",
      "train loss:0.0009788303771739828\n",
      "train loss:0.017484478392265156\n",
      "train loss:0.0012988887373962333\n",
      "train loss:0.0010748918519619217\n",
      "train loss:0.0015830638803868522\n",
      "train loss:0.002295746403857509\n",
      "train loss:0.0036401483834987954\n",
      "train loss:0.005258711985833201\n",
      "train loss:0.0010473264506378\n",
      "train loss:0.002051794486220849\n",
      "train loss:0.006529391818298882\n",
      "train loss:0.0014727702947038485\n",
      "train loss:5.449030364126962e-05\n",
      "train loss:0.002092212186137543\n",
      "train loss:0.00035789844256686634\n",
      "train loss:0.00059969526905858\n",
      "train loss:0.0015471222371404125\n",
      "train loss:0.0053591805761024226\n",
      "train loss:0.0007177044860208004\n",
      "train loss:0.0007329478738303747\n",
      "train loss:0.0061734163661945765\n",
      "train loss:0.0012794883552627394\n",
      "train loss:0.00014242340884096673\n",
      "train loss:0.005389183842122266\n",
      "train loss:0.0023949395689919755\n",
      "train loss:0.00044063764631690523\n",
      "train loss:0.0019018575693405778\n",
      "train loss:0.002056351141704796\n",
      "train loss:0.04218154263386686\n",
      "train loss:0.003204463585070461\n",
      "train loss:0.004202763526495772\n",
      "train loss:6.726421133303467e-05\n",
      "train loss:0.007496053757160361\n",
      "train loss:0.003407121369235001\n",
      "train loss:0.011351083586190549\n",
      "train loss:0.001320742675868793\n",
      "train loss:0.0048513984934871164\n",
      "train loss:0.007939252400665694\n",
      "train loss:0.0018904380110262504\n",
      "train loss:0.0032583051727009048\n",
      "train loss:0.001491291625386494\n",
      "train loss:0.0008982938669401451\n",
      "train loss:0.0021960386032353497\n",
      "train loss:0.0028673673544372764\n",
      "train loss:9.399962903191457e-05\n",
      "train loss:0.017382410504668832\n",
      "train loss:0.0071028006937558855\n",
      "train loss:0.002412414027529668\n",
      "train loss:0.0005820858953326517\n",
      "train loss:0.0004175655242653453\n",
      "train loss:0.002090303634907825\n",
      "train loss:0.0011631812234971822\n",
      "train loss:1.4762195636764633e-05\n",
      "train loss:0.0005182921024023015\n",
      "train loss:0.0016576327871644764\n",
      "train loss:0.0024498067338904308\n",
      "train loss:0.0014021602059318536\n",
      "train loss:0.0020710003645295766\n",
      "train loss:0.010190175531239812\n",
      "train loss:0.0021920657999926767\n",
      "train loss:0.003377509122157681\n",
      "train loss:0.0007845713899473407\n",
      "train loss:0.0028238013153792114\n",
      "train loss:0.00010450239107496874\n",
      "train loss:0.02413351511370105\n",
      "train loss:0.004411363086692927\n",
      "train loss:0.00048359989869567547\n",
      "train loss:0.039879817299259275\n",
      "train loss:0.00041132444034471574\n",
      "train loss:0.003059767752273458\n",
      "train loss:0.00034188402025591607\n",
      "train loss:0.0055250122737292824\n",
      "train loss:0.0006417922480659653\n",
      "train loss:0.0021395017786907944\n",
      "train loss:0.0004890245287847958\n",
      "train loss:0.0014910367200843097\n",
      "train loss:0.0023816882718001742\n",
      "train loss:0.0015842567903078175\n",
      "train loss:0.0025022945457764388\n",
      "train loss:0.00119382755950523\n",
      "train loss:0.0033954634676975604\n",
      "train loss:0.0002341285347361653\n",
      "train loss:0.0006522483677509938\n",
      "train loss:0.00030137098686459603\n",
      "train loss:0.00248252559785705\n",
      "train loss:0.000489603651697648\n",
      "train loss:0.0015600475143359876\n",
      "train loss:0.005075335719363507\n",
      "train loss:0.0016023616630268837\n",
      "train loss:0.00035589536362587703\n",
      "train loss:0.00017194895245532547\n",
      "train loss:0.006169435638876781\n",
      "train loss:0.0004594727100614003\n",
      "train loss:0.004033765973971872\n",
      "train loss:0.00029595555711610415\n",
      "train loss:0.0010867081906080637\n",
      "train loss:0.0011095612735667537\n",
      "train loss:0.001131136190529753\n",
      "train loss:0.0017706638441717473\n",
      "train loss:0.000600462241949648\n",
      "train loss:0.0007415248271269344\n",
      "train loss:0.0006028442663583439\n",
      "train loss:0.007619155815551114\n",
      "train loss:0.005219621325454865\n",
      "train loss:0.00877644965820009\n",
      "train loss:0.0007132023359776611\n",
      "train loss:0.0008311598004359488\n",
      "train loss:0.003996715668428894\n",
      "train loss:0.0008764660342678236\n",
      "train loss:0.0016444293965195134\n",
      "train loss:0.003239468656515549\n",
      "train loss:0.0002224697502635143\n",
      "train loss:0.0008083803027514962\n",
      "train loss:0.0016493728613182516\n",
      "train loss:0.0005702478756941798\n",
      "train loss:0.00020325872219283805\n",
      "train loss:0.006696505977318599\n",
      "train loss:0.0017987237509410085\n",
      "train loss:0.001100837166256662\n",
      "train loss:0.0006081459404740992\n",
      "train loss:0.0012408106952739287\n",
      "train loss:0.002404772736100264\n",
      "train loss:0.0026964475948776066\n",
      "train loss:0.006166428379369719\n",
      "train loss:0.0025027003170685126\n",
      "train loss:0.0018218850244158132\n",
      "train loss:0.00189132570383604\n",
      "train loss:0.0011404903177108482\n",
      "train loss:0.002056727964962208\n",
      "train loss:0.0051272016634292235\n",
      "train loss:0.002915954806564921\n",
      "train loss:0.0024597635783476825\n",
      "train loss:0.004147579678116496\n",
      "train loss:0.005205465820652778\n",
      "train loss:0.000888494150789945\n",
      "train loss:0.00036107870189669054\n",
      "train loss:0.00022149938339075838\n",
      "train loss:0.0009395734781883805\n",
      "train loss:0.0004593213191774001\n",
      "train loss:0.002469296941726633\n",
      "train loss:0.026595496910160513\n",
      "train loss:0.005291923586907681\n",
      "train loss:0.00041658478461923136\n",
      "train loss:0.007150396680104534\n",
      "train loss:0.0012927819559345251\n",
      "train loss:0.0006474111576750481\n",
      "train loss:0.008919666536674729\n",
      "train loss:0.0008093216977565379\n",
      "train loss:0.0018899337705318919\n",
      "train loss:0.0018548181013449507\n",
      "train loss:0.008001730373517476\n",
      "train loss:0.0018841879964642506\n",
      "train loss:0.0022173892000610855\n",
      "train loss:0.0013639871641722265\n",
      "train loss:0.003864114474092664\n",
      "train loss:0.0013659684750993344\n",
      "train loss:0.0071285200879371464\n",
      "train loss:0.012466853322064504\n",
      "train loss:0.00319253784061247\n",
      "train loss:0.009494279355288819\n",
      "train loss:0.0013125840095172523\n",
      "train loss:0.0035470982867817375\n",
      "train loss:0.00012740802604291083\n",
      "train loss:0.0005483036890769157\n",
      "train loss:0.0003838913203367644\n",
      "train loss:0.0004934350949143195\n",
      "train loss:0.0020173377857124627\n",
      "train loss:0.003735159709517766\n",
      "train loss:0.0005263609618196805\n",
      "train loss:0.000624782718690227\n",
      "train loss:0.000862521850159023\n",
      "train loss:0.004485685778486497\n",
      "train loss:0.0032246307118760386\n",
      "train loss:7.324747931868834e-05\n",
      "train loss:0.003029428174865523\n",
      "train loss:0.003730057996051397\n",
      "train loss:0.0009572337371872284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0033845185315034966\n",
      "train loss:0.0004997059465359601\n",
      "train loss:0.0017232457683836228\n",
      "train loss:0.006874913782387609\n",
      "train loss:5.865193104922321e-05\n",
      "train loss:0.00039829478594477643\n",
      "train loss:0.001887190334366493\n",
      "train loss:0.0012191199517114291\n",
      "train loss:0.0015100638461050726\n",
      "train loss:0.0009677545047072485\n",
      "train loss:0.0003335289738375808\n",
      "train loss:0.0008176943391468844\n",
      "train loss:0.0009596260336234861\n",
      "train loss:0.0002201942728287188\n",
      "train loss:0.004730754244414337\n",
      "train loss:0.004592601669316319\n",
      "train loss:0.0036959172219804375\n",
      "train loss:0.0013704987202724694\n",
      "train loss:0.0004191988980782101\n",
      "train loss:0.00024650353569983364\n",
      "train loss:0.0014880499385355097\n",
      "train loss:0.003060861077976831\n",
      "train loss:0.0001360782776377964\n",
      "train loss:0.0006553871714376091\n",
      "train loss:0.0026635628636583917\n",
      "train loss:0.00032854263174996893\n",
      "train loss:0.000984999318172627\n",
      "train loss:0.0008501892555515173\n",
      "train loss:0.0011842709482092406\n",
      "train loss:0.0017726213226059609\n",
      "train loss:0.00041463362038558494\n",
      "train loss:0.03143970216801399\n",
      "train loss:0.0004029737530316126\n",
      "train loss:0.0064707995618204315\n",
      "train loss:0.003908984444677402\n",
      "train loss:0.0012002029074213063\n",
      "train loss:0.001850671089688364\n",
      "train loss:0.004247384260736497\n",
      "train loss:0.013307150975958897\n",
      "train loss:0.0013896187347794836\n",
      "train loss:0.011678858928000689\n",
      "train loss:0.0007718955918942858\n",
      "train loss:0.006337833003763923\n",
      "train loss:0.0032297928096994576\n",
      "train loss:0.006011198237199842\n",
      "train loss:0.002339538141375157\n",
      "train loss:0.0008602366451321129\n",
      "train loss:0.0029706148909493823\n",
      "train loss:0.011753303507317363\n",
      "train loss:0.0019551814458831016\n",
      "train loss:0.004814923489461949\n",
      "train loss:0.00033347764658549705\n",
      "train loss:0.0058045127080188245\n",
      "train loss:0.0006291482689689061\n",
      "train loss:0.0002772407390556974\n",
      "train loss:0.01930325306418331\n",
      "train loss:0.006796074566709788\n",
      "train loss:0.001492315866237927\n",
      "train loss:0.003637330050855246\n",
      "train loss:0.0011641140222371344\n",
      "train loss:0.0024501864943046823\n",
      "train loss:0.008790693367387421\n",
      "train loss:0.00376327354315625\n",
      "train loss:0.00038623972288631605\n",
      "train loss:0.014464253651423977\n",
      "train loss:0.0006483756364914862\n",
      "train loss:0.0013512456020653184\n",
      "train loss:0.011821093364593485\n",
      "train loss:0.00013109341152413726\n",
      "train loss:0.004269265803126244\n",
      "train loss:0.022014249662469947\n",
      "train loss:0.006009343563384868\n",
      "train loss:0.02832551307745715\n",
      "train loss:0.0017617709783457663\n",
      "train loss:0.016562167136205598\n",
      "train loss:0.002424385845161855\n",
      "train loss:0.0010655750743578634\n",
      "train loss:0.024008841975760708\n",
      "train loss:0.00626129079380473\n",
      "train loss:0.0011962606142521474\n",
      "train loss:0.0019227589839065976\n",
      "train loss:0.0002755736902719713\n",
      "train loss:0.0014183116203611856\n",
      "train loss:0.0011820658821743715\n",
      "train loss:0.0023531557327119037\n",
      "train loss:0.00029235067732147617\n",
      "train loss:0.00750235367330761\n",
      "train loss:0.0013414270591528645\n",
      "train loss:0.0031920444777480063\n",
      "train loss:0.0016960931504579357\n",
      "train loss:0.002229099922744762\n",
      "train loss:0.007517518384708368\n",
      "train loss:0.03312368348871222\n",
      "train loss:0.0005873339926506525\n",
      "train loss:0.0012079672562678987\n",
      "train loss:0.00944441144190792\n",
      "train loss:0.0008090775559953672\n",
      "train loss:0.002743265371707881\n",
      "train loss:0.000638201364624768\n",
      "train loss:0.010398093186410268\n",
      "train loss:0.000372105885521212\n",
      "train loss:0.0005339289760901571\n",
      "train loss:0.003277818314737585\n",
      "train loss:0.0010738778958693251\n",
      "train loss:0.001857904569192436\n",
      "train loss:0.0006867699376918749\n",
      "train loss:0.0058084970938740476\n",
      "train loss:0.0027917458544455378\n",
      "train loss:0.00021646989713853857\n",
      "train loss:0.00085807634373799\n",
      "train loss:0.0042361360642261415\n",
      "train loss:0.004320794912019089\n",
      "train loss:0.0037910688977063913\n",
      "train loss:0.0043733781578451\n",
      "train loss:0.00016805622005673697\n",
      "train loss:0.024523414098474566\n",
      "train loss:0.006638811146167982\n",
      "train loss:0.0008029859912404747\n",
      "train loss:0.0020301050316423085\n",
      "train loss:0.0013386427710841916\n",
      "train loss:0.0005657955652084993\n",
      "train loss:0.0016312512410449234\n",
      "train loss:0.0004396294059613752\n",
      "train loss:0.0046514654703508716\n",
      "train loss:0.0003619768219184576\n",
      "train loss:0.0010378355670314899\n",
      "train loss:0.08609846905124866\n",
      "train loss:0.01880228240199664\n",
      "train loss:0.0002846365801053498\n",
      "train loss:0.0029151029550347003\n",
      "train loss:0.005831808675601201\n",
      "train loss:0.0015262808687229704\n",
      "train loss:0.019183048730818063\n",
      "train loss:0.0011810810474523576\n",
      "train loss:0.002032601985223799\n",
      "train loss:0.0022047235132355656\n",
      "train loss:0.00016954113922625923\n",
      "train loss:0.0067042959952834155\n",
      "train loss:0.0008404193572479125\n",
      "train loss:0.0017146325055975192\n",
      "train loss:0.004920022355587871\n",
      "train loss:0.0024107823728303163\n",
      "train loss:0.0031902514822386304\n",
      "train loss:0.0005091415372870712\n",
      "train loss:0.0007911127426095768\n",
      "train loss:0.0015983491350975707\n",
      "train loss:0.004924065772189684\n",
      "train loss:0.007530120587855742\n",
      "train loss:0.0005632224954809072\n",
      "train loss:0.0004008992659979195\n",
      "train loss:0.008540071625372913\n",
      "train loss:0.0019195354998602276\n",
      "train loss:0.010366775544580502\n",
      "train loss:0.001615484781174118\n",
      "train loss:0.0034893448989967656\n",
      "train loss:0.001578827395670339\n",
      "train loss:0.0012507906585113096\n",
      "train loss:0.0010991622258102519\n",
      "train loss:0.0019205681438078563\n",
      "train loss:0.0017003776718160044\n",
      "train loss:0.09484291766704996\n",
      "train loss:0.009646976671683542\n",
      "train loss:0.0011778608609066437\n",
      "train loss:7.190218262888968e-05\n",
      "train loss:0.0003533847724302299\n",
      "train loss:0.0008779137743561918\n",
      "train loss:0.0012224530630363447\n",
      "train loss:0.0004385823150738594\n",
      "train loss:0.0023175503157944885\n",
      "train loss:0.0009743223764550167\n",
      "train loss:0.009042215616697406\n",
      "train loss:0.0021116183418483287\n",
      "train loss:0.0013279721541994821\n",
      "train loss:0.0017156621106654496\n",
      "train loss:0.002321832173773959\n",
      "train loss:0.003490259556708362\n",
      "train loss:0.0010155243236083702\n",
      "train loss:0.004271480518758445\n",
      "train loss:0.0005506799762851745\n",
      "train loss:0.017272209198699363\n",
      "train loss:0.0011162763560722247\n",
      "train loss:0.001510833130991057\n",
      "train loss:0.0009111977327999006\n",
      "train loss:0.0010339321816238186\n",
      "train loss:0.008477538173058635\n",
      "train loss:0.06065250648571251\n",
      "train loss:0.0036285602721348744\n",
      "train loss:0.0006972886444652386\n",
      "train loss:0.0023379068846959227\n",
      "train loss:0.002734999068661652\n",
      "train loss:0.0033096951367734637\n",
      "train loss:0.005104561905267515\n",
      "train loss:0.0008063805469962151\n",
      "train loss:5.775761262948723e-05\n",
      "train loss:0.0025778514140466554\n",
      "train loss:0.0015656064761997751\n",
      "train loss:0.0003299265810016494\n",
      "train loss:0.006020494210372131\n",
      "train loss:0.0006302361323688568\n",
      "train loss:0.002492346929391872\n",
      "train loss:0.0002660972473155984\n",
      "train loss:0.0026939586178666257\n",
      "train loss:0.0010348604516791774\n",
      "train loss:0.002352314594801202\n",
      "train loss:0.0031892866514348535\n",
      "train loss:9.417842505396325e-05\n",
      "train loss:0.0030440512269668575\n",
      "train loss:0.0032711602158725993\n",
      "train loss:0.0022980272240480997\n",
      "train loss:0.0018564595147611419\n",
      "train loss:0.001410514952801623\n",
      "train loss:0.0051048760486399536\n",
      "train loss:0.0016700469258368448\n",
      "train loss:0.0021195412356171737\n",
      "train loss:0.0006839960172977825\n",
      "train loss:0.00214431412015724\n",
      "train loss:0.0010075475839648109\n",
      "train loss:0.0017050008004127351\n",
      "train loss:0.00026495968163362367\n",
      "train loss:0.0016126103725382826\n",
      "train loss:0.01035892035372762\n",
      "train loss:4.909864677872875e-05\n",
      "train loss:0.003130574851190031\n",
      "train loss:0.0006766578549502713\n",
      "train loss:0.015639360440076175\n",
      "train loss:0.000914570319209205\n",
      "train loss:0.0009042449388376761\n",
      "train loss:0.0012583220766022663\n",
      "train loss:0.018946367560636973\n",
      "train loss:0.0015238617272229171\n",
      "train loss:0.0017531068180275415\n",
      "train loss:0.0005028945903471911\n",
      "train loss:0.00046915529149810594\n",
      "train loss:0.002478889703115002\n",
      "train loss:0.005436028621278942\n",
      "train loss:0.010814028206598703\n",
      "train loss:0.0062575894862010005\n",
      "train loss:0.000778671696525754\n",
      "train loss:0.0004502195623816789\n",
      "train loss:0.0024487091129716627\n",
      "train loss:0.0008185390216550809\n",
      "train loss:0.0014682949719713092\n",
      "train loss:0.002187518595017411\n",
      "train loss:0.003765631562807603\n",
      "=== epoch:13, train acc:0.998, test acc:0.985 ===\n",
      "train loss:0.0030014517953970616\n",
      "train loss:0.008705397389696806\n",
      "train loss:0.0009132326636381167\n",
      "train loss:0.0003182521017217233\n",
      "train loss:0.0038213063961316696\n",
      "train loss:8.78177565152597e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0009593316874818618\n",
      "train loss:0.0011437682375439232\n",
      "train loss:0.001564473193780453\n",
      "train loss:0.0005747601544520716\n",
      "train loss:0.01678620812289485\n",
      "train loss:0.00020019093573643638\n",
      "train loss:0.0036518805671161255\n",
      "train loss:0.003197535405835791\n",
      "train loss:0.0007707933329023701\n",
      "train loss:0.0014177200699002186\n",
      "train loss:0.000958111779391321\n",
      "train loss:0.0017469046532335014\n",
      "train loss:0.0012499888611144409\n",
      "train loss:0.003774481008700949\n",
      "train loss:0.0029836419436985112\n",
      "train loss:0.00018438226669658167\n",
      "train loss:0.001740348480783965\n",
      "train loss:0.004381703783237792\n",
      "train loss:0.0034641303579038274\n",
      "train loss:0.0012175696715929125\n",
      "train loss:0.0021480496778393905\n",
      "train loss:0.001489273042868038\n",
      "train loss:0.00034107367892175736\n",
      "train loss:0.000741746879566447\n",
      "train loss:0.002816401704349533\n",
      "train loss:0.0037415564323885074\n",
      "train loss:0.0012199226922359342\n",
      "train loss:0.00744584908361114\n",
      "train loss:0.0018675979076418694\n",
      "train loss:0.003625936541282913\n",
      "train loss:0.004208750006687897\n",
      "train loss:0.0004502793895539507\n",
      "train loss:0.0002590692335354704\n",
      "train loss:0.00041333906500604567\n",
      "train loss:0.0016607583320996002\n",
      "train loss:0.014981265958680325\n",
      "train loss:0.000263677710955854\n",
      "train loss:0.0002759271544896518\n",
      "train loss:0.004976230351709256\n",
      "train loss:0.0021679332431304133\n",
      "train loss:0.0007562202931455562\n",
      "train loss:0.003622019196558799\n",
      "train loss:0.001866653494572519\n",
      "train loss:0.0005120016364591477\n",
      "train loss:0.003755368721576699\n",
      "train loss:0.0008336278088822528\n",
      "train loss:0.003729493121694793\n",
      "train loss:0.001909760503152434\n",
      "train loss:0.0024507627703499605\n",
      "train loss:0.0019139121460100285\n",
      "train loss:0.0034761004010849922\n",
      "train loss:0.002845426409182369\n",
      "train loss:0.0014908076102088416\n",
      "train loss:0.0008886057477155586\n",
      "train loss:0.0025519026841938804\n",
      "train loss:0.00028998505435886245\n",
      "train loss:0.0002557617265176578\n",
      "train loss:0.009698858439172136\n",
      "train loss:0.00022877945995288527\n",
      "train loss:0.0014152581586496344\n",
      "train loss:0.004355751238960436\n",
      "train loss:0.000913746680980463\n",
      "train loss:0.0006171717940153095\n",
      "train loss:0.002226612474344627\n",
      "train loss:4.131899898308898e-05\n",
      "train loss:0.0011535629077103246\n",
      "train loss:0.0003540771612218572\n",
      "train loss:0.0004891593681628735\n",
      "train loss:0.0008156161976133141\n",
      "train loss:0.0016208018544012037\n",
      "train loss:0.005624756800083273\n",
      "train loss:0.002417533307030544\n",
      "train loss:0.0004828240206885214\n",
      "train loss:0.001014390946693893\n",
      "train loss:0.000695294262591744\n",
      "train loss:0.0010209925798201627\n",
      "train loss:0.014436492975584058\n",
      "train loss:0.0003410925879326765\n",
      "train loss:0.03510209263691524\n",
      "train loss:0.002225766553269419\n",
      "train loss:0.0037183638413927477\n",
      "train loss:0.006232327895053253\n",
      "train loss:0.002694129339975863\n",
      "train loss:0.04675742676498627\n",
      "train loss:0.0004354940978761422\n",
      "train loss:0.0019120876051839167\n",
      "train loss:0.0019200699313955865\n",
      "train loss:0.0018484319252689582\n",
      "train loss:0.0006140530010641896\n",
      "train loss:0.0007525854259244261\n",
      "train loss:0.007504779219977807\n",
      "train loss:0.0044998244620598725\n",
      "train loss:0.015798359510045528\n",
      "train loss:0.0060075173790298895\n",
      "train loss:0.00031416723080710543\n",
      "train loss:0.0005477624110962619\n",
      "train loss:0.003914255386440866\n",
      "train loss:0.014791370815171036\n",
      "train loss:0.0005019544201482711\n",
      "train loss:0.03283592714316545\n",
      "train loss:0.001175843714888853\n",
      "train loss:0.00014875837861724357\n",
      "train loss:0.0036330422811374856\n",
      "train loss:0.0022778686177131365\n",
      "train loss:0.01922870080137367\n",
      "train loss:0.004972583159721164\n",
      "train loss:0.0005655108466891492\n",
      "train loss:0.0012184244354939426\n",
      "train loss:0.006492901769030389\n",
      "train loss:0.0014504637270133728\n",
      "train loss:0.0006251810176864798\n",
      "train loss:0.0003696665105693422\n",
      "train loss:0.0007851818934665306\n",
      "train loss:0.0010025074914132403\n",
      "train loss:0.0008002966815959338\n",
      "train loss:0.011362408912811262\n",
      "train loss:0.0004616146487242691\n",
      "train loss:0.054212616097725395\n",
      "train loss:0.0012969361052728753\n",
      "train loss:0.0011086993561444415\n",
      "train loss:0.0012202984147290828\n",
      "train loss:0.0024303382653526034\n",
      "train loss:0.002585660211690432\n",
      "train loss:0.0003885442795168076\n",
      "train loss:0.0046153996223664805\n",
      "train loss:0.0024974773818083303\n",
      "train loss:0.0020026229172355058\n",
      "train loss:0.0007508275166707727\n",
      "train loss:0.010883217972339923\n",
      "train loss:0.0019684051959502243\n",
      "train loss:0.005516606554005851\n",
      "train loss:0.000858227902806353\n",
      "train loss:0.002441580612318963\n",
      "train loss:0.001987894429468734\n",
      "train loss:0.005573970943873356\n",
      "train loss:0.003572657202245919\n",
      "train loss:0.00044840540241099285\n",
      "train loss:0.005544071210849082\n",
      "train loss:0.0025690315571634415\n",
      "train loss:0.006154672626794157\n",
      "train loss:0.0005290118615627428\n",
      "train loss:0.0009427079196199252\n",
      "train loss:0.004892568978894119\n",
      "train loss:0.0025666089817432975\n",
      "train loss:0.00020165111895619083\n",
      "train loss:0.0014281439404948333\n",
      "train loss:0.0003036469638369373\n",
      "train loss:0.0009747263733319886\n",
      "train loss:0.003431995582317576\n",
      "train loss:0.01988818975191002\n",
      "train loss:0.0023286960010777016\n",
      "train loss:0.0004312642944908671\n",
      "train loss:0.0022128122136704413\n",
      "train loss:0.005548233193488767\n",
      "train loss:0.0003511771313366934\n",
      "train loss:0.0042264615837896416\n",
      "train loss:0.002531471958190507\n",
      "train loss:0.003984012208642761\n",
      "train loss:0.0015513637487758027\n",
      "train loss:0.0013259408931324138\n",
      "train loss:0.00014529677649085564\n",
      "train loss:0.0008348591281569249\n",
      "train loss:0.0018283152414425702\n",
      "train loss:0.0022594462537410672\n",
      "train loss:0.012938088545137859\n",
      "train loss:0.0021228580572662963\n",
      "train loss:0.0013626224167359762\n",
      "train loss:0.00705608241524272\n",
      "train loss:0.0023470662109281283\n",
      "train loss:0.0004691290759422249\n",
      "train loss:0.009249140049237764\n",
      "train loss:0.0014551702749697496\n",
      "train loss:0.0032682463505573924\n",
      "train loss:0.001573282339206509\n",
      "train loss:0.0032396667968562936\n",
      "train loss:0.01222983927683\n",
      "train loss:0.0013938567554859932\n",
      "train loss:0.00047500197942687577\n",
      "train loss:0.002555442448018764\n",
      "train loss:0.00040208997798011156\n",
      "train loss:0.002263824798764407\n",
      "train loss:0.0012155552668041423\n",
      "train loss:0.0002660215700737605\n",
      "train loss:0.0028322516194767343\n",
      "train loss:0.0002233760781340991\n",
      "train loss:0.001861807240487853\n",
      "train loss:0.0030996819612195436\n",
      "train loss:0.0001231523071101078\n",
      "train loss:0.0001481783992384707\n",
      "train loss:0.004840994814291748\n",
      "train loss:0.004235985843092159\n",
      "train loss:0.0008475189943190025\n",
      "train loss:0.0028532595171269466\n",
      "train loss:0.0018145275763358653\n",
      "train loss:0.008574949697219454\n",
      "train loss:0.005876427783265768\n",
      "train loss:0.011015613040695395\n",
      "train loss:0.0009921969563843908\n",
      "train loss:0.0123154904202755\n",
      "train loss:0.01495444082748646\n",
      "train loss:0.0008187348979875414\n",
      "train loss:0.0006855744310290772\n",
      "train loss:0.0011296682259118419\n",
      "train loss:0.0007589834129002027\n",
      "train loss:0.00010433683067212409\n",
      "train loss:0.0031994582671922506\n",
      "train loss:0.00014159571678078073\n",
      "train loss:0.0003731142711639529\n",
      "train loss:0.000695764583869241\n",
      "train loss:0.0012596975664824922\n",
      "train loss:0.0020187065893621644\n",
      "train loss:0.00942355136424475\n",
      "train loss:0.0003084640299218391\n",
      "train loss:0.00026462997740832816\n",
      "train loss:0.0002443229105457001\n",
      "train loss:0.0016375430881226136\n",
      "train loss:0.0019469067099579817\n",
      "train loss:0.0016340630990266153\n",
      "train loss:0.006447079829917067\n",
      "train loss:0.0006821246109689445\n",
      "train loss:0.00574116822004976\n",
      "train loss:0.0017665606276476564\n",
      "train loss:0.002669069090827512\n",
      "train loss:0.007130732848696404\n",
      "train loss:0.0002527939255504572\n",
      "train loss:0.00454039072855749\n",
      "train loss:0.0007571337127794422\n",
      "train loss:0.0011916221778396673\n",
      "train loss:0.0009210589618885701\n",
      "train loss:0.0002529730345446164\n",
      "train loss:0.00024997291716468903\n",
      "train loss:0.0009919876366708965\n",
      "train loss:0.0015169633571513361\n",
      "train loss:0.006825314967423234\n",
      "train loss:0.0006002166701727925\n",
      "train loss:0.0004609076553318886\n",
      "train loss:0.0016943131640721404\n",
      "train loss:0.007628956513731828\n",
      "train loss:0.00027358891233500833\n",
      "train loss:0.0013723167453781851\n",
      "train loss:0.0005545555835280918\n",
      "train loss:0.00038812024531207923\n",
      "train loss:0.002860971877373743\n",
      "train loss:0.004917283922858341\n",
      "train loss:0.0037686115986260366\n",
      "train loss:0.004191083773899596\n",
      "train loss:0.00042782437449322245\n",
      "train loss:0.0010799888113138664\n",
      "train loss:0.0033544609747654342\n",
      "train loss:0.0019936699150057673\n",
      "train loss:0.0007046603707713845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.001613768111896536\n",
      "train loss:0.0011310822681354104\n",
      "train loss:0.006831250222963852\n",
      "train loss:0.003726903254268697\n",
      "train loss:0.003082805357172846\n",
      "train loss:0.0036879410295276153\n",
      "train loss:0.006263727792139772\n",
      "train loss:0.0010545268513564461\n",
      "train loss:0.00020299484525611474\n",
      "train loss:0.0032497638230387406\n",
      "train loss:0.0027080422872065276\n",
      "train loss:0.0023754673406361617\n",
      "train loss:0.0018003621101379303\n",
      "train loss:0.003641719003047969\n",
      "train loss:0.004399273169945951\n",
      "train loss:0.0006036881464777606\n",
      "train loss:0.0016472638457711406\n",
      "train loss:0.006058081099333964\n",
      "train loss:0.002480071821735918\n",
      "train loss:0.013456674161744057\n",
      "train loss:0.008819666924125325\n",
      "train loss:0.011631762271224495\n",
      "train loss:0.0007495422277388797\n",
      "train loss:0.034721931849709474\n",
      "train loss:0.0008691231403827847\n",
      "train loss:0.0007263065116863724\n",
      "train loss:0.0006616187351850317\n",
      "train loss:0.0025998987639711015\n",
      "train loss:0.005570346455132421\n",
      "train loss:0.002700717716863052\n",
      "train loss:0.024089546634785096\n",
      "train loss:0.0025350260250656194\n",
      "train loss:0.0379384847467411\n",
      "train loss:0.00016772415294079795\n",
      "train loss:0.0018511404228549384\n",
      "train loss:0.00046527866393896626\n",
      "train loss:0.0035748297899450806\n",
      "train loss:0.0014268169549110773\n",
      "train loss:0.000999997922920534\n",
      "train loss:0.006117422436145608\n",
      "train loss:9.46272278015831e-05\n",
      "train loss:0.0016162303402529766\n",
      "train loss:0.0018361173832212958\n",
      "train loss:0.0003493446312454919\n",
      "train loss:0.0049121748258136155\n",
      "train loss:0.0005368940949483619\n",
      "train loss:0.008340602970242266\n",
      "train loss:0.002413585540920758\n",
      "train loss:0.0005432549091456253\n",
      "train loss:0.0005974418996133674\n",
      "train loss:0.003728089414130244\n",
      "train loss:0.001487084000692062\n",
      "train loss:0.00803782905350618\n",
      "train loss:0.002080621693529585\n",
      "train loss:0.006507606996080672\n",
      "train loss:8.794802119886747e-05\n",
      "train loss:0.0017694648740412208\n",
      "train loss:0.0007503227968765056\n",
      "train loss:0.001347684955334931\n",
      "train loss:0.0017837317819415402\n",
      "train loss:0.0007035886977994175\n",
      "train loss:0.0019321398777072615\n",
      "train loss:0.00018208251766694674\n",
      "train loss:0.0006619672698966744\n",
      "train loss:0.0013675886861334212\n",
      "train loss:0.024300907373313314\n",
      "train loss:0.0005255802501466231\n",
      "train loss:0.0012866558313290727\n",
      "train loss:0.0032392531669010066\n",
      "train loss:0.0005694422430838936\n",
      "train loss:3.7459084212530336e-05\n",
      "train loss:0.0013369469984604884\n",
      "train loss:0.003173673908641597\n",
      "train loss:0.0022214903017990922\n",
      "train loss:0.002690872678499406\n",
      "train loss:0.00021098021782401476\n",
      "train loss:0.0024443456634895746\n",
      "train loss:0.0006202059673123532\n",
      "train loss:0.0002950221959266426\n",
      "train loss:0.005980320490944894\n",
      "train loss:0.00036350328021463827\n",
      "train loss:0.0003435631872383073\n",
      "train loss:0.00017547964497029788\n",
      "train loss:0.002951107972781049\n",
      "train loss:0.0004005115355383178\n",
      "train loss:0.000299442907648276\n",
      "train loss:0.0001149169462697332\n",
      "train loss:0.0009910538915479513\n",
      "train loss:0.004138264684571556\n",
      "train loss:0.002290501597917784\n",
      "train loss:0.001393358065131669\n",
      "train loss:0.000593976822780972\n",
      "train loss:0.0005331137902432961\n",
      "train loss:0.0008396811684594673\n",
      "train loss:0.002154043760186739\n",
      "train loss:0.00014510255472269303\n",
      "train loss:0.0025831629631833004\n",
      "train loss:0.0011873310249495871\n",
      "train loss:0.00269748836580871\n",
      "train loss:0.0012342843390937045\n",
      "train loss:0.0006746547034375223\n",
      "train loss:0.0009457116935075068\n",
      "train loss:0.00034020648925160084\n",
      "train loss:0.0009507148667890289\n",
      "train loss:0.008934344667819664\n",
      "train loss:0.0019830631127424696\n",
      "train loss:0.0034206755155279972\n",
      "train loss:0.0017016981245390969\n",
      "train loss:0.02765497196668567\n",
      "train loss:0.0006013576821803423\n",
      "train loss:0.0024244434836411402\n",
      "train loss:0.00048237507853101763\n",
      "train loss:0.0012224666095382542\n",
      "train loss:0.0026513004824548266\n",
      "train loss:0.0015946202881995583\n",
      "train loss:0.004933291445768318\n",
      "train loss:0.004812891260018857\n",
      "train loss:0.0028659752190739697\n",
      "train loss:0.007831918836048161\n",
      "train loss:0.001106438799559721\n",
      "train loss:0.0019571978806394792\n",
      "train loss:0.0009063728034366811\n",
      "train loss:4.63917512562956e-05\n",
      "train loss:0.00020585770004117598\n",
      "train loss:0.003787398940313507\n",
      "train loss:0.009500582409267521\n",
      "train loss:0.00025654610788817574\n",
      "train loss:0.0026473299798531746\n",
      "train loss:0.0010073551608519244\n",
      "train loss:0.0006577290344916345\n",
      "train loss:0.008220336673345579\n",
      "train loss:0.0020897145443274905\n",
      "train loss:0.002728072224980067\n",
      "train loss:0.00034169045732658666\n",
      "train loss:0.0007016713728294706\n",
      "train loss:0.0010858039183835278\n",
      "train loss:0.0005368362643233825\n",
      "train loss:0.000944457780109784\n",
      "train loss:0.0044117305231846414\n",
      "train loss:0.00030435960537530433\n",
      "train loss:0.01626755410411898\n",
      "train loss:0.002957527618060329\n",
      "train loss:0.0012756243676887587\n",
      "train loss:0.00197629888427102\n",
      "train loss:0.0021617152213797173\n",
      "train loss:0.0017958129309231183\n",
      "train loss:0.0017530425614239525\n",
      "train loss:0.0037083467140632963\n",
      "train loss:0.0001563030406954034\n",
      "train loss:0.0016922580721351896\n",
      "train loss:0.0007351651394084673\n",
      "train loss:0.0003553901031289236\n",
      "train loss:0.0009673258997949007\n",
      "train loss:0.00018655332215851227\n",
      "train loss:0.002685241494424201\n",
      "train loss:0.006163713438240506\n",
      "train loss:0.001162971035451811\n",
      "train loss:0.0020810216498214655\n",
      "train loss:0.0007822888103063553\n",
      "train loss:0.0013738724838667968\n",
      "train loss:0.0010166928636585499\n",
      "train loss:0.0009902846386926215\n",
      "train loss:0.0005246036531867898\n",
      "train loss:0.016551603392403687\n",
      "train loss:0.0001246179893999501\n",
      "train loss:0.0003768692756364081\n",
      "train loss:0.0018233706751833313\n",
      "train loss:0.0020544066241587338\n",
      "train loss:0.002325819552935344\n",
      "train loss:0.0026046052465297097\n",
      "train loss:0.0009442454368550474\n",
      "train loss:0.00839989498990615\n",
      "train loss:0.00043739696994105434\n",
      "train loss:0.005354127424036646\n",
      "train loss:0.0012562926173208463\n",
      "train loss:0.0012887854774086863\n",
      "train loss:0.0011480836123919798\n",
      "train loss:6.399205875622412e-05\n",
      "train loss:0.0015592370971133193\n",
      "train loss:0.00048683592611315483\n",
      "train loss:0.005894514318864464\n",
      "train loss:0.0003881642023037216\n",
      "train loss:0.0012001223283172788\n",
      "train loss:0.008937607558910937\n",
      "train loss:0.0024117271932119215\n",
      "train loss:0.0017864379458543648\n",
      "train loss:0.00013392279516967262\n",
      "train loss:0.0001330371544928817\n",
      "train loss:0.012778872788624197\n",
      "train loss:0.0015589427303696853\n",
      "train loss:0.0023391059209037506\n",
      "train loss:0.0006096191620301069\n",
      "train loss:0.0010928445460087095\n",
      "train loss:0.00313008002760598\n",
      "train loss:0.004775495128762284\n",
      "train loss:0.0012196996133732574\n",
      "train loss:0.0030413780607052286\n",
      "train loss:0.005047711671892306\n",
      "train loss:0.0007703551978518573\n",
      "train loss:0.00034936014164672116\n",
      "train loss:0.0003193251094788533\n",
      "train loss:0.0018383667847396937\n",
      "train loss:0.0006523534943041291\n",
      "train loss:0.00030507680011988803\n",
      "train loss:0.0008093920271107939\n",
      "train loss:0.0036534412934667733\n",
      "train loss:0.0010113620619221608\n",
      "train loss:0.003093967344769502\n",
      "train loss:0.0023255450735335964\n",
      "train loss:0.00019331670263746947\n",
      "train loss:0.0005968135605201477\n",
      "train loss:0.0009292546733445151\n",
      "train loss:0.002059810537015508\n",
      "train loss:0.00021784680653787636\n",
      "train loss:0.002333506786023822\n",
      "train loss:0.0011844518557291228\n",
      "train loss:0.00020117902587372824\n",
      "train loss:0.010859745107725385\n",
      "train loss:0.0001470079782388588\n",
      "train loss:4.8301096886521935e-05\n",
      "train loss:0.004486250589139007\n",
      "train loss:0.014409658670154504\n",
      "train loss:0.0030477129766241125\n",
      "train loss:0.0032859059568742786\n",
      "train loss:0.0005611621731684201\n",
      "train loss:0.002027283334520581\n",
      "train loss:0.000870869218658484\n",
      "train loss:0.00030621082285456304\n",
      "train loss:0.0016651200848171394\n",
      "train loss:0.0004081606647209302\n",
      "train loss:0.004203360043940357\n",
      "train loss:0.0007993726726277606\n",
      "train loss:0.0001846944906716535\n",
      "train loss:0.0023214959817001737\n",
      "train loss:0.0009647658328905239\n",
      "train loss:0.00011711703673234575\n",
      "train loss:0.23625401719634445\n",
      "train loss:0.0016397499046033651\n",
      "train loss:0.00022285139533237942\n",
      "train loss:0.0024964755611866446\n",
      "train loss:0.002056159531806903\n",
      "train loss:0.00022285863949166474\n",
      "train loss:6.981566743995477e-05\n",
      "train loss:0.006448320388588268\n",
      "train loss:0.00023503950604346357\n",
      "train loss:0.0006209915618447777\n",
      "train loss:0.001174576174790998\n",
      "train loss:0.0017794834679147166\n",
      "train loss:0.0005891821834093719\n",
      "train loss:0.0020566253329001053\n",
      "train loss:0.006623566569731982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.003881057472326633\n",
      "train loss:0.001775429693947463\n",
      "train loss:0.0009997447622758927\n",
      "train loss:0.0001507462552530168\n",
      "train loss:0.0011031720599168247\n",
      "train loss:0.0005603380444248792\n",
      "train loss:0.0012549381451069748\n",
      "train loss:0.000958013367184305\n",
      "train loss:0.00021879914244784206\n",
      "train loss:0.00048583789117056\n",
      "train loss:0.0013375444563711422\n",
      "train loss:0.00026302722910314904\n",
      "train loss:0.00272458678673048\n",
      "train loss:0.00273111240904687\n",
      "train loss:0.004162026666338014\n",
      "train loss:0.004652071737662317\n",
      "train loss:0.001602341752808768\n",
      "train loss:0.0022589532515893363\n",
      "train loss:0.0005985986216548241\n",
      "train loss:0.0005277802918741942\n",
      "train loss:0.00040770537586841074\n",
      "train loss:0.0038966765620234893\n",
      "train loss:0.0004189991338611022\n",
      "train loss:0.001130347372460778\n",
      "train loss:0.0007309481027573063\n",
      "train loss:0.0001619152513774478\n",
      "train loss:0.0030115050410061527\n",
      "train loss:0.00222966324639946\n",
      "train loss:0.00037079504805205527\n",
      "train loss:0.00041676232074710226\n",
      "train loss:0.0006212475125761097\n",
      "train loss:0.0005998967780466286\n",
      "train loss:0.003678786146222587\n",
      "train loss:0.0009790495742575448\n",
      "train loss:0.0007268264985387363\n",
      "train loss:0.0024792863189796632\n",
      "train loss:0.0005564250620731838\n",
      "train loss:0.0001237726257257607\n",
      "train loss:0.010043715230916779\n",
      "train loss:0.0003628177218710593\n",
      "train loss:0.0005876312879478003\n",
      "train loss:0.0014630000070871502\n",
      "train loss:0.0014151363638374376\n",
      "train loss:6.966345008400579e-05\n",
      "train loss:0.0015990431223242383\n",
      "train loss:9.359387319709476e-05\n",
      "train loss:0.0008622162094525257\n",
      "train loss:0.0011768404431985726\n",
      "train loss:0.0007704563182082488\n",
      "train loss:0.001337000448386446\n",
      "train loss:0.001510344376038791\n",
      "train loss:0.002819065474775633\n",
      "train loss:0.001710036668140982\n",
      "train loss:0.002143419451425692\n",
      "train loss:0.0021290690749709578\n",
      "train loss:0.00017538304381162676\n",
      "train loss:0.002291606371862161\n",
      "train loss:0.0003346345069260928\n",
      "train loss:0.0020768025731213295\n",
      "train loss:0.0005672535081092474\n",
      "train loss:0.026134334543850713\n",
      "train loss:0.0011471084354117247\n",
      "train loss:0.00033130389003144397\n",
      "train loss:0.0007016450059067268\n",
      "train loss:0.0007049240625398211\n",
      "train loss:0.00012706517026350806\n",
      "train loss:0.0009025518478203942\n",
      "train loss:0.0007035052747178224\n",
      "train loss:9.116604013122281e-05\n",
      "train loss:0.0018852970000956448\n",
      "train loss:0.0005156582724874359\n",
      "train loss:0.0002799381938174909\n",
      "train loss:0.0019913509754172255\n",
      "train loss:0.0015198537154877867\n",
      "train loss:0.002571437139390017\n",
      "train loss:0.0229366156510075\n",
      "train loss:0.0031364314569820805\n",
      "train loss:0.008024032232799011\n",
      "train loss:0.0037527692511090944\n",
      "train loss:0.0011970057580119329\n",
      "train loss:0.0006678344613764137\n",
      "train loss:0.0018944030916254494\n",
      "train loss:0.00542649230250449\n",
      "train loss:0.004685466552922557\n",
      "train loss:0.0004086759283043117\n",
      "train loss:0.0022216683923005688\n",
      "train loss:0.0018323701616176388\n",
      "train loss:0.0009556415317612258\n",
      "train loss:0.002414637898993666\n",
      "train loss:0.0029757519035776433\n",
      "train loss:0.015007186018350771\n",
      "train loss:0.007159854559902301\n",
      "=== epoch:14, train acc:0.995, test acc:0.983 ===\n",
      "train loss:0.004441896257753075\n",
      "train loss:0.002070879739121051\n",
      "train loss:0.0072520932408115775\n",
      "train loss:0.0009765380038957819\n",
      "train loss:0.00010848547918229979\n",
      "train loss:0.003023879842434264\n",
      "train loss:0.00306926027679743\n",
      "train loss:0.004238150656566477\n",
      "train loss:0.0003232455511038097\n",
      "train loss:0.004877994389458723\n",
      "train loss:0.0013797660611342814\n",
      "train loss:0.0002491090665005654\n",
      "train loss:0.002298222573389454\n",
      "train loss:0.000604625270879565\n",
      "train loss:0.0044132248620943485\n",
      "train loss:0.001354373184411748\n",
      "train loss:0.004296291333457131\n",
      "train loss:0.0024082591378979602\n",
      "train loss:0.0010810620140039484\n",
      "train loss:0.0020737918855469194\n",
      "train loss:0.0009762055306289229\n",
      "train loss:0.0039037185311995475\n",
      "train loss:0.0020016759474440478\n",
      "train loss:0.0003827131909960072\n",
      "train loss:0.0012059255362351186\n",
      "train loss:0.0005243652160073619\n",
      "train loss:0.002984098196527193\n",
      "train loss:0.000604851054279511\n",
      "train loss:0.001085824488399032\n",
      "train loss:0.006834597797649333\n",
      "train loss:0.003430168508321031\n",
      "train loss:0.00012713292160289612\n",
      "train loss:0.0004764241853903378\n",
      "train loss:0.0005230940639076769\n",
      "train loss:0.0012463169488826538\n",
      "train loss:0.00010529159923330171\n",
      "train loss:0.03236310580139741\n",
      "train loss:0.0007554000246160089\n",
      "train loss:0.002915970773734301\n",
      "train loss:0.0006367881686014606\n",
      "train loss:0.0061109084565699944\n",
      "train loss:0.01345945977745823\n",
      "train loss:0.0006332404798676335\n",
      "train loss:0.0036770062503373884\n",
      "train loss:0.007151354604266152\n",
      "train loss:0.001522269973228409\n",
      "train loss:0.0006359423219539819\n",
      "train loss:0.0004707875680288204\n",
      "train loss:0.005911688490294116\n",
      "train loss:0.0018498662391464634\n",
      "train loss:0.0017369929612930405\n",
      "train loss:0.0022406945995238404\n",
      "train loss:0.00048079148262269436\n",
      "train loss:0.00022690094928119235\n",
      "train loss:7.894687625552228e-05\n",
      "train loss:0.0005885813543071048\n",
      "train loss:0.00017450404658454508\n",
      "train loss:0.004004896898743368\n",
      "train loss:0.0005449406617301438\n",
      "train loss:0.004579002305003656\n",
      "train loss:0.001534260512356269\n",
      "train loss:0.0006061471915420488\n",
      "train loss:9.117080159284467e-05\n",
      "train loss:0.000570456888675909\n",
      "train loss:0.000390093910832\n",
      "train loss:0.0020677730235873853\n",
      "train loss:0.0025157255989378836\n",
      "train loss:0.007153439323462874\n",
      "train loss:0.0008890439378784876\n",
      "train loss:0.003590681776599118\n",
      "train loss:0.002563570728066702\n",
      "train loss:0.0002887887256196161\n",
      "train loss:0.0004678496701479447\n",
      "train loss:0.00012719983744942265\n",
      "train loss:0.001663628356255388\n",
      "train loss:0.0005406934337834439\n",
      "train loss:0.0010558054893194256\n",
      "train loss:0.0002164862028223566\n",
      "train loss:0.0016134998794923516\n",
      "train loss:0.0019565059599536976\n",
      "train loss:0.00042309668308533876\n",
      "train loss:8.86220378744837e-05\n",
      "train loss:0.0008709596731981029\n",
      "train loss:0.0005153250757254636\n",
      "train loss:0.0012163023757096225\n",
      "train loss:0.00011570640681653081\n",
      "train loss:0.0019907183690702087\n",
      "train loss:0.036590992148238355\n",
      "train loss:0.0005938354706638586\n",
      "train loss:0.021438728625693192\n",
      "train loss:0.000443239557603599\n",
      "train loss:0.0071641617754063655\n",
      "train loss:0.0005672378999866832\n",
      "train loss:0.0021645085441807876\n",
      "train loss:0.0018103183594648977\n",
      "train loss:0.0016965456555775276\n",
      "train loss:0.0001642873940383337\n",
      "train loss:0.00046809428754239935\n",
      "train loss:0.0042040198023478165\n",
      "train loss:0.0010510522519964387\n",
      "train loss:0.0027202582805491153\n",
      "train loss:0.012174210004535634\n",
      "train loss:0.00011927667481333302\n",
      "train loss:0.0020837433182320847\n",
      "train loss:0.0003313463943698957\n",
      "train loss:0.00501043763634483\n",
      "train loss:0.00022076653781372797\n",
      "train loss:0.0008971077249109677\n",
      "train loss:0.0016564664509477606\n",
      "train loss:0.00145455160083833\n",
      "train loss:0.0007515938469480553\n",
      "train loss:0.00048090295558078644\n",
      "train loss:0.0032524883652547167\n",
      "train loss:0.000232424546063896\n",
      "train loss:0.0006232142628966022\n",
      "train loss:0.0007028650230410069\n",
      "train loss:0.000753066814601824\n",
      "train loss:0.0005675855553360182\n",
      "train loss:0.014014918085892527\n",
      "train loss:0.0005576895739663289\n",
      "train loss:0.00039453960364096904\n",
      "train loss:0.000885701660590043\n",
      "train loss:0.0015930573352201424\n",
      "train loss:0.0008941442601175391\n",
      "train loss:0.0036370555627572828\n",
      "train loss:0.0077450207705193905\n",
      "train loss:0.002870730319582622\n",
      "train loss:0.0006611969880850541\n",
      "train loss:0.0005668161274719275\n",
      "train loss:6.217373344560331e-05\n",
      "train loss:0.0005209373588810898\n",
      "train loss:0.0005164834493435587\n",
      "train loss:0.000997295951034834\n",
      "train loss:0.0002564351182303206\n",
      "train loss:0.00027441567910112914\n",
      "train loss:0.0004523299116918277\n",
      "train loss:0.0012091459839867697\n",
      "train loss:0.00016166915108332646\n",
      "train loss:0.0001559447914385087\n",
      "train loss:0.002709833712752673\n",
      "train loss:0.0030321579480211748\n",
      "train loss:0.00017703408692488\n",
      "train loss:0.0003245629293036942\n",
      "train loss:0.0001592224857285958\n",
      "train loss:0.0005864787542994548\n",
      "train loss:0.0005355835787823271\n",
      "train loss:0.0005267602780861814\n",
      "train loss:0.0008713248707603162\n",
      "train loss:0.0008293445235922408\n",
      "train loss:0.0005776242307648117\n",
      "train loss:0.0010538908805468458\n",
      "train loss:0.0017945693986158205\n",
      "train loss:0.0015740114434478073\n",
      "train loss:0.0015098165814012817\n",
      "train loss:0.0027225805180552762\n",
      "train loss:0.0014996511075013174\n",
      "train loss:4.579228090171736e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0003822054320138964\n",
      "train loss:8.886563358179826e-05\n",
      "train loss:0.0032384580082078927\n",
      "train loss:0.0018271389702240648\n",
      "train loss:0.004103631597365044\n",
      "train loss:0.0026161361062580256\n",
      "train loss:0.0008375515305459101\n",
      "train loss:0.00020223117259903197\n",
      "train loss:0.14509389531866076\n",
      "train loss:0.001100581802532232\n",
      "train loss:0.014154094118606696\n",
      "train loss:0.0025754008275488366\n",
      "train loss:0.001996954963661316\n",
      "train loss:0.002083130680877687\n",
      "train loss:0.0007482693914626122\n",
      "train loss:0.0023615768368900935\n",
      "train loss:0.00017285146976765608\n",
      "train loss:0.004812770312367704\n",
      "train loss:0.0027602341335291524\n",
      "train loss:0.0008772138419960451\n",
      "train loss:0.002407350318291189\n",
      "train loss:0.0004940229200144095\n",
      "train loss:0.004583943487728978\n",
      "train loss:0.0048425844047101235\n",
      "train loss:0.0002809153490173757\n",
      "train loss:0.000292698978935708\n",
      "train loss:0.00021071600763247783\n",
      "train loss:0.00022182819639143074\n",
      "train loss:0.0003199490516089465\n",
      "train loss:0.0017835426304035258\n",
      "train loss:0.0022214303284283253\n",
      "train loss:0.00038573362082351464\n",
      "train loss:0.002132656939049794\n",
      "train loss:0.003275156937329342\n",
      "train loss:0.014143375328975272\n",
      "train loss:0.0014122912547946897\n",
      "train loss:0.0019049838700413483\n",
      "train loss:0.00018328325259900865\n",
      "train loss:0.007988372973743333\n",
      "train loss:4.155289536141108e-05\n",
      "train loss:0.003907208904619285\n",
      "train loss:0.01855832993481537\n",
      "train loss:0.0010549615491298489\n",
      "train loss:0.0019651431122340644\n",
      "train loss:0.00017159424329092092\n",
      "train loss:0.004896646249577924\n",
      "train loss:0.0012824902073866188\n",
      "train loss:0.0022131919004810753\n",
      "train loss:0.0008894939657327886\n",
      "train loss:0.0006321561450922583\n",
      "train loss:0.00043112898760186663\n",
      "train loss:0.0004381625552226494\n",
      "train loss:0.00048197497700837777\n",
      "train loss:0.0010242186370499537\n",
      "train loss:0.001575502640762871\n",
      "train loss:0.0008716825663022991\n",
      "train loss:5.461470030641512e-05\n",
      "train loss:0.0006825337288417818\n",
      "train loss:0.0004984102601323844\n",
      "train loss:0.0009873954031000953\n",
      "train loss:0.05156008007057948\n",
      "train loss:0.0010872675059341059\n",
      "train loss:0.005484531871077869\n",
      "train loss:8.657497471677203e-05\n",
      "train loss:0.000869469419475218\n",
      "train loss:0.0005482135486543058\n",
      "train loss:0.0001611128549615633\n",
      "train loss:0.005154099524965693\n",
      "train loss:0.0001233665660840284\n",
      "train loss:0.0003771361691992757\n",
      "train loss:0.0022231919871363083\n",
      "train loss:0.0018912026738133472\n",
      "train loss:0.0008423100683653802\n",
      "train loss:0.003153112157202967\n",
      "train loss:0.0013794127051645047\n",
      "train loss:0.0037143753170275754\n",
      "train loss:0.00121306760673237\n",
      "train loss:0.0025786879137667685\n",
      "train loss:0.00017107861876162797\n",
      "train loss:0.0023877452832786984\n",
      "train loss:0.0028709218824388323\n",
      "train loss:0.00026524428138355067\n",
      "train loss:0.0011871925509950828\n",
      "train loss:0.0006002283779637165\n",
      "train loss:0.0005049527108030926\n",
      "train loss:0.0008895628536702788\n",
      "train loss:8.672999085824402e-05\n",
      "train loss:0.0013407651444570217\n",
      "train loss:0.0008447027904938749\n",
      "train loss:0.0002473088236015133\n",
      "train loss:0.0006959869638641507\n",
      "train loss:0.0019496530015502694\n",
      "train loss:0.0010175946179804768\n",
      "train loss:0.0010426834309111123\n",
      "train loss:7.524047284713787e-05\n",
      "train loss:2.58633855239044e-05\n",
      "train loss:0.0010446755079420481\n",
      "train loss:0.0006060441370482141\n",
      "train loss:0.0005396608918309914\n",
      "train loss:0.0009269289357275896\n",
      "train loss:0.0008492096953178684\n",
      "train loss:0.00071097464955151\n",
      "train loss:0.0034425614681237253\n",
      "train loss:0.004111836752677164\n",
      "train loss:0.0007056964978315402\n",
      "train loss:0.0005902437110427556\n",
      "train loss:0.002041329486163552\n",
      "train loss:0.0003772268212839007\n",
      "train loss:0.001067947726440189\n",
      "train loss:0.003645020887154837\n",
      "train loss:0.0014581649776047275\n",
      "train loss:0.0014439535439491722\n",
      "train loss:0.002598024104590168\n",
      "train loss:0.00010436482112118254\n",
      "train loss:0.0007440399262497355\n",
      "train loss:0.002126225403006183\n",
      "train loss:0.00016879449643602428\n",
      "train loss:0.00014434000069466917\n",
      "train loss:0.00039733518221246556\n",
      "train loss:0.00010396918673041374\n",
      "train loss:0.0001529402004073934\n",
      "train loss:0.0009863743164863917\n",
      "train loss:1.1010110031883952e-05\n",
      "train loss:0.0003147811632360703\n",
      "train loss:0.0049969942633052745\n",
      "train loss:0.005466738107149772\n",
      "train loss:0.06098978940099533\n",
      "train loss:0.0005378068493103095\n",
      "train loss:0.0013757823241374422\n",
      "train loss:0.0019440274899134651\n",
      "train loss:0.00022795737410307308\n",
      "train loss:0.006406373807011103\n",
      "train loss:0.001712398923117971\n",
      "train loss:0.00037040496364655325\n",
      "train loss:0.006677886598834491\n",
      "train loss:0.00035335927632678614\n",
      "train loss:0.0076430331509132875\n",
      "train loss:0.008604968369934029\n",
      "train loss:0.0009608381084738652\n",
      "train loss:0.00011048465333488362\n",
      "train loss:0.0009172370791252722\n",
      "train loss:0.0015702227333613537\n",
      "train loss:9.3401631289627e-05\n",
      "train loss:0.0005662525423285137\n",
      "train loss:0.0008314008830052007\n",
      "train loss:6.765708481353671e-05\n",
      "train loss:1.6106563015911305e-05\n",
      "train loss:0.00010168263807812669\n",
      "train loss:3.725934764320828e-05\n",
      "train loss:0.00020125786967841366\n",
      "train loss:0.0007828680954243985\n",
      "train loss:0.0003743610666053737\n",
      "train loss:0.0004937590248179264\n",
      "train loss:0.0006910619778454689\n",
      "train loss:0.00010312658014969657\n",
      "train loss:0.00020041421477905275\n",
      "train loss:0.0056538597092174415\n",
      "train loss:0.020537420811169743\n",
      "train loss:0.001115211704626464\n",
      "train loss:0.0022289515716280317\n",
      "train loss:0.00028839651440581624\n",
      "train loss:0.001711947043644093\n",
      "train loss:0.002071858020441766\n",
      "train loss:0.0009472102070007541\n",
      "train loss:0.0008688071537327068\n",
      "train loss:0.003907365162378404\n",
      "train loss:6.917692676058809e-05\n",
      "train loss:0.000808134398600133\n",
      "train loss:0.001307428680926688\n",
      "train loss:0.006833481808102603\n",
      "train loss:0.01097670028718948\n",
      "train loss:0.001111198712278175\n",
      "train loss:0.00582880500225565\n",
      "train loss:0.0005635221612335417\n",
      "train loss:0.015147186669233514\n",
      "train loss:0.003073600452476264\n",
      "train loss:0.0016022438455787114\n",
      "train loss:0.004007142673597251\n",
      "train loss:2.394390721983051e-05\n",
      "train loss:0.0003387923126973021\n",
      "train loss:0.0011131177693723974\n",
      "train loss:0.0008858852951809158\n",
      "train loss:0.005350877854502765\n",
      "train loss:0.0014125122415897\n",
      "train loss:0.0010737824717187373\n",
      "train loss:0.003183380598705751\n",
      "train loss:0.0011223499912836085\n",
      "train loss:0.00207594080913592\n",
      "train loss:0.00013624708663999968\n",
      "train loss:0.005933980198516977\n",
      "train loss:0.0012803715053758887\n",
      "train loss:0.0002750001245499388\n",
      "train loss:7.070561107557653e-05\n",
      "train loss:0.0028305018278944054\n",
      "train loss:0.0012705200042573328\n",
      "train loss:0.0029112689419171166\n",
      "train loss:0.00010406593710093746\n",
      "train loss:0.0010701038756897694\n",
      "train loss:0.000767756356366773\n",
      "train loss:0.0015810176593226797\n",
      "train loss:0.00015542647786966215\n",
      "train loss:0.0006240372101913073\n",
      "train loss:0.0011105824747925706\n",
      "train loss:0.00045313804864677146\n",
      "train loss:0.0007296725341419158\n",
      "train loss:0.0036105437232095065\n",
      "train loss:0.011411077665843488\n",
      "train loss:0.0003750242464560358\n",
      "train loss:0.0003296557722021421\n",
      "train loss:0.0009927719206087743\n",
      "train loss:0.00040199449638968714\n",
      "train loss:0.0002532147219898161\n",
      "train loss:0.003444375999763959\n",
      "train loss:0.002271167188024554\n",
      "train loss:0.001471756510626158\n",
      "train loss:0.00028509129787599076\n",
      "train loss:0.002555336543891436\n",
      "train loss:0.001447791839271348\n",
      "train loss:0.00018099213761430213\n",
      "train loss:0.0019836484406330996\n",
      "train loss:0.001423998679141906\n",
      "train loss:0.0010433134364824501\n",
      "train loss:0.0010744269863721464\n",
      "train loss:0.00857502262436581\n",
      "train loss:0.0016423153491453779\n",
      "train loss:0.0009053198727207932\n",
      "train loss:0.0008817396900113071\n",
      "train loss:0.0005643148352198792\n",
      "train loss:1.8570671501839878e-05\n",
      "train loss:0.00018571265156935672\n",
      "train loss:6.555744930690433e-05\n",
      "train loss:0.0008350608170170132\n",
      "train loss:0.0003080275203647806\n",
      "train loss:0.0002422257312578806\n",
      "train loss:0.00012080447724977308\n",
      "train loss:0.0003019159468800966\n",
      "train loss:0.000285053466475201\n",
      "train loss:0.0010088110463673233\n",
      "train loss:0.007416776410583791\n",
      "train loss:0.00021519982282922946\n",
      "train loss:0.0012644045949203844\n",
      "train loss:0.00035111600974026177\n",
      "train loss:0.0003575891642706254\n",
      "train loss:0.0004532153200768357\n",
      "train loss:0.0015458752126852008\n",
      "train loss:0.0006753527281032484\n",
      "train loss:0.0005850871994240493\n",
      "train loss:0.049740146742689006\n",
      "train loss:0.002754034474882775\n",
      "train loss:0.03849561498674715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.000572156211890476\n",
      "train loss:0.0007460889368798772\n",
      "train loss:0.0017594443113882908\n",
      "train loss:0.0017869596842677695\n",
      "train loss:0.000554317453352714\n",
      "train loss:3.3115520890739075e-05\n",
      "train loss:0.00013103201487437162\n",
      "train loss:0.002120047003490806\n",
      "train loss:0.0006597687705868494\n",
      "train loss:0.0002037169843247149\n",
      "train loss:0.0001551825729100143\n",
      "train loss:0.004365280104696102\n",
      "train loss:0.06793203578066384\n",
      "train loss:0.0005194550993339757\n",
      "train loss:0.00016856278022192678\n",
      "train loss:0.000548325852477741\n",
      "train loss:8.828626625229587e-05\n",
      "train loss:0.0011710996714334272\n",
      "train loss:0.0001345775802736845\n",
      "train loss:0.0032900341214018186\n",
      "train loss:0.0007367449640356067\n",
      "train loss:0.0013807280077057914\n",
      "train loss:0.007368297054656378\n",
      "train loss:0.0023121924150779284\n",
      "train loss:0.002143527548537536\n",
      "train loss:0.004270551081768729\n",
      "train loss:0.0015394759180464455\n",
      "train loss:0.0008176509864938702\n",
      "train loss:0.000840737026554352\n",
      "train loss:0.0012553561236526183\n",
      "train loss:0.002779196108709028\n",
      "train loss:0.001057130795566971\n",
      "train loss:0.00036842514512515573\n",
      "train loss:0.0006325787136516266\n",
      "train loss:0.0009583020667514691\n",
      "train loss:0.0029545307002506356\n",
      "train loss:0.00190184188260596\n",
      "train loss:0.006845811735914134\n",
      "train loss:0.0012810006673008912\n",
      "train loss:0.0005246544591223606\n",
      "train loss:0.00033500464520482123\n",
      "train loss:0.0026767302357954205\n",
      "train loss:0.000648979285563019\n",
      "train loss:0.0026334860513447826\n",
      "train loss:0.0001962725492740644\n",
      "train loss:0.0007868637035530404\n",
      "train loss:0.0008800155923238362\n",
      "train loss:5.403438510342783e-06\n",
      "train loss:0.0008507795783362795\n",
      "train loss:0.0009909459762926394\n",
      "train loss:0.00011050013807300686\n",
      "train loss:0.0013277350683137434\n",
      "train loss:0.00030840131451925453\n",
      "train loss:0.0004993788439635268\n",
      "train loss:0.0006274728846525856\n",
      "train loss:0.0006071282427897068\n",
      "train loss:0.0009167112431196214\n",
      "train loss:0.0003805184151037501\n",
      "train loss:8.19716080933045e-05\n",
      "train loss:0.0025217099876664382\n",
      "train loss:0.0009193325035617595\n",
      "train loss:0.007452969792007863\n",
      "train loss:0.00013133308853025325\n",
      "train loss:0.002803395499364205\n",
      "train loss:0.005044096345528939\n",
      "train loss:0.034747680185836405\n",
      "train loss:0.00012721448255122977\n",
      "train loss:0.00023117204647676\n",
      "train loss:0.00020190896579290273\n",
      "train loss:0.003689263067328984\n",
      "train loss:0.0001674128869523493\n",
      "train loss:0.00013024280948606263\n",
      "train loss:0.0014856012433460764\n",
      "train loss:0.000171656787552591\n",
      "train loss:0.0016321948367432498\n",
      "train loss:0.0007186718195641037\n",
      "train loss:0.0015274897131982996\n",
      "train loss:0.0021457265863958504\n",
      "train loss:0.018792015258073707\n",
      "train loss:0.0011374556096664485\n",
      "train loss:0.0008703891853406749\n",
      "train loss:0.0030175088898134546\n",
      "train loss:0.002628987423237978\n",
      "train loss:0.0030344758588905785\n",
      "train loss:0.0002289124381492216\n",
      "train loss:0.0014753412050868258\n",
      "train loss:0.00044348588324448463\n",
      "train loss:0.005661476316735947\n",
      "train loss:0.0007504208523492771\n",
      "train loss:0.00023514360831269578\n",
      "train loss:0.0013720851982525514\n",
      "train loss:0.0004300863470177627\n",
      "train loss:0.039338079395325326\n",
      "train loss:0.0006357110647607322\n",
      "train loss:0.000946834991628093\n",
      "train loss:0.000481466277568507\n",
      "train loss:0.00023202403852563155\n",
      "train loss:0.0016961797934632492\n",
      "train loss:0.0031267916274217854\n",
      "train loss:0.006925406903915736\n",
      "train loss:0.011597724264241178\n",
      "train loss:0.0014239112879355605\n",
      "train loss:0.0010149211891719865\n",
      "train loss:0.0026319187772012058\n",
      "train loss:0.006535148522186324\n",
      "train loss:0.0020785392135661243\n",
      "train loss:0.0014801794055051168\n",
      "train loss:0.001392030251699238\n",
      "train loss:0.0009047212627598679\n",
      "train loss:0.0007893282624771297\n",
      "train loss:0.0007737950568048109\n",
      "train loss:0.0013365982772464958\n",
      "train loss:0.0005852090343104609\n",
      "train loss:0.00270799275039141\n",
      "train loss:0.0014283638686018857\n",
      "train loss:0.002853508528776482\n",
      "train loss:0.0013277366507123928\n",
      "train loss:0.014725239536676956\n",
      "train loss:0.0010470415983919008\n",
      "train loss:0.0010266848231219878\n",
      "train loss:0.008796467768418227\n",
      "train loss:0.0006010087329785637\n",
      "train loss:0.000893992609053291\n",
      "train loss:0.0012973533140912847\n",
      "train loss:0.003728256228206909\n",
      "train loss:0.00015556998534434335\n",
      "train loss:0.010240622408268962\n",
      "train loss:0.0016848101727882526\n",
      "train loss:0.0025129583744386104\n",
      "train loss:0.0008110836637925806\n",
      "train loss:0.0006904551485015525\n",
      "train loss:0.0066424248079295966\n",
      "train loss:0.0003724440857387507\n",
      "train loss:0.00018852543078997055\n",
      "train loss:0.0036764155194914864\n",
      "train loss:0.0009253742915508862\n",
      "train loss:0.0023069258606467436\n",
      "train loss:0.00037009089606537143\n",
      "train loss:0.018157466360294874\n",
      "train loss:0.0003321474661290839\n",
      "train loss:0.013103245818360481\n",
      "train loss:0.002969620806036642\n",
      "train loss:0.00039234729260794826\n",
      "train loss:0.002125307207681878\n",
      "train loss:0.0006930500285547615\n",
      "train loss:0.00020719846500015808\n",
      "train loss:0.0007960625515540823\n",
      "train loss:0.0006375845847311505\n",
      "train loss:0.004124855666068311\n",
      "train loss:0.00032869047502341917\n",
      "train loss:0.0008581830067208705\n",
      "train loss:0.005500511452446419\n",
      "train loss:0.0010157806004566046\n",
      "train loss:0.0046863621277583615\n",
      "train loss:0.00021179582417310615\n",
      "train loss:0.0016317785540649282\n",
      "train loss:0.0016914149633742292\n",
      "train loss:0.0012746286978946094\n",
      "train loss:0.007464142391642967\n",
      "train loss:0.003287813429061418\n",
      "train loss:0.0007966819726248572\n",
      "train loss:0.0006039198913733425\n",
      "train loss:0.11103244790003373\n",
      "train loss:0.001979397559005779\n",
      "train loss:0.000497006712933128\n",
      "train loss:0.009923621077294774\n",
      "train loss:0.00029783694654337254\n",
      "train loss:0.0027424510378184887\n",
      "train loss:0.002487073586034479\n",
      "train loss:0.0006726615236216783\n",
      "train loss:0.00022260462048792758\n",
      "train loss:0.0009537542521744033\n",
      "train loss:0.0006489218560854037\n",
      "train loss:0.0023857940584746796\n",
      "train loss:0.0031989516907099894\n",
      "train loss:0.0005553660700627059\n",
      "train loss:0.0018469950036692645\n",
      "train loss:0.0002956408418691912\n",
      "train loss:0.0019970336250105807\n",
      "train loss:0.0019524604634414222\n",
      "train loss:0.001902183840017993\n",
      "train loss:0.00019242261343950615\n",
      "train loss:0.0014877064106210015\n",
      "train loss:0.0016780856448459574\n",
      "train loss:0.00017887295249787723\n",
      "train loss:0.0005933808563123906\n",
      "train loss:0.0011381051470255139\n",
      "train loss:3.6200123887697666e-05\n",
      "train loss:0.0008636332400628356\n",
      "train loss:0.0017902677501899192\n",
      "train loss:0.00016560701486771656\n",
      "train loss:0.00015719037154995083\n",
      "train loss:0.001590449500778287\n",
      "=== epoch:15, train acc:0.996, test acc:0.986 ===\n",
      "train loss:7.38859402751895e-05\n",
      "train loss:0.0006214596008025557\n",
      "train loss:0.0020011000266950034\n",
      "train loss:0.002941440456184249\n",
      "train loss:0.002535522840629806\n",
      "train loss:0.00017989037445836437\n",
      "train loss:0.0022078021020214816\n",
      "train loss:0.0003396642888858116\n",
      "train loss:0.00014152108794643836\n",
      "train loss:0.0005475566148612006\n",
      "train loss:0.0023284513493021016\n",
      "train loss:0.008800071361815015\n",
      "train loss:0.002510007940833353\n",
      "train loss:0.001424497455783976\n",
      "train loss:3.361483707784961e-05\n",
      "train loss:0.0029901813075351295\n",
      "train loss:0.005385945997954984\n",
      "train loss:6.843185771066232e-05\n",
      "train loss:0.0007952898939586101\n",
      "train loss:0.003782365686316095\n",
      "train loss:0.004043721661371411\n",
      "train loss:0.0005998210989995174\n",
      "train loss:0.001296425068235314\n",
      "train loss:0.005124508722621716\n",
      "train loss:0.0014138849647440301\n",
      "train loss:0.001647833962622351\n",
      "train loss:0.0018948347884762693\n",
      "train loss:0.0010169138519580767\n",
      "train loss:0.0004213984372643424\n",
      "train loss:0.0001456406887247629\n",
      "train loss:0.0018713984837084618\n",
      "train loss:0.001918352559888688\n",
      "train loss:0.0015193420862946996\n",
      "train loss:0.0026967450241752043\n",
      "train loss:0.0007803843821974743\n",
      "train loss:0.00013803449673272775\n",
      "train loss:0.00430189314081478\n",
      "train loss:0.000131179657041452\n",
      "train loss:0.0006944186170181828\n",
      "train loss:0.004191226917276386\n",
      "train loss:0.001827763134129916\n",
      "train loss:0.0010289359200804725\n",
      "train loss:0.0007923545312257683\n",
      "train loss:0.00041540590049793006\n",
      "train loss:0.0014510619543007955\n",
      "train loss:0.0008879173110591409\n",
      "train loss:0.00010040425173100346\n",
      "train loss:0.0012616189995596273\n",
      "train loss:0.00044355924295004403\n",
      "train loss:0.0034979122691886255\n",
      "train loss:0.00037265594698390606\n",
      "train loss:0.003997674685586205\n",
      "train loss:0.00017887246050745414\n",
      "train loss:0.00020877696592566102\n",
      "train loss:0.0004601647310040515\n",
      "train loss:0.003508478533650251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:6.195253093039048e-05\n",
      "train loss:0.001653513463193549\n",
      "train loss:0.00029743634108866127\n",
      "train loss:0.0001670581447192678\n",
      "train loss:0.0005887767892237719\n",
      "train loss:0.0024101368245644035\n",
      "train loss:0.028655282713538205\n",
      "train loss:0.0006994965862280956\n",
      "train loss:0.0006824823361306077\n",
      "train loss:0.00042281128263195087\n",
      "train loss:0.01339936955171863\n",
      "train loss:0.0010167064673078407\n",
      "train loss:0.0006032384924874464\n",
      "train loss:0.0009485724514345606\n",
      "train loss:0.0025540202673911623\n",
      "train loss:0.0006274460309642379\n",
      "train loss:0.0012161789493567452\n",
      "train loss:0.00022569730006345115\n",
      "train loss:0.00010898909028085606\n",
      "train loss:0.00046381924157285094\n",
      "train loss:6.990465114223945e-05\n",
      "train loss:0.0009600602892507306\n",
      "train loss:4.2811063872247844e-05\n",
      "train loss:0.005777306941371533\n",
      "train loss:0.0013051420729316102\n",
      "train loss:0.001152176105343038\n",
      "train loss:0.0010820581082241164\n",
      "train loss:0.0004991554786702721\n",
      "train loss:0.0003999810239686429\n",
      "train loss:0.0017651847042759838\n",
      "train loss:0.0002921483363212392\n",
      "train loss:0.006025001003415212\n",
      "train loss:0.002488060674709656\n",
      "train loss:0.00017576295172200965\n",
      "train loss:0.0007190600079959389\n",
      "train loss:0.0029242921440228425\n",
      "train loss:0.0013374319280110576\n",
      "train loss:0.0016733004108470512\n",
      "train loss:5.540645458398395e-05\n",
      "train loss:7.488664606266309e-05\n",
      "train loss:8.956139228606315e-05\n",
      "train loss:0.0013509872028250036\n",
      "train loss:0.0036846538915616305\n",
      "train loss:0.00033047214329832574\n",
      "train loss:0.0003243647627939055\n",
      "train loss:0.007330474174266326\n",
      "train loss:0.0018105776318117412\n",
      "train loss:0.0002794010910805071\n",
      "train loss:0.00011561360853896132\n",
      "train loss:0.00026891625563806867\n",
      "train loss:0.0003506570132958565\n",
      "train loss:0.001341300214094378\n",
      "train loss:0.004192358607621065\n",
      "train loss:0.0007189394728579664\n",
      "train loss:0.00042727728241115343\n",
      "train loss:0.0024529416821469823\n",
      "train loss:0.0017239988614729381\n",
      "train loss:0.0001893193700694763\n",
      "train loss:0.00031858476692781163\n",
      "train loss:0.00019731300289812658\n",
      "train loss:0.0004437570955374366\n",
      "train loss:0.017042578854357872\n",
      "train loss:0.002547150270443261\n",
      "train loss:0.00015855906225125568\n",
      "train loss:0.0010931955210805378\n",
      "train loss:0.0018985767694813447\n",
      "train loss:0.00029462795566507626\n",
      "train loss:0.0011413876242973775\n",
      "train loss:0.002202619883822972\n",
      "train loss:0.000577006314694171\n",
      "train loss:0.000223845908956251\n",
      "train loss:0.002716853692072906\n",
      "train loss:3.202957788209349e-05\n",
      "train loss:0.0015464416825332389\n",
      "train loss:0.0017286911938832294\n",
      "train loss:0.001635392035468477\n",
      "train loss:0.0001809970220937867\n",
      "train loss:0.0011216321652586043\n",
      "train loss:0.0021283370610083595\n",
      "train loss:0.0005211807482548757\n",
      "train loss:7.304572178036919e-05\n",
      "train loss:7.108672096971111e-05\n",
      "train loss:0.0009748109457608703\n",
      "train loss:0.0011706423190722655\n",
      "train loss:2.2861067703402255e-05\n",
      "train loss:0.00016910856241881042\n",
      "train loss:0.004725002491587461\n",
      "train loss:0.0012951223857466227\n",
      "train loss:0.00013472338000458306\n",
      "train loss:0.00035674474889356954\n",
      "train loss:0.000370137238150343\n",
      "train loss:0.00015255580308708717\n",
      "train loss:0.00017639870889739898\n",
      "train loss:0.00017499882157558832\n",
      "train loss:0.0013853785788024986\n",
      "train loss:0.00035939159844711237\n",
      "train loss:4.0133798287198284e-05\n",
      "train loss:7.874696297649223e-05\n",
      "train loss:0.0005323878111462416\n",
      "train loss:7.511301124021547e-05\n",
      "train loss:0.00023606568858888414\n",
      "train loss:0.00017730474413493985\n",
      "train loss:0.0018101072690517841\n",
      "train loss:0.002300400090600436\n",
      "train loss:3.674329906289234e-05\n",
      "train loss:0.002487544093956098\n",
      "train loss:6.32980501892507e-05\n",
      "train loss:0.00032848210355523703\n",
      "train loss:0.0001658962821284183\n",
      "train loss:7.885372423710643e-05\n",
      "train loss:0.001777647750372855\n",
      "train loss:0.0018918738628861248\n",
      "train loss:0.0005515831608431965\n",
      "train loss:0.00016121305724990314\n",
      "train loss:0.0012605309615429374\n",
      "train loss:7.167010157915639e-05\n",
      "train loss:0.00361352150165683\n",
      "train loss:0.0024813001642795722\n",
      "train loss:0.0009983488152193935\n",
      "train loss:0.00018844511714687117\n",
      "train loss:0.0017454469354721614\n",
      "train loss:0.03437227826647142\n",
      "train loss:7.631787561835333e-05\n",
      "train loss:0.00041189763792982836\n",
      "train loss:0.0005351058567878399\n",
      "train loss:0.008040072772634273\n",
      "train loss:0.004562695335055409\n",
      "train loss:0.0004565843335891833\n",
      "train loss:0.0002115795077532562\n",
      "train loss:0.0010449574960765737\n",
      "train loss:0.0007413067075208841\n",
      "train loss:0.001409288525690475\n",
      "train loss:0.00014929214910848484\n",
      "train loss:0.00028004383641717\n",
      "train loss:6.096910594133601e-05\n",
      "train loss:0.0015760227772698304\n",
      "train loss:0.0017053526927744683\n",
      "train loss:0.00019424108521797455\n",
      "train loss:0.0012860269837712641\n",
      "train loss:0.001038138472502598\n",
      "train loss:0.012026317049182776\n",
      "train loss:0.0006286151177757674\n",
      "train loss:0.0005225010688981298\n",
      "train loss:0.001026901244112549\n",
      "train loss:0.00038168771732192426\n",
      "train loss:0.001109407571652939\n",
      "train loss:0.0026561660925722066\n",
      "train loss:0.0001736940845200781\n",
      "train loss:0.0028239593284835624\n",
      "train loss:0.00034987378694789644\n",
      "train loss:0.0010773945184478417\n",
      "train loss:5.091974745627997e-05\n",
      "train loss:9.323372973494397e-05\n",
      "train loss:0.00038719853478065807\n",
      "train loss:0.00045954287218512296\n",
      "train loss:4.196818821463363e-05\n",
      "train loss:0.0004437798444780895\n",
      "train loss:0.0009493083769837189\n",
      "train loss:0.0014467260778587543\n",
      "train loss:0.00012209819651049305\n",
      "train loss:0.0007453905997627759\n",
      "train loss:0.00010224326189104123\n",
      "train loss:0.0003651167005562608\n",
      "train loss:0.0005817568785913392\n",
      "train loss:1.8612209879628394e-05\n",
      "train loss:0.0002979585100291795\n",
      "train loss:0.0004356426861303755\n",
      "train loss:0.00015395934389499338\n",
      "train loss:0.0004944162339770127\n",
      "train loss:5.132113308730662e-05\n",
      "train loss:0.000582925177133336\n",
      "train loss:0.004949020186962011\n",
      "train loss:0.0004952448252453537\n",
      "train loss:2.864359562802021e-05\n",
      "train loss:0.0012942052336689192\n",
      "train loss:3.300984718584228e-05\n",
      "train loss:0.0003419362706334614\n",
      "train loss:0.000757206850954115\n",
      "train loss:0.0006042052747412\n",
      "train loss:0.00013079303747784787\n",
      "train loss:0.00037078799214425176\n",
      "train loss:0.0001437278091069486\n",
      "train loss:0.0001060875087317799\n",
      "train loss:0.0004679288636391581\n",
      "train loss:0.0008943896438869045\n",
      "train loss:0.001436837137458157\n",
      "train loss:0.00045357432220215156\n",
      "train loss:0.0011308805985161977\n",
      "train loss:0.00042927787605832803\n",
      "train loss:0.001382570663311676\n",
      "train loss:0.00027264794051429583\n",
      "train loss:0.00040237222109266676\n",
      "train loss:3.224596197447681e-05\n",
      "train loss:0.0006341474668732417\n",
      "train loss:0.0006670912501383005\n",
      "train loss:0.0034190602529991453\n",
      "train loss:0.00021212793078036494\n",
      "train loss:0.003368136009731239\n",
      "train loss:0.00021823486587284623\n",
      "train loss:5.578274485732438e-05\n",
      "train loss:0.0003992134140805451\n",
      "train loss:0.0002359484372926109\n",
      "train loss:9.523428303354162e-05\n",
      "train loss:0.002030506820138924\n",
      "train loss:0.00015679335698920083\n",
      "train loss:0.0028090908970404404\n",
      "train loss:0.0008646215810078704\n",
      "train loss:0.001328408110454441\n",
      "train loss:0.0018668073381368612\n",
      "train loss:0.00028911790354203\n",
      "train loss:0.0004993885177490643\n",
      "train loss:0.0006910546457103384\n",
      "train loss:0.0009994740109172699\n",
      "train loss:0.00039203710663448084\n",
      "train loss:0.01268833002787714\n",
      "train loss:0.0004695202152138869\n",
      "train loss:0.0013194492317046302\n",
      "train loss:0.0001431911811911196\n",
      "train loss:0.006291631034151424\n",
      "train loss:0.0005076633959982797\n",
      "train loss:0.0032979732657884963\n",
      "train loss:0.004785578677260402\n",
      "train loss:4.3773279954289333e-05\n",
      "train loss:0.0006960661558856087\n",
      "train loss:0.0017558848396434215\n",
      "train loss:0.0003177846739812128\n",
      "train loss:0.0012522877437448892\n",
      "train loss:0.0009458882033038792\n",
      "train loss:0.0035860270775207336\n",
      "train loss:0.0018523051489264108\n",
      "train loss:0.0013229254281337782\n",
      "train loss:0.0006098371853783111\n",
      "train loss:0.002122016588375037\n",
      "train loss:0.0004460180633359658\n",
      "train loss:5.1849083530673654e-05\n",
      "train loss:0.0004111974650568572\n",
      "train loss:0.0010408666694487114\n",
      "train loss:0.007023954186291101\n",
      "train loss:0.001412957526639312\n",
      "train loss:0.0005692578197227311\n",
      "train loss:0.0061291421538352256\n",
      "train loss:0.004589903981547616\n",
      "train loss:0.0007664576740810164\n",
      "train loss:0.000514504322431181\n",
      "train loss:0.0009101237012837379\n",
      "train loss:0.0010017467300736404\n",
      "train loss:0.00010006530950152379\n",
      "train loss:2.525087111620201e-05\n",
      "train loss:0.0010995607537006462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0005091681671355038\n",
      "train loss:0.0011493048182594776\n",
      "train loss:0.0001021814195271809\n",
      "train loss:0.005733461297218001\n",
      "train loss:0.000985895887344461\n",
      "train loss:0.0001409024679895027\n",
      "train loss:0.014468845978680742\n",
      "train loss:0.00017971060230974838\n",
      "train loss:2.0735784330871552e-05\n",
      "train loss:3.622080391781441e-05\n",
      "train loss:0.0006501669723461062\n",
      "train loss:0.000785277424865179\n",
      "train loss:0.0003148865074927932\n",
      "train loss:6.511845509818592e-05\n",
      "train loss:0.0011754406409825657\n",
      "train loss:0.0012663935494494558\n",
      "train loss:0.0011095270672762572\n",
      "train loss:0.0023870123128187674\n",
      "train loss:4.100433726400261e-05\n",
      "train loss:0.0007798578774925174\n",
      "train loss:2.553401039438942e-05\n",
      "train loss:0.0008961782431432486\n",
      "train loss:0.0122883015254205\n",
      "train loss:0.0014841446765050837\n",
      "train loss:0.0004552852303467618\n",
      "train loss:0.0009496334590814663\n",
      "train loss:0.0011421228476278164\n",
      "train loss:0.0014144856394108047\n",
      "train loss:8.144040153914335e-06\n",
      "train loss:0.00029788242754226295\n",
      "train loss:0.000271540580498248\n",
      "train loss:0.001388298474076626\n",
      "train loss:0.00030400437240519373\n",
      "train loss:7.980227020512963e-05\n",
      "train loss:4.658654649061773e-05\n",
      "train loss:0.0011969230622874227\n",
      "train loss:0.011924832842548907\n",
      "train loss:0.0040796836047536636\n",
      "train loss:0.008782722084870064\n",
      "train loss:0.0010261722529993265\n",
      "train loss:0.0011904165286049724\n",
      "train loss:0.0025979281631527584\n",
      "train loss:0.0007645900520101473\n",
      "train loss:0.0016034348694865827\n",
      "train loss:0.0013308576103307913\n",
      "train loss:0.0005848850480714346\n",
      "train loss:0.0008231283917744626\n",
      "train loss:0.001966264572294411\n",
      "train loss:0.0003116632301436384\n",
      "train loss:0.0012774180208987892\n",
      "train loss:0.00010479487213677209\n",
      "train loss:0.012104699939976815\n",
      "train loss:6.37381350645458e-05\n",
      "train loss:4.780358890838495e-05\n",
      "train loss:5.464521579694732e-05\n",
      "train loss:0.00032767942802341553\n",
      "train loss:0.04996225445082523\n",
      "train loss:0.0032179614809717634\n",
      "train loss:0.009850863832497354\n",
      "train loss:0.002450894662146855\n",
      "train loss:0.0010811598412371517\n",
      "train loss:0.0010998293215368265\n",
      "train loss:0.0002735421588874152\n",
      "train loss:0.004199769871146497\n",
      "train loss:0.002675948452037391\n",
      "train loss:0.0006717010976069822\n",
      "train loss:0.0001946431903500144\n",
      "train loss:0.0003976390074735402\n",
      "train loss:0.0055039796059027256\n",
      "train loss:0.0006136621664376563\n",
      "train loss:0.004136453674199343\n",
      "train loss:8.437045363832217e-05\n",
      "train loss:0.009989415565696795\n",
      "train loss:0.0006909599466896815\n",
      "train loss:0.0028665303260809304\n",
      "train loss:0.00015756967514594749\n",
      "train loss:0.0007386954179736369\n",
      "train loss:0.0017284099865542738\n",
      "train loss:0.0019913434006101093\n",
      "train loss:0.0002313627326284\n",
      "train loss:5.4717618120964825e-05\n",
      "train loss:0.0021494700793913617\n",
      "train loss:0.0003693989378245592\n",
      "train loss:0.0015326158903938001\n",
      "train loss:0.0039008591548339595\n",
      "train loss:0.0031443675040389255\n",
      "train loss:0.0016971421669113673\n",
      "train loss:0.001047756188211555\n",
      "train loss:0.0021273840603485855\n",
      "train loss:0.0002487472835425334\n",
      "train loss:0.00045204104313376363\n",
      "train loss:0.0002643146360624478\n",
      "train loss:0.00046160176328915797\n",
      "train loss:0.0020509395244586153\n",
      "train loss:0.00015382323662650402\n",
      "train loss:0.00030021048772823787\n",
      "train loss:0.0024626649774812347\n",
      "train loss:0.022794817100625274\n",
      "train loss:0.005519880309435219\n",
      "train loss:0.0002460147968066269\n",
      "train loss:0.007092980809803739\n",
      "train loss:0.001433872614542407\n",
      "train loss:0.0006705628900517163\n",
      "train loss:0.0010123028591740493\n",
      "train loss:0.0010873144032885124\n",
      "train loss:0.0014042636577079701\n",
      "train loss:0.01078178913977294\n",
      "train loss:8.868914331808458e-05\n",
      "train loss:0.0004945676802467594\n",
      "train loss:0.00883075897401376\n",
      "train loss:0.00048340434810550774\n",
      "train loss:0.0007082603814924426\n",
      "train loss:0.00042813253993183726\n",
      "train loss:0.0024986388275583045\n",
      "train loss:0.0013646297165398889\n",
      "train loss:0.00010070407102002439\n",
      "train loss:0.00027619062424680556\n",
      "train loss:3.600449931958973e-05\n",
      "train loss:0.0005489623409625492\n",
      "train loss:0.010368958317707164\n",
      "train loss:0.01083290428570976\n",
      "train loss:0.009395344547091367\n",
      "train loss:0.00013769391875508938\n",
      "train loss:3.900399667988177e-05\n",
      "train loss:0.00344944054487734\n",
      "train loss:0.0013299412849865084\n",
      "train loss:0.0006367492266574087\n",
      "train loss:0.000958829468579865\n",
      "train loss:0.00045520324227497666\n",
      "train loss:0.00038928191371242636\n",
      "train loss:0.0009418088476965069\n",
      "train loss:0.00013139013199490757\n",
      "train loss:0.0005256016841660605\n",
      "train loss:0.0009896007448903755\n",
      "train loss:0.004773160126619306\n",
      "train loss:0.002177573160706591\n",
      "train loss:0.0010788341263547872\n",
      "train loss:0.0028549905828806686\n",
      "train loss:0.002412515021165684\n",
      "train loss:0.0005392615039661916\n",
      "train loss:0.007623870856883738\n",
      "train loss:0.0037784187718838914\n",
      "train loss:0.00042272845436484006\n",
      "train loss:0.0005845130012832919\n",
      "train loss:0.001186119538385566\n",
      "train loss:0.0007129046798842455\n",
      "train loss:2.975926593600216e-05\n",
      "train loss:0.00023001426154436812\n",
      "train loss:0.0005799541213043031\n",
      "train loss:0.0004125600755906454\n",
      "train loss:0.00014046087617746385\n",
      "train loss:0.0014453923524364823\n",
      "train loss:0.002326320544465771\n",
      "train loss:0.00039724659099605534\n",
      "train loss:0.0016252233392656918\n",
      "train loss:0.00035448343487268886\n",
      "train loss:0.0034118445476221853\n",
      "train loss:0.0005385202859041018\n",
      "train loss:0.0007294373066164612\n",
      "train loss:0.000922438800435133\n",
      "train loss:0.002462016942077676\n",
      "train loss:0.0016145722533588962\n",
      "train loss:0.0008109479852951343\n",
      "train loss:0.0023006004599149805\n",
      "train loss:0.00030130608300387476\n",
      "train loss:0.00026325279991682446\n",
      "train loss:0.00038529593125073957\n",
      "train loss:0.004910497560925472\n",
      "train loss:0.0007402758067953096\n",
      "train loss:0.0015391681102242928\n",
      "train loss:0.0010698807954501567\n",
      "train loss:0.0005406091547697552\n",
      "train loss:0.000525714201387393\n",
      "train loss:0.0011681644141095938\n",
      "train loss:5.972343325962964e-05\n",
      "train loss:0.002474939468905116\n",
      "train loss:0.0023301697536017513\n",
      "train loss:2.1304016516956458e-05\n",
      "train loss:0.00010987102047538372\n",
      "train loss:0.019287062089685544\n",
      "train loss:0.0037407863930356117\n",
      "train loss:0.0008283198277277781\n",
      "train loss:0.0020657705427192612\n",
      "train loss:0.0004300674878023538\n",
      "train loss:0.0008665109722982596\n",
      "train loss:0.0007224900356709718\n",
      "train loss:0.0035266237215330286\n",
      "train loss:0.005367805608759702\n",
      "train loss:0.0007456942764426093\n",
      "train loss:0.002319238521004665\n",
      "train loss:0.000552975749449109\n",
      "train loss:0.001391207729030018\n",
      "train loss:0.00014382131490049184\n",
      "train loss:0.0002919984832394521\n",
      "train loss:0.0007916133283731516\n",
      "train loss:0.010570092412474787\n",
      "train loss:0.0008911926405505482\n",
      "train loss:0.0016690574131394873\n",
      "train loss:2.4173866395967038e-05\n",
      "train loss:7.259388764060415e-05\n",
      "train loss:0.005872018097617218\n",
      "train loss:0.00020508267356467277\n",
      "train loss:0.0006434559882041871\n",
      "train loss:0.0006109983661532612\n",
      "train loss:0.0001353221344042767\n",
      "train loss:0.004235571102942471\n",
      "train loss:0.001968301884397143\n",
      "train loss:0.002135368460203326\n",
      "train loss:0.00015070670564405023\n",
      "train loss:0.002058104393276544\n",
      "train loss:0.007970861991696658\n",
      "train loss:0.0007189845406315405\n",
      "train loss:0.009400864758649163\n",
      "train loss:0.0015270504556788838\n",
      "train loss:0.004854358953871635\n",
      "train loss:0.00018623017973823223\n",
      "train loss:0.00018810875000867846\n",
      "train loss:0.0010601141462487417\n",
      "train loss:0.034111271047347666\n",
      "train loss:0.0008330448315515632\n",
      "train loss:3.7095880495592415e-05\n",
      "train loss:0.0017236026857301945\n",
      "train loss:0.0015377505972251238\n",
      "train loss:0.0013813616995299919\n",
      "train loss:9.735341818756169e-05\n",
      "train loss:0.00370854535115855\n",
      "train loss:0.00015564961867309715\n",
      "train loss:0.0010659744951361653\n",
      "train loss:0.0005977319060119337\n",
      "train loss:0.0013838074852012352\n",
      "train loss:0.00046090445623798293\n",
      "train loss:0.001006203309464175\n",
      "train loss:0.04702785318928198\n",
      "train loss:0.00034336859563289114\n",
      "train loss:0.00046677524514848913\n",
      "train loss:0.0001250908160495616\n",
      "train loss:0.0008273270421164387\n",
      "train loss:0.00025215202723922537\n",
      "train loss:0.0020812977534914786\n",
      "train loss:0.0004746181487386142\n",
      "train loss:0.0028501425466586406\n",
      "train loss:0.0014882883745014305\n",
      "train loss:0.0002808805885804899\n",
      "train loss:3.5463315970975834e-05\n",
      "train loss:0.0007838877484120981\n",
      "train loss:0.0023376911312910737\n",
      "train loss:0.0022494807992744816\n",
      "train loss:0.00014042836652393631\n",
      "train loss:0.00024562445341741153\n",
      "train loss:0.004066012025531536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0033413906474323977\n",
      "train loss:0.00014674004772659386\n",
      "train loss:0.005043494524125701\n",
      "train loss:0.0015506942506143765\n",
      "train loss:0.00162165955498905\n",
      "train loss:7.71962067677491e-05\n",
      "train loss:0.002068250442805886\n",
      "train loss:4.001754892842419e-05\n",
      "train loss:0.00010718992229416127\n",
      "train loss:0.0007733310238202462\n",
      "train loss:0.0004996547617573083\n",
      "train loss:0.000811658793240163\n",
      "train loss:0.0006916096496228452\n",
      "train loss:3.8175156444805644e-05\n",
      "train loss:0.011583387643771965\n",
      "train loss:0.00010417486787735091\n",
      "train loss:0.0014863571818645412\n",
      "train loss:0.0007390812973001505\n",
      "train loss:0.004754464595404414\n",
      "train loss:0.007113452688847658\n",
      "train loss:0.0003621926591025568\n",
      "train loss:0.0001325128178589809\n",
      "train loss:1.498975902901127e-05\n",
      "train loss:0.0006909905242487336\n",
      "train loss:5.62661374552658e-05\n",
      "train loss:0.0007398957682820241\n",
      "train loss:0.0026223551199128707\n",
      "train loss:0.0037147630518236468\n",
      "train loss:0.0001546575703159565\n",
      "train loss:0.0018788531041529716\n",
      "train loss:0.0007256281750711629\n",
      "train loss:0.0020935200427688325\n",
      "train loss:0.0001380612804663037\n",
      "train loss:0.0012288062841753294\n",
      "train loss:0.0009855847168404872\n",
      "train loss:6.098048173886783e-05\n",
      "train loss:0.005817735421886767\n",
      "train loss:0.0004633415538322675\n",
      "train loss:0.0023259161595462134\n",
      "train loss:0.0027567251411314917\n",
      "train loss:0.0012573655533185421\n",
      "train loss:0.00042173901753673473\n",
      "train loss:0.0007505767071449565\n",
      "train loss:0.0008002850144000454\n",
      "train loss:0.00020320661303428397\n",
      "=== epoch:16, train acc:0.999, test acc:0.986 ===\n",
      "train loss:0.0013857627235595027\n",
      "train loss:0.006434715652649751\n",
      "train loss:0.00041108209030054304\n",
      "train loss:0.001483245043396276\n",
      "train loss:0.000532629243400306\n",
      "train loss:9.071586318993452e-05\n",
      "train loss:0.00011474398817965075\n",
      "train loss:0.005933652108544891\n",
      "train loss:0.00026464246362964904\n",
      "train loss:0.0002993412223952919\n",
      "train loss:0.0012397364134933602\n",
      "train loss:0.003081875622277659\n",
      "train loss:0.00046982239576589787\n",
      "train loss:1.5877496661020122e-05\n",
      "train loss:0.00014449839832217758\n",
      "train loss:0.002994643896316838\n",
      "train loss:5.2599329089381704e-05\n",
      "train loss:0.00028939057303946076\n",
      "train loss:0.0008873836535567756\n",
      "train loss:0.00078062672823353\n",
      "train loss:3.238548972001869e-05\n",
      "train loss:0.010638102318996877\n",
      "train loss:0.00043137040095341883\n",
      "train loss:0.00030198171279436584\n",
      "train loss:8.83682929392031e-05\n",
      "train loss:0.001054880851713979\n",
      "train loss:0.00012529022245891697\n",
      "train loss:0.000263843148009925\n",
      "train loss:0.00045398357798450946\n",
      "train loss:0.0009585686539143213\n",
      "train loss:0.0001427659905301909\n",
      "train loss:0.0002572121218067543\n",
      "train loss:0.003318822492175838\n",
      "train loss:0.0008149556981841419\n",
      "train loss:0.0004974621503468377\n",
      "train loss:0.0025213827753420835\n",
      "train loss:0.0018627431934390485\n",
      "train loss:0.0012386126487465236\n",
      "train loss:8.041759489756732e-05\n",
      "train loss:0.00039307874473145835\n",
      "train loss:0.008147151521346107\n",
      "train loss:0.00012136620365191586\n",
      "train loss:0.0018096650004191639\n",
      "train loss:0.0004909260409343756\n",
      "train loss:0.00017855294882829055\n",
      "train loss:0.0002206399532302197\n",
      "train loss:0.0027550632519687253\n",
      "train loss:0.009508330446776007\n",
      "train loss:0.0003338538948914264\n",
      "train loss:0.029606927371496016\n",
      "train loss:7.755447315991551e-05\n",
      "train loss:0.000986520849949656\n",
      "train loss:0.0023044752834545856\n",
      "train loss:0.0004274327453031577\n",
      "train loss:0.001455363479074131\n",
      "train loss:0.0008759801539262009\n",
      "train loss:0.0017400129542900805\n",
      "train loss:0.0003876113443158772\n",
      "train loss:8.693341190622444e-05\n",
      "train loss:0.0011244061778290344\n",
      "train loss:0.0017903019104864902\n",
      "train loss:7.651649096635467e-05\n",
      "train loss:0.0001251841690593818\n",
      "train loss:0.000676744267384568\n",
      "train loss:0.0002485465175137437\n",
      "train loss:0.0007351827005131731\n",
      "train loss:0.0007585777346002573\n",
      "train loss:0.0010857486686833976\n",
      "train loss:0.00011612246010475445\n",
      "train loss:0.0016063101751360796\n",
      "train loss:0.00023604417981014323\n",
      "train loss:0.01853889509791675\n",
      "train loss:0.0002853046142011977\n",
      "train loss:0.0008965451127678249\n",
      "train loss:0.0005751958625039743\n",
      "train loss:0.00011287722031287158\n",
      "train loss:0.0012443154641175126\n",
      "train loss:0.000234894007690307\n",
      "train loss:0.0070615010645237565\n",
      "train loss:0.0003168516130930865\n",
      "train loss:0.00011549273726293428\n",
      "train loss:0.00022524521356552508\n",
      "train loss:4.22364430145021e-05\n",
      "train loss:0.0004853758788779664\n",
      "train loss:0.001592832153051056\n",
      "train loss:0.0008160516613451102\n",
      "train loss:5.209285776223874e-05\n",
      "train loss:0.0023875491048005295\n",
      "train loss:9.812125447136957e-05\n",
      "train loss:0.0005913675318825738\n",
      "train loss:9.336176565591042e-05\n",
      "train loss:0.00018835731440756945\n",
      "train loss:0.0007171795993044009\n",
      "train loss:0.0001405700725825248\n",
      "train loss:0.0007269056401403989\n",
      "train loss:0.0003962833628638122\n",
      "train loss:0.0003328298964709955\n",
      "train loss:0.0002782786282796293\n",
      "train loss:0.0016469358370321257\n",
      "train loss:0.0032963722978798142\n",
      "train loss:0.0002954310317424633\n",
      "train loss:0.001415685589915491\n",
      "train loss:0.00013315646023852128\n",
      "train loss:0.003480653960748306\n",
      "train loss:0.001487468257799596\n",
      "train loss:0.0008555096297141857\n",
      "train loss:3.796871922481989e-05\n",
      "train loss:0.0009079960555791973\n",
      "train loss:0.004419977676325708\n",
      "train loss:0.0047560932801407356\n",
      "train loss:0.00020358868030389245\n",
      "train loss:0.00019903845160665695\n",
      "train loss:0.021200813445833074\n",
      "train loss:0.00015204136558137976\n",
      "train loss:2.141605031363212e-05\n",
      "train loss:0.0007618773178349618\n",
      "train loss:0.0012653029532888724\n",
      "train loss:0.0011140792614540587\n",
      "train loss:0.0007806939764166401\n",
      "train loss:0.001297474873552307\n",
      "train loss:0.002159835507797464\n",
      "train loss:0.08062117504819902\n",
      "train loss:0.0038562845727433486\n",
      "train loss:0.0014573353366779257\n",
      "train loss:0.0008539266309939189\n",
      "train loss:0.0004599481349208321\n",
      "train loss:0.006520834379650822\n",
      "train loss:0.0002958920193594926\n",
      "train loss:0.0014042703945502924\n",
      "train loss:0.0005510390049152968\n",
      "train loss:0.0013203984157183568\n",
      "train loss:0.0006839363278846481\n",
      "train loss:0.00017266911892781234\n",
      "train loss:0.001486738756075549\n",
      "train loss:0.002019939851878615\n",
      "train loss:0.00018178913358533804\n",
      "train loss:2.9797744725763733e-05\n",
      "train loss:0.0025888240177711994\n",
      "train loss:0.00023640444234315227\n",
      "train loss:0.0009968741002417915\n",
      "train loss:0.0025871480378589677\n",
      "train loss:0.00012195043853605649\n",
      "train loss:0.0014552324906087653\n",
      "train loss:0.0004934244318407524\n",
      "train loss:0.00024686506445451104\n",
      "train loss:0.001204544594383251\n",
      "train loss:0.0029625627015778173\n",
      "train loss:0.0013105461847722555\n",
      "train loss:0.009059938135744077\n",
      "train loss:0.0004519433085618403\n",
      "train loss:0.0007602828645293921\n",
      "train loss:0.0011031515331560613\n",
      "train loss:0.00018282685484799117\n",
      "train loss:0.0002864664939915004\n",
      "train loss:0.0018913297469096044\n",
      "train loss:0.0005807129669965275\n",
      "train loss:4.03224872734089e-05\n",
      "train loss:0.0002858156240107601\n",
      "train loss:0.002896988355019231\n",
      "train loss:8.316474656885103e-05\n",
      "train loss:0.0005353330212642145\n",
      "train loss:0.001178322407376651\n",
      "train loss:0.0014399365011773444\n",
      "train loss:0.0005996631178111886\n",
      "train loss:0.015152131708818321\n",
      "train loss:8.716302808198105e-05\n",
      "train loss:0.0008994550146949058\n",
      "train loss:0.001187160748548927\n",
      "train loss:0.033579071291652836\n",
      "train loss:7.951857341212951e-05\n",
      "train loss:0.0024027120294091643\n",
      "train loss:0.001321326095087033\n",
      "train loss:0.06722453255323836\n",
      "train loss:0.00027964482548398966\n",
      "train loss:0.00017984457199181196\n",
      "train loss:0.004933194476909602\n",
      "train loss:0.0006336242381465927\n",
      "train loss:0.000356667535784858\n",
      "train loss:0.00019012002565164322\n",
      "train loss:0.001015605430092834\n",
      "train loss:0.003000552452103227\n",
      "train loss:0.0005746082466543225\n",
      "train loss:0.0006136314062394273\n",
      "train loss:0.0024327937338708867\n",
      "train loss:0.0007757875677209103\n",
      "train loss:0.0010364530048309517\n",
      "train loss:0.00452701756139075\n",
      "train loss:6.368594264744698e-05\n",
      "train loss:0.015266518950078092\n",
      "train loss:0.0002382628396799801\n",
      "train loss:0.0008652955341886508\n",
      "train loss:0.0014076008143230582\n",
      "train loss:0.002195048736104069\n",
      "train loss:0.0015478731206232583\n",
      "train loss:1.643895639184897e-05\n",
      "train loss:0.007440606485096312\n",
      "train loss:0.0002280427516011496\n",
      "train loss:0.0011973842522774957\n",
      "train loss:0.000341467188648501\n",
      "train loss:0.0018883259317234032\n",
      "train loss:0.005469950355976972\n",
      "train loss:0.002718529374062396\n",
      "train loss:0.00015865795866838272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.016181792685239033\n",
      "train loss:0.0006345935952508593\n",
      "train loss:0.000347811182198439\n",
      "train loss:0.00016176218809113547\n",
      "train loss:0.000655174540107066\n",
      "train loss:0.0006822133522394869\n",
      "train loss:0.0017860978995596025\n",
      "train loss:0.003448180995237412\n",
      "train loss:0.0008141429221074056\n",
      "train loss:0.0008225058155982562\n",
      "train loss:0.000299301367795592\n",
      "train loss:0.004312304277462563\n",
      "train loss:0.0026808348890489113\n",
      "train loss:0.004377875483366546\n",
      "train loss:0.00024216046193158472\n",
      "train loss:0.00023135665979315574\n",
      "train loss:0.0005577544642949006\n",
      "train loss:0.000645719875170828\n",
      "train loss:0.0016290866801627456\n",
      "train loss:0.0002578554628031388\n",
      "train loss:0.00047304407928047785\n",
      "train loss:0.0005214472367545471\n",
      "train loss:0.003625918745476371\n",
      "train loss:0.0003225741428337625\n",
      "train loss:0.0023811956145263786\n",
      "train loss:0.010862253970186175\n",
      "train loss:0.0002179343836574597\n",
      "train loss:0.0005599649008540716\n",
      "train loss:0.00020030760957310208\n",
      "train loss:0.0009046858582373421\n",
      "train loss:0.0011665188173653326\n",
      "train loss:0.00023713464259238318\n",
      "train loss:0.0010725070593838276\n",
      "train loss:0.0001827185863623971\n",
      "train loss:0.0015199607222957348\n",
      "train loss:0.004141574225142264\n",
      "train loss:0.00013761306577277213\n",
      "train loss:0.00037087655414082954\n",
      "train loss:0.00029981768683211307\n",
      "train loss:0.00018265855146442186\n",
      "train loss:0.002220475956460103\n",
      "train loss:0.01843000620099741\n",
      "train loss:0.0005018571554948177\n",
      "train loss:0.0006071779272462164\n",
      "train loss:0.0005249418554637443\n",
      "train loss:0.0011305300514837743\n",
      "train loss:0.0002764435024676341\n",
      "train loss:0.0003209571314587008\n",
      "train loss:0.0009092735879457762\n",
      "train loss:0.00038386601274881824\n",
      "train loss:0.00018892044367859498\n",
      "train loss:0.0014680349671866181\n",
      "train loss:0.0010959939169567646\n",
      "train loss:0.0005676126385004295\n",
      "train loss:0.005098210588127133\n",
      "train loss:0.002249677276429959\n",
      "train loss:0.00024172155191052008\n",
      "train loss:0.0002013174156293184\n",
      "train loss:0.0013621991740500417\n",
      "train loss:0.009582108900151981\n",
      "train loss:0.0017924560056713531\n",
      "train loss:0.0006723512551640638\n",
      "train loss:5.635250839691949e-06\n",
      "train loss:0.00041516333244823547\n",
      "train loss:0.0002851247977169475\n",
      "train loss:0.0005551925086782207\n",
      "train loss:0.00021175517932960276\n",
      "train loss:0.0011768279223034759\n",
      "train loss:0.0009134894092310543\n",
      "train loss:0.0005409034199297513\n",
      "train loss:0.00011076200844694465\n",
      "train loss:0.0017368347226750967\n",
      "train loss:0.00010916904020933757\n",
      "train loss:0.0031882954006543455\n",
      "train loss:0.000509793774209045\n",
      "train loss:0.0002479328881675465\n",
      "train loss:0.0015120413598120753\n",
      "train loss:0.000178404925813667\n",
      "train loss:3.522609507264081e-05\n",
      "train loss:0.0043084492021109675\n",
      "train loss:0.00037073144787093377\n",
      "train loss:0.0009194157904877812\n",
      "train loss:0.0004334016671954707\n",
      "train loss:0.0014841435201772446\n",
      "train loss:0.0022873702416211657\n",
      "train loss:0.00025181622401510196\n",
      "train loss:0.0023542508320327257\n",
      "train loss:0.002931267390206491\n",
      "train loss:0.001084857513051912\n",
      "train loss:0.0011595386498864705\n",
      "train loss:0.0007058270964949028\n",
      "train loss:0.00016361253378974246\n",
      "train loss:0.003082132118822998\n",
      "train loss:0.0005301387237687886\n",
      "train loss:0.00033169920178609377\n",
      "train loss:6.278746223121357e-05\n",
      "train loss:0.0006192914868473101\n",
      "train loss:0.003159291702664443\n",
      "train loss:0.012510136597260364\n",
      "train loss:8.410087156989268e-06\n",
      "train loss:0.001559288912198869\n",
      "train loss:0.00033049443058497623\n",
      "train loss:0.0002780595899322259\n",
      "train loss:0.004013447674309406\n",
      "train loss:0.00026139297124803154\n",
      "train loss:0.00287326687030998\n",
      "train loss:0.0004654448592700233\n",
      "train loss:0.00026904228669061584\n",
      "train loss:0.0015293121184990943\n",
      "train loss:0.00013291223595913112\n",
      "train loss:0.0013256736538144354\n",
      "train loss:0.00016504410506496274\n",
      "train loss:0.00020837920499407796\n",
      "train loss:0.00017054216663346655\n",
      "train loss:0.003978328239587937\n",
      "train loss:0.003767504854536432\n",
      "train loss:0.0014972323902430445\n",
      "train loss:0.004114467333206212\n",
      "train loss:0.0005651291559463004\n",
      "train loss:0.0015632499781451426\n",
      "train loss:0.0008992996567076149\n",
      "train loss:0.0005942713369219201\n",
      "train loss:0.0010769865616892121\n",
      "train loss:0.0019040437847719501\n",
      "train loss:4.31162378176841e-05\n",
      "train loss:0.00020099531202814178\n",
      "train loss:0.002164554979907389\n",
      "train loss:0.0002577628261699862\n",
      "train loss:0.0004907009116977907\n",
      "train loss:0.003951164945431848\n",
      "train loss:0.00010171191335729822\n",
      "train loss:0.0012724043554137668\n",
      "train loss:0.0018820081303514487\n",
      "train loss:0.0002955143158391458\n",
      "train loss:0.0008744737848814755\n",
      "train loss:0.0037716657520271233\n",
      "train loss:2.9869580119469435e-05\n",
      "train loss:0.0001211153166350676\n",
      "train loss:0.0010302636035556438\n",
      "train loss:0.0002634123403513593\n",
      "train loss:0.0002214884523075727\n",
      "train loss:0.0008224667211995728\n",
      "train loss:0.000766289997164897\n",
      "train loss:0.0018112482502388443\n",
      "train loss:0.0004437617304904129\n",
      "train loss:0.00249302045557646\n",
      "train loss:0.0023818716880852422\n",
      "train loss:0.00029060957779834167\n",
      "train loss:0.0015513499695328952\n",
      "train loss:0.0011666552259490754\n",
      "train loss:0.001963567670019206\n",
      "train loss:0.00010960394063435132\n",
      "train loss:0.00014602436570280208\n",
      "train loss:5.19823416075982e-05\n",
      "train loss:0.0009075539922455967\n",
      "train loss:2.339771569453304e-05\n",
      "train loss:0.0009158784758534776\n",
      "train loss:0.0004098642647575037\n",
      "train loss:0.00017088470692467334\n",
      "train loss:4.011589214146832e-05\n",
      "train loss:0.001210944637993122\n",
      "train loss:0.0005814688348799498\n",
      "train loss:0.00013027151354978763\n",
      "train loss:0.00034386149671168176\n",
      "train loss:0.017101356175305747\n",
      "train loss:0.0016512889456932385\n",
      "train loss:0.001135695105407603\n",
      "train loss:0.0009190408486938033\n",
      "train loss:0.0004262243069483591\n",
      "train loss:7.281670047369332e-05\n",
      "train loss:0.00016935716618285163\n",
      "train loss:0.0003770088608919218\n",
      "train loss:0.00016408896467353363\n",
      "train loss:0.0003971955014598608\n",
      "train loss:1.0720467241412958e-05\n",
      "train loss:0.0005120814868760222\n",
      "train loss:0.0008680372601734221\n",
      "train loss:0.00038274715100465253\n",
      "train loss:0.0007122668012516258\n",
      "train loss:0.000607367185908988\n",
      "train loss:8.924505043739088e-05\n",
      "train loss:0.0003847404652322686\n",
      "train loss:0.0006088995295287238\n",
      "train loss:6.059925737926427e-05\n",
      "train loss:0.0034370489869176802\n",
      "train loss:0.00020603555667385607\n",
      "train loss:0.0008691722214013157\n",
      "train loss:0.0005400811205522814\n",
      "train loss:0.0003216543754915214\n",
      "train loss:0.0018273004825472233\n",
      "train loss:0.0007264386207341496\n",
      "train loss:0.00010249368955569596\n",
      "train loss:0.0001356592011712202\n",
      "train loss:0.00017369518076737302\n",
      "train loss:0.0005850327546035139\n",
      "train loss:3.116712911008115e-05\n",
      "train loss:0.0025262697956710115\n",
      "train loss:0.0018261470042645412\n",
      "train loss:0.0012061458202801889\n",
      "train loss:0.0016324410893486386\n",
      "train loss:0.0004594274965628532\n",
      "train loss:0.0003488895886023344\n",
      "train loss:0.0013719183251939627\n",
      "train loss:0.0002236501069153065\n",
      "train loss:0.0005217289009644523\n",
      "train loss:0.001193822129845375\n",
      "train loss:8.283367825103586e-05\n",
      "train loss:6.111804364313202e-05\n",
      "train loss:0.0004829245365425048\n",
      "train loss:1.6740936421751138e-05\n",
      "train loss:0.0008157176904081271\n",
      "train loss:0.0005035900882273005\n",
      "train loss:0.001800888775212045\n",
      "train loss:0.0005839318695523718\n",
      "train loss:0.0004205440186183386\n",
      "train loss:0.0002455717604548885\n",
      "train loss:0.00030320444412682777\n",
      "train loss:0.0010894470186593194\n",
      "train loss:0.0004419650521530228\n",
      "train loss:0.016296982202190793\n",
      "train loss:0.0006864838265366191\n",
      "train loss:0.0008273116839940417\n",
      "train loss:0.00011247307607215033\n",
      "train loss:0.001288974453117073\n",
      "train loss:0.0009787633219367445\n",
      "train loss:0.0014712208118734028\n",
      "train loss:0.0008802436177675672\n",
      "train loss:0.010098786297678027\n",
      "train loss:0.0008085275010571221\n",
      "train loss:0.003148274253744613\n",
      "train loss:0.0005909536077824429\n",
      "train loss:0.00035876889072534426\n",
      "train loss:0.00019181023730415988\n",
      "train loss:0.0004579775295431973\n",
      "train loss:0.0010789180545431884\n",
      "train loss:0.00013242948910994185\n",
      "train loss:0.00014966466583981623\n",
      "train loss:0.003629325993170528\n",
      "train loss:0.0019528162796247708\n",
      "train loss:0.001259993904661929\n",
      "train loss:0.00016017956433587966\n",
      "train loss:0.00011612977030712944\n",
      "train loss:0.0007190832148661007\n",
      "train loss:4.001086665458798e-05\n",
      "train loss:8.524158042509394e-05\n",
      "train loss:0.0008768287171481589\n",
      "train loss:0.0003050955692153704\n",
      "train loss:0.014141012231519767\n",
      "train loss:0.0063501523599304764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.08662603823830956\n",
      "train loss:0.00042115987164003716\n",
      "train loss:4.010755360790193e-05\n",
      "train loss:0.0018620344368628105\n",
      "train loss:0.0005336205342098974\n",
      "train loss:0.00013168088364912695\n",
      "train loss:3.324797754332393e-05\n",
      "train loss:0.02127827443728022\n",
      "train loss:0.0035513343812066086\n",
      "train loss:0.0009318335861036005\n",
      "train loss:0.0018202794298689603\n",
      "train loss:0.0029974559258125876\n",
      "train loss:0.0004198593797546318\n",
      "train loss:0.0012594102740948953\n",
      "train loss:0.015976118120497724\n",
      "train loss:0.0005375773376210171\n",
      "train loss:0.0008546756764028599\n",
      "train loss:0.0007846466868831965\n",
      "train loss:0.0008019291291326155\n",
      "train loss:0.007928342524673077\n",
      "train loss:0.00048806292727158736\n",
      "train loss:0.0011877301963840992\n",
      "train loss:0.0011027617605389602\n",
      "train loss:4.935290420226898e-05\n",
      "train loss:0.0005319374740197683\n",
      "train loss:0.002633429443747028\n",
      "train loss:0.0004185674802694506\n",
      "train loss:0.0007956910405782026\n",
      "train loss:5.0756018951002524e-05\n",
      "train loss:0.011435959413757217\n",
      "train loss:0.00022350457644500324\n",
      "train loss:0.004110273225564843\n",
      "train loss:0.0003309564926453147\n",
      "train loss:0.03604103826545252\n",
      "train loss:0.00011682429424509276\n",
      "train loss:0.0009000853523209411\n",
      "train loss:0.0017993781400794764\n",
      "train loss:0.0009719016536704638\n",
      "train loss:0.00017750734684003746\n",
      "train loss:0.000377598831156622\n",
      "train loss:0.00021226160663394291\n",
      "train loss:0.003558356765157853\n",
      "train loss:0.011407863518489755\n",
      "train loss:0.00029171284495257835\n",
      "train loss:0.0005061516695843582\n",
      "train loss:0.0013052825591387477\n",
      "train loss:0.00022821448186484465\n",
      "train loss:5.341352579140593e-05\n",
      "train loss:7.139820212601205e-05\n",
      "train loss:8.675360691665357e-05\n",
      "train loss:0.0029267055173600592\n",
      "train loss:0.0004860948530830671\n",
      "train loss:0.0030547311302690306\n",
      "train loss:0.00018075750256710526\n",
      "train loss:0.001798250604075765\n",
      "train loss:0.002148615840644043\n",
      "train loss:0.0008771886564149407\n",
      "train loss:4.259829114177778e-05\n",
      "train loss:0.0014614356851040252\n",
      "train loss:0.0008381634305555555\n",
      "train loss:0.0003539686089803027\n",
      "train loss:0.006100254127780602\n",
      "train loss:0.0007230131488455021\n",
      "train loss:0.0011339394799954488\n",
      "train loss:0.00014874206699746578\n",
      "train loss:0.00020769774652133895\n",
      "train loss:0.0003496231391448256\n",
      "train loss:0.00012352159780001804\n",
      "train loss:0.0026290211015003234\n",
      "train loss:0.002179673082015879\n",
      "train loss:0.003584674412078383\n",
      "train loss:0.00010596551643833764\n",
      "train loss:0.0011099457779884436\n",
      "train loss:0.0001406800880139113\n",
      "train loss:0.0003947910953506477\n",
      "train loss:0.0015157242024036724\n",
      "train loss:0.0004014138916075351\n",
      "train loss:0.0008273737743114542\n",
      "train loss:0.0010012329509231541\n",
      "train loss:0.0002118293814208472\n",
      "train loss:0.0007870642123627499\n",
      "train loss:0.0016060245226515453\n",
      "train loss:0.00014458882579467005\n",
      "train loss:6.961876937375269e-05\n",
      "train loss:0.004694185668067653\n",
      "train loss:5.570729835003198e-05\n",
      "train loss:0.0008827808331089046\n",
      "train loss:0.00043665234659791117\n",
      "train loss:0.00263556113763317\n",
      "train loss:0.00026635344468572465\n",
      "train loss:0.001837760250630312\n",
      "train loss:0.0026785035273913715\n",
      "train loss:8.144254199695687e-05\n",
      "train loss:0.0005578073034228022\n",
      "train loss:5.530445070989886e-05\n",
      "train loss:0.00038510820141251666\n",
      "train loss:0.0009409831334082594\n",
      "train loss:0.00017479877518219875\n",
      "train loss:6.430308245326088e-05\n",
      "train loss:0.0002935542655052207\n",
      "train loss:0.0013749616069695292\n",
      "train loss:0.00012428237024327956\n",
      "train loss:0.0004975495764403761\n",
      "train loss:7.289738716666309e-05\n",
      "train loss:0.000530979047561858\n",
      "train loss:0.0004252886019852453\n",
      "train loss:0.0001328993129143411\n",
      "train loss:0.0001196572325013786\n",
      "train loss:0.00044557716636761056\n",
      "train loss:0.00010802394126637203\n",
      "train loss:0.0002047862643785702\n",
      "train loss:0.00010899277068838021\n",
      "train loss:0.0014189122738265147\n",
      "train loss:0.00011181356957927205\n",
      "train loss:0.001415683316128734\n",
      "train loss:0.00035533263894844166\n",
      "train loss:0.00021195637960166462\n",
      "train loss:4.022267986974422e-05\n",
      "train loss:0.0002812998575415282\n",
      "train loss:0.00014094486813199413\n",
      "train loss:0.0008608467378476479\n",
      "train loss:0.003439702166737614\n",
      "train loss:0.00020514255362682224\n",
      "train loss:6.105820961818933e-05\n",
      "train loss:0.0019189102288991783\n",
      "train loss:0.001399008309533599\n",
      "train loss:8.893468881705922e-05\n",
      "train loss:0.0003503673399968353\n",
      "train loss:0.00018122824546508857\n",
      "train loss:0.001650424405796768\n",
      "train loss:0.00014995180473111324\n",
      "train loss:0.00016809711323527684\n",
      "train loss:0.00020983615034342662\n",
      "train loss:0.0005426748598316174\n",
      "train loss:0.006756273201362176\n",
      "train loss:3.3275239273880503e-05\n",
      "train loss:7.116245281445333e-05\n",
      "train loss:0.0020844537631707913\n",
      "train loss:0.0013577195300372871\n",
      "train loss:0.0014803673712026184\n",
      "train loss:0.0009464848098778095\n",
      "train loss:0.0011660645279587616\n",
      "train loss:0.0008240869569991893\n",
      "train loss:0.027059800426849755\n",
      "train loss:0.00020019702033303687\n",
      "train loss:0.001454292980392412\n",
      "train loss:0.0006311536488793445\n",
      "train loss:3.354012590392854e-05\n",
      "=== epoch:17, train acc:0.999, test acc:0.986 ===\n",
      "train loss:0.002443948587054191\n",
      "train loss:7.16678274114252e-05\n",
      "train loss:0.00013535014668565154\n",
      "train loss:0.0008613679034689632\n",
      "train loss:0.00010576557494416528\n",
      "train loss:8.901886718522941e-05\n",
      "train loss:0.0007751010974178554\n",
      "train loss:0.0013503118530290984\n",
      "train loss:0.0007074794626698984\n",
      "train loss:0.0002226252315464209\n",
      "train loss:9.379846365240252e-05\n",
      "train loss:0.00015442607584515978\n",
      "train loss:0.0016948683107003294\n",
      "train loss:0.0006199809700263029\n",
      "train loss:0.00040420476350789305\n",
      "train loss:8.611370436111619e-05\n",
      "train loss:0.0002650593681008201\n",
      "train loss:1.2051658581493163e-05\n",
      "train loss:0.0007466517592628479\n",
      "train loss:0.007103435858197626\n",
      "train loss:0.004635098166393285\n",
      "train loss:0.0006738519778571958\n",
      "train loss:0.00016516975744297559\n",
      "train loss:0.0038222281127619977\n",
      "train loss:0.00019010157514502866\n",
      "train loss:0.000502775857852975\n",
      "train loss:0.0012996079083321941\n",
      "train loss:0.0004336798089649637\n",
      "train loss:0.00018526083157176178\n",
      "train loss:8.25634429065258e-05\n",
      "train loss:0.0018728147955449342\n",
      "train loss:0.00812221128536686\n",
      "train loss:0.0007655904821008614\n",
      "train loss:0.0002117480793808508\n",
      "train loss:0.0011132608885862756\n",
      "train loss:0.0005836291121607293\n",
      "train loss:0.0004090018252665216\n",
      "train loss:0.0006164788439990361\n",
      "train loss:0.002587905087329738\n",
      "train loss:0.002056515468252406\n",
      "train loss:0.002477187564772256\n",
      "train loss:0.0015148306375902879\n",
      "train loss:0.0016491441461253995\n",
      "train loss:0.004801664240495291\n",
      "train loss:0.005356533320526328\n",
      "train loss:6.588791392898142e-05\n",
      "train loss:0.03434790138381254\n",
      "train loss:0.0001032179706874586\n",
      "train loss:0.0035632868401510144\n",
      "train loss:0.003218348218397171\n",
      "train loss:4.104796941062191e-05\n",
      "train loss:0.006627703329753423\n",
      "train loss:0.0036393846217736784\n",
      "train loss:0.0003026765958154385\n",
      "train loss:0.001665484396481045\n",
      "train loss:0.006317291321722276\n",
      "train loss:6.075712932360323e-05\n",
      "train loss:0.0002361468586030806\n",
      "train loss:0.001457065102780941\n",
      "train loss:0.001802885285325677\n",
      "train loss:0.0011944912726049573\n",
      "train loss:0.0018487360394552383\n",
      "train loss:0.0011057200412526319\n",
      "train loss:0.0035701772765898117\n",
      "train loss:0.0002013690843855333\n",
      "train loss:0.0012445535534884508\n",
      "train loss:0.012489389486364086\n",
      "train loss:0.002552069460436598\n",
      "train loss:0.00013820822389726636\n",
      "train loss:0.00037777159768836484\n",
      "train loss:0.0001266832398837294\n",
      "train loss:0.0032050161380472353\n",
      "train loss:0.00250976666695003\n",
      "train loss:0.0007318766431457116\n",
      "train loss:0.005191297243603432\n",
      "train loss:0.004439332461365481\n",
      "train loss:0.00013541648404337924\n",
      "train loss:0.0014110623113405593\n",
      "train loss:0.0012197634087468521\n",
      "train loss:0.0005237373319323255\n",
      "train loss:0.0002591420753337057\n",
      "train loss:0.00019781235169118123\n",
      "train loss:0.0006621900546505371\n",
      "train loss:0.0015252967236542966\n",
      "train loss:9.21583532877499e-05\n",
      "train loss:0.002947368890702891\n",
      "train loss:0.00018448380061618564\n",
      "train loss:0.001535468762831656\n",
      "train loss:5.455775813572034e-05\n",
      "train loss:0.0002966898771721076\n",
      "train loss:6.043433239386255e-05\n",
      "train loss:0.004353894329344207\n",
      "train loss:0.00013014499892566897\n",
      "train loss:0.0028023223196019962\n",
      "train loss:0.005045437396225197\n",
      "train loss:0.00014067032990024758\n",
      "train loss:0.0004680939097480169\n",
      "train loss:0.00013541496238877063\n",
      "train loss:5.201637018418016e-05\n",
      "train loss:0.002164571553510136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.00018209920361393481\n",
      "train loss:0.00030240392634691473\n",
      "train loss:0.00036839168856668173\n",
      "train loss:0.00024068203388646905\n",
      "train loss:0.0014811981946200562\n",
      "train loss:0.0011636684982745374\n",
      "train loss:0.0002208340061013252\n",
      "train loss:0.0006756411044735682\n",
      "train loss:0.0006624831236493158\n",
      "train loss:0.0018188619661902914\n",
      "train loss:0.0013238859957271556\n",
      "train loss:0.00021195790506808212\n",
      "train loss:0.000847749374016016\n",
      "train loss:0.00023834269584871155\n",
      "train loss:0.00020122178276719187\n",
      "train loss:0.00015870225790528126\n",
      "train loss:0.0022227081473388337\n",
      "train loss:0.0010728848880801724\n",
      "train loss:0.0005897775564107295\n",
      "train loss:0.0006952050939584116\n",
      "train loss:0.0002022626487272658\n",
      "train loss:0.0012092436436482701\n",
      "train loss:0.000874598085159597\n",
      "train loss:0.0014620227134829334\n",
      "train loss:0.0004882419613859102\n",
      "train loss:8.463823154332356e-05\n",
      "train loss:9.81644722809972e-05\n",
      "train loss:0.0019312131731551447\n",
      "train loss:0.0005037569535279383\n",
      "train loss:1.1059534091600441e-05\n",
      "train loss:0.0006787714849764109\n",
      "train loss:0.00025691558992302024\n",
      "train loss:0.00213496057308919\n",
      "train loss:6.538770586129508e-05\n",
      "train loss:0.0006402609792875025\n",
      "train loss:0.00021813545799182128\n",
      "train loss:0.0010823324714543876\n",
      "train loss:0.0018476165655982142\n",
      "train loss:2.0769247026677648e-05\n",
      "train loss:0.0011165188017408405\n",
      "train loss:0.0006391886775835066\n",
      "train loss:0.00020378390800036454\n",
      "train loss:0.00019114156253274245\n",
      "train loss:0.0035084034404716624\n",
      "train loss:0.0008294492881493902\n",
      "train loss:0.00024421001917453216\n",
      "train loss:0.000519040386596342\n",
      "train loss:8.019221684753276e-05\n",
      "train loss:7.855741586837466e-05\n",
      "train loss:0.0007487546263287355\n",
      "train loss:5.564331706612943e-05\n",
      "train loss:3.949378504378498e-05\n",
      "train loss:0.0025817171210110713\n",
      "train loss:3.241834346482463e-05\n",
      "train loss:0.0009828210860005186\n",
      "train loss:0.0002811821397605368\n",
      "train loss:0.0007399474797499542\n",
      "train loss:0.001597630562728661\n",
      "train loss:0.000458202847266712\n",
      "train loss:0.07058934021316289\n",
      "train loss:0.001697759025145541\n",
      "train loss:0.0012685302300855475\n",
      "train loss:0.00021306876270939238\n",
      "train loss:0.0008073698628338869\n",
      "train loss:0.0012120756884310723\n",
      "train loss:0.001016793171625048\n",
      "train loss:0.00039747170293614754\n",
      "train loss:0.006950883355573614\n",
      "train loss:0.001028416713406355\n",
      "train loss:0.0009246301534301836\n",
      "train loss:0.002245307252210861\n",
      "train loss:0.0003357137461245324\n",
      "train loss:0.003990334599505447\n",
      "train loss:0.0007069939625038198\n",
      "train loss:0.000863094123578923\n",
      "train loss:0.0002569015861064319\n",
      "train loss:0.001506263781068669\n",
      "train loss:0.00890048582016012\n",
      "train loss:0.00216645996537986\n",
      "train loss:0.004277846635374643\n",
      "train loss:0.0020246834271790168\n",
      "train loss:0.00042293753676269784\n",
      "train loss:0.0010532499313306923\n",
      "train loss:0.0005049838630935291\n",
      "train loss:0.0012568356012604789\n",
      "train loss:0.003705884454610328\n",
      "train loss:0.0043765239769556555\n",
      "train loss:0.0023097799678020595\n",
      "train loss:0.005490091097239698\n",
      "train loss:0.00039149998181898534\n",
      "train loss:0.0009657703668951001\n",
      "train loss:0.00023916400786504633\n",
      "train loss:0.0008251753468587275\n",
      "train loss:0.0005200848531336814\n",
      "train loss:0.0008101358221747845\n",
      "train loss:0.0025615605322991063\n",
      "train loss:0.004038255113171089\n",
      "train loss:0.006259162350325988\n",
      "train loss:0.0005741655523002939\n",
      "train loss:0.0007389001614874842\n",
      "train loss:0.004469490801376268\n",
      "train loss:0.005299403452193124\n",
      "train loss:0.0014431760671735747\n",
      "train loss:5.719489714412307e-05\n",
      "train loss:0.00233011833359376\n",
      "train loss:0.0006106675323502617\n",
      "train loss:0.0007845988415955054\n",
      "train loss:0.003839998617150308\n",
      "train loss:0.0036776717018344412\n",
      "train loss:0.0013335267284557\n",
      "train loss:0.0012041057439168494\n",
      "train loss:1.625993739228115e-05\n",
      "train loss:0.0008564381109906345\n",
      "train loss:0.00025258654944358813\n",
      "train loss:0.017370614473010124\n",
      "train loss:0.0034553698402582742\n",
      "train loss:0.0004542687733809362\n",
      "train loss:0.0028892401205543737\n",
      "train loss:0.0012078084506591611\n",
      "train loss:0.0012137074614729671\n",
      "train loss:1.8170294703795395e-05\n",
      "train loss:0.015468518058191296\n",
      "train loss:0.0005814109054698272\n",
      "train loss:0.0006280061490655837\n",
      "train loss:0.0001457568876277747\n",
      "train loss:6.887469551659219e-05\n",
      "train loss:8.132219945363211e-05\n",
      "train loss:0.00036312029883064375\n",
      "train loss:0.0001723854775859395\n",
      "train loss:0.0003702516914794382\n",
      "train loss:0.000732234732900906\n",
      "train loss:0.0008991792728252399\n",
      "train loss:0.00012166767391711429\n",
      "train loss:0.0016839252906394892\n",
      "train loss:0.00038047959799069643\n",
      "train loss:0.00202986207683919\n",
      "train loss:0.0002780993979808926\n",
      "train loss:0.002350271564505989\n",
      "train loss:0.00221319493644368\n",
      "train loss:0.0009340255956043312\n",
      "train loss:0.001177579789531789\n",
      "train loss:0.00010986756082239431\n",
      "train loss:0.0006738165217966344\n",
      "train loss:0.0013975735804448112\n",
      "train loss:0.0004552275899907754\n",
      "train loss:0.0012208701168463605\n",
      "train loss:0.002173812078615916\n",
      "train loss:0.00021483005246417236\n",
      "train loss:0.005308363828231939\n",
      "train loss:6.18155129800975e-05\n",
      "train loss:0.00012399561593107934\n",
      "train loss:0.0028900044835767336\n",
      "train loss:2.7045659512300345e-05\n",
      "train loss:0.0020095330337236515\n",
      "train loss:0.0005856667200095371\n",
      "train loss:0.0008382027447659908\n",
      "train loss:0.00016728237071990702\n",
      "train loss:0.0011363440082417444\n",
      "train loss:6.983478298828738e-05\n",
      "train loss:0.00018974770593837447\n",
      "train loss:0.0008287761126456323\n",
      "train loss:0.0002924806135116276\n",
      "train loss:0.0007908328832154801\n",
      "train loss:0.0006168997474310206\n",
      "train loss:0.0025248973768118993\n",
      "train loss:0.006219806800047205\n",
      "train loss:0.013422268275881938\n",
      "train loss:0.0003884844147853503\n",
      "train loss:0.00035383456805523146\n",
      "train loss:0.0027731090773071326\n",
      "train loss:0.0005060705209699603\n",
      "train loss:0.00023166365196141715\n",
      "train loss:0.00015928877352062944\n",
      "train loss:0.0007322066701957434\n",
      "train loss:0.00023312535007934582\n",
      "train loss:4.9206762952274386e-05\n",
      "train loss:0.0004931050472832694\n",
      "train loss:0.0010478373450547973\n",
      "train loss:0.00040581239685648905\n",
      "train loss:0.00011886445721140611\n",
      "train loss:0.0018483002929323045\n",
      "train loss:0.011938459625058724\n",
      "train loss:0.0010644938652910106\n",
      "train loss:0.0012638904914953578\n",
      "train loss:0.00015454615763326786\n",
      "train loss:0.0011931070334082665\n",
      "train loss:0.0007908647069779596\n",
      "train loss:0.00016959670746373294\n",
      "train loss:0.0002154127610261542\n",
      "train loss:0.00014871759529066058\n",
      "train loss:0.00026910437460020653\n",
      "train loss:0.00012525240559291975\n",
      "train loss:0.0002682012131198408\n",
      "train loss:0.00017829317147461003\n",
      "train loss:0.0042017191749186495\n",
      "train loss:7.17819219009825e-05\n",
      "train loss:0.0037987947408015875\n",
      "train loss:2.415018111630102e-05\n",
      "train loss:0.00045496052431646386\n",
      "train loss:0.0008213670954341282\n",
      "train loss:0.00039289299031632487\n",
      "train loss:0.04480661660949859\n",
      "train loss:0.0011797430084451643\n",
      "train loss:6.519971987322591e-05\n",
      "train loss:0.0002589060868794667\n",
      "train loss:0.0015312703779290435\n",
      "train loss:0.0006189540125325111\n",
      "train loss:0.0021873081749667596\n",
      "train loss:0.0001231970136940186\n",
      "train loss:0.0011484191951052307\n",
      "train loss:8.91916237714801e-05\n",
      "train loss:4.707649997487011e-05\n",
      "train loss:0.00026505112781956963\n",
      "train loss:9.818715068081823e-05\n",
      "train loss:0.00012294805931505043\n",
      "train loss:0.00010571966054752049\n",
      "train loss:0.0005027016197497786\n",
      "train loss:0.00012091104779865858\n",
      "train loss:0.0012600046025115043\n",
      "train loss:5.334532032886685e-05\n",
      "train loss:0.0011798908428371957\n",
      "train loss:0.000266738007131203\n",
      "train loss:0.00021400930883517442\n",
      "train loss:0.0004704208213004022\n",
      "train loss:0.00023967655704306665\n",
      "train loss:0.0013223226921646044\n",
      "train loss:0.001856034271122157\n",
      "train loss:0.0012496473437455242\n",
      "train loss:0.0003551403579080466\n",
      "train loss:0.00046209740588777415\n",
      "train loss:0.0010115098262867426\n",
      "train loss:0.0010387006806447194\n",
      "train loss:0.0012946158549609079\n",
      "train loss:0.00023715284074072568\n",
      "train loss:5.129769663682314e-05\n",
      "train loss:0.028347454165735267\n",
      "train loss:0.00022573486696871446\n",
      "train loss:9.464493084645335e-05\n",
      "train loss:0.00034305787082363826\n",
      "train loss:0.00048107347381185156\n",
      "train loss:0.00023339878580044384\n",
      "train loss:0.00033313780707911226\n",
      "train loss:0.0007042368404064412\n",
      "train loss:0.0019417206370950981\n",
      "train loss:0.0013166029923469993\n",
      "train loss:0.0002458554645557061\n",
      "train loss:0.005811651132366657\n",
      "train loss:9.660657501330173e-05\n",
      "train loss:0.002983561189487744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.00024430013739231945\n",
      "train loss:0.00040077442448312574\n",
      "train loss:3.2543098176336896e-05\n",
      "train loss:0.0006711571655981628\n",
      "train loss:0.0005829147913785299\n",
      "train loss:0.00010148077221441662\n",
      "train loss:3.1557633719087084e-05\n",
      "train loss:1.4278823435030996e-05\n",
      "train loss:0.00032973603506784356\n",
      "train loss:2.8116414315660884e-05\n",
      "train loss:0.0006462110368546551\n",
      "train loss:0.00035486156253440154\n",
      "train loss:0.0011456206611375414\n",
      "train loss:0.0019231464252006778\n",
      "train loss:0.0007119112127618295\n",
      "train loss:0.00043516424931198554\n",
      "train loss:0.0056298560819317235\n",
      "train loss:0.00020518308051226382\n",
      "train loss:0.0004925508290018215\n",
      "train loss:0.001299636183727183\n",
      "train loss:0.00013498535339547994\n",
      "train loss:0.0011442754244495345\n",
      "train loss:0.00021174025171761938\n",
      "train loss:0.00021622853767207584\n",
      "train loss:4.368320561457778e-05\n",
      "train loss:0.0006695529210470375\n",
      "train loss:0.00020309805088965797\n",
      "train loss:0.00015309487018746738\n",
      "train loss:0.0008788531751689957\n",
      "train loss:0.0004554111917064786\n",
      "train loss:0.0006527457824717786\n",
      "train loss:0.00022643817307511688\n",
      "train loss:0.006266845297163256\n",
      "train loss:0.0004017494697597242\n",
      "train loss:0.003963329916423246\n",
      "train loss:0.00034464416591099687\n",
      "train loss:0.0007661654812099016\n",
      "train loss:7.023050526292649e-05\n",
      "train loss:0.00029508611159643805\n",
      "train loss:0.008758544329012295\n",
      "train loss:0.00012896467143247935\n",
      "train loss:0.00019889036340012454\n",
      "train loss:0.00028135452775851185\n",
      "train loss:0.0009676348412558905\n",
      "train loss:0.001644416465414068\n",
      "train loss:7.89063054336142e-05\n",
      "train loss:0.00042613045876728115\n",
      "train loss:0.0010379692834858569\n",
      "train loss:6.175932533576537e-05\n",
      "train loss:4.149099038874503e-05\n",
      "train loss:1.3904446952597355e-05\n",
      "train loss:8.521129515173981e-05\n",
      "train loss:4.32825849628072e-05\n",
      "train loss:2.853250103387636e-05\n",
      "train loss:0.00021836792225326772\n",
      "train loss:0.0005564823470299124\n",
      "train loss:0.00010830598356963065\n",
      "train loss:0.00016333104248910076\n",
      "train loss:0.0006956261074617946\n",
      "train loss:0.0006990818342062102\n",
      "train loss:0.008461119420658048\n",
      "train loss:0.009883343785062079\n",
      "train loss:0.00012238067763933213\n",
      "train loss:6.714067215536305e-05\n",
      "train loss:0.012276416629188332\n",
      "train loss:0.0007149398346892271\n",
      "train loss:2.4185573965940385e-05\n",
      "train loss:0.0009668786928675336\n",
      "train loss:5.409859954770419e-05\n",
      "train loss:0.0006192245646001724\n",
      "train loss:0.00044876918400343235\n",
      "train loss:0.002311938023981509\n",
      "train loss:0.00011735587527675645\n",
      "train loss:0.0004218900443110045\n",
      "train loss:0.0865499859522714\n",
      "train loss:0.00023985957274833523\n",
      "train loss:0.002262621770524451\n",
      "train loss:0.002067621850865758\n",
      "train loss:0.00044601369006770967\n",
      "train loss:0.0005389518205492576\n",
      "train loss:0.00047488651506752655\n",
      "train loss:0.0006504364196824549\n",
      "train loss:0.00010305399595515045\n",
      "train loss:0.00023456217243996133\n",
      "train loss:0.0001591817831120044\n",
      "train loss:0.0019237468744479423\n",
      "train loss:0.0005726962184940056\n",
      "train loss:0.00046706817904980825\n",
      "train loss:2.3110369142760942e-05\n",
      "train loss:0.0014219465903263222\n",
      "train loss:0.0008391177427151893\n",
      "train loss:0.00029073645126860196\n",
      "train loss:3.376468599561226e-05\n",
      "train loss:0.0005115807293863007\n",
      "train loss:0.004867753216124065\n",
      "train loss:0.00035147360916132353\n",
      "train loss:0.0027018421583412332\n",
      "train loss:0.012008871599386845\n",
      "train loss:0.0024353314008402054\n",
      "train loss:0.000389060831273214\n",
      "train loss:0.0001380098224667011\n",
      "train loss:0.0001638693803975453\n",
      "train loss:0.0017668648419345181\n",
      "train loss:0.0021080239406956335\n",
      "train loss:0.000659957363489265\n",
      "train loss:8.339481705024137e-05\n",
      "train loss:0.00041907921022855016\n",
      "train loss:0.030842113361160984\n",
      "train loss:0.0033918358564452183\n",
      "train loss:0.0003188688718448702\n",
      "train loss:0.0009267548384935596\n",
      "train loss:0.0020506957786797235\n",
      "train loss:0.0005992731017312505\n",
      "train loss:9.029435946772227e-05\n",
      "train loss:8.821011974806591e-05\n",
      "train loss:0.00042083539776138594\n",
      "train loss:0.0002599119232700769\n",
      "train loss:0.0005374347748809236\n",
      "train loss:0.0017600366785696414\n",
      "train loss:0.00329847029670168\n",
      "train loss:4.2432570847662566e-05\n",
      "train loss:0.00048653684319315617\n",
      "train loss:2.5329165874972735e-05\n",
      "train loss:0.0002608557250720742\n",
      "train loss:0.00035267463617384427\n",
      "train loss:0.00028648807437923806\n",
      "train loss:0.0004598361098680219\n",
      "train loss:0.000494271443806494\n",
      "train loss:0.01524875214879614\n",
      "train loss:0.0003738523799185068\n",
      "train loss:0.003330767004564492\n",
      "train loss:0.0006713178066610475\n",
      "train loss:0.0029049801343336073\n",
      "train loss:0.00012115926504268507\n",
      "train loss:1.5072656863253347e-05\n",
      "train loss:0.0012137107838113418\n",
      "train loss:0.00019470139513614676\n",
      "train loss:0.00013795077439908947\n",
      "train loss:0.0002050311793653679\n",
      "train loss:0.0030017691335419523\n",
      "train loss:0.00033011452783472553\n",
      "train loss:0.007271611243206725\n",
      "train loss:0.0008346631384846553\n",
      "train loss:0.0016958481028175349\n",
      "train loss:0.0008822781545717952\n",
      "train loss:0.0022616982952401122\n",
      "train loss:0.026412536990208596\n",
      "train loss:0.00030878304962866465\n",
      "train loss:0.0009213410811260977\n",
      "train loss:0.00014897844410463381\n",
      "train loss:0.0007327344612730766\n",
      "train loss:0.0006746390816105689\n",
      "train loss:0.001399380001569713\n",
      "train loss:0.0009223755487647639\n",
      "train loss:0.0005407402018803927\n",
      "train loss:0.0015539728882744811\n",
      "train loss:8.595689422115715e-05\n",
      "train loss:9.719140807056978e-06\n",
      "train loss:0.00015536493238250094\n",
      "train loss:0.002545685998290972\n",
      "train loss:0.0014587594314534446\n",
      "train loss:0.0003580149377619174\n",
      "train loss:0.0011797221766562107\n",
      "train loss:0.00021605727889335575\n",
      "train loss:0.0004305697890890151\n",
      "train loss:0.001125531897863052\n",
      "train loss:0.0030752651771676744\n",
      "train loss:0.00952600132981629\n",
      "train loss:0.0004189490931229456\n",
      "train loss:0.0007850408053441973\n",
      "train loss:0.0001156428280809268\n",
      "train loss:3.734097488942192e-05\n",
      "train loss:0.000533218611989729\n",
      "train loss:0.002646658780970832\n",
      "train loss:0.00012883472714101025\n",
      "train loss:0.004654720743704524\n",
      "train loss:0.002808257688439867\n",
      "train loss:0.0011989370597545836\n",
      "train loss:0.0075418960641779895\n",
      "train loss:0.00020055733060244597\n",
      "train loss:7.158064340491958e-05\n",
      "train loss:9.754400176816382e-05\n",
      "train loss:0.0007876293830275169\n",
      "train loss:0.00044620844010860973\n",
      "train loss:0.0026542544280448773\n",
      "train loss:0.000208818142814254\n",
      "train loss:0.0011891755776449805\n",
      "train loss:0.003335833559235528\n",
      "train loss:0.009071979073569882\n",
      "train loss:7.551388393276685e-05\n",
      "train loss:0.00011997110236626337\n",
      "train loss:8.721166909401956e-05\n",
      "train loss:0.0012430103990565859\n",
      "train loss:5.187036209928743e-05\n",
      "train loss:0.0009057595192681034\n",
      "train loss:0.0007944018985360655\n",
      "train loss:0.0009599180996414869\n",
      "train loss:4.721173682870752e-05\n",
      "train loss:0.0003880493590006552\n",
      "train loss:0.00024972120148357527\n",
      "train loss:0.0004980124526485249\n",
      "train loss:0.0013517366967204045\n",
      "train loss:0.0004182445158504528\n",
      "train loss:0.0008604168796615352\n",
      "train loss:9.957253696557364e-05\n",
      "train loss:0.00022434629779502702\n",
      "train loss:0.0015063399612353708\n",
      "train loss:0.002080871441505432\n",
      "train loss:0.0009985034013980656\n",
      "train loss:1.762235199845643e-05\n",
      "train loss:6.507949162620752e-05\n",
      "train loss:0.000616154531118244\n",
      "train loss:0.0016741828338570127\n",
      "train loss:0.0566277132650563\n",
      "train loss:0.00122513131277657\n",
      "train loss:7.185267037277505e-05\n",
      "train loss:0.00016443846984434004\n",
      "train loss:1.4507448013092379e-05\n",
      "train loss:0.0001444785843507219\n",
      "train loss:0.001373969531456361\n",
      "train loss:0.03133424634686969\n",
      "train loss:0.002777114892552673\n",
      "train loss:0.00040670799327530073\n",
      "train loss:0.0033568676339004776\n",
      "train loss:0.00024994001254171597\n",
      "train loss:0.0005157374313185593\n",
      "train loss:0.001112241020682124\n",
      "train loss:0.0006939290042961134\n",
      "train loss:0.0013456017603463736\n",
      "train loss:0.0010004872166859623\n",
      "train loss:0.0006590808539768914\n",
      "train loss:0.0011058302405728916\n",
      "train loss:2.2356439457210125e-05\n",
      "train loss:0.00014243616802403613\n",
      "train loss:0.00026876333746947477\n",
      "train loss:7.099082817775373e-05\n",
      "train loss:0.0011646113257830102\n",
      "train loss:0.00074564532178982\n",
      "train loss:0.0026299067372522926\n",
      "train loss:0.00011136773566344054\n",
      "train loss:0.00020700926917304228\n",
      "train loss:7.012726616711237e-05\n",
      "train loss:0.0006375410862268033\n",
      "train loss:0.001048992413724321\n",
      "train loss:0.0003382418951698912\n",
      "train loss:0.0016493757661463634\n",
      "train loss:0.00016212471670067513\n",
      "train loss:0.0007614365569464386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.001756607288450912\n",
      "train loss:0.0003090692167194445\n",
      "train loss:0.004004779063588611\n",
      "=== epoch:18, train acc:1.0, test acc:0.99 ===\n",
      "train loss:4.256367445193061e-05\n",
      "train loss:0.0005175805484385934\n",
      "train loss:0.0006038777294493683\n",
      "train loss:0.00018366209845512005\n",
      "train loss:0.00045953555055360743\n",
      "train loss:0.0005256262631215106\n",
      "train loss:0.0019450045704252114\n",
      "train loss:0.00038778993540540486\n",
      "train loss:1.4800404116900244e-05\n",
      "train loss:0.002760500385116368\n",
      "train loss:0.0010915649144597579\n",
      "train loss:0.001109169517277246\n",
      "train loss:0.0008697378230665911\n",
      "train loss:0.000355371788890593\n",
      "train loss:0.0009099374838812397\n",
      "train loss:0.0004333953797402449\n",
      "train loss:0.0007705267574142975\n",
      "train loss:0.0005443423135594699\n",
      "train loss:0.00015151263442944187\n",
      "train loss:0.00011134661691416225\n",
      "train loss:0.0016714585007403416\n",
      "train loss:0.00030454517647545505\n",
      "train loss:0.0006306091521788754\n",
      "train loss:0.00010704590710466783\n",
      "train loss:0.0029252701917796527\n",
      "train loss:0.0013163282528664457\n",
      "train loss:0.0031038215802115156\n",
      "train loss:0.0002148841204932132\n",
      "train loss:0.0025309518785404618\n",
      "train loss:0.0009389643789483788\n",
      "train loss:0.000611814627636391\n",
      "train loss:0.000549875388899057\n",
      "train loss:0.0005711982987461732\n",
      "train loss:5.3508939908176405e-05\n",
      "train loss:0.0016302109368580278\n",
      "train loss:0.0006562862454653879\n",
      "train loss:5.468389159506399e-05\n",
      "train loss:0.0008647576413155923\n",
      "train loss:0.006282322692670601\n",
      "train loss:0.0007623100863364645\n",
      "train loss:0.0017176528088652416\n",
      "train loss:0.00013636700014955098\n",
      "train loss:0.000460016133764227\n",
      "train loss:0.0003435602417555315\n",
      "train loss:0.0012208354859557134\n",
      "train loss:0.00099618920479014\n",
      "train loss:0.0002847096534318678\n",
      "train loss:0.0010694531115445303\n",
      "train loss:0.0006723482358093926\n",
      "train loss:0.00024506261560811325\n",
      "train loss:0.0002205620171212903\n",
      "train loss:8.112465154068492e-05\n",
      "train loss:5.914225308029381e-05\n",
      "train loss:0.0001517338350659026\n",
      "train loss:3.7464640158605384e-05\n",
      "train loss:0.0014053453777786973\n",
      "train loss:0.0016936754055180067\n",
      "train loss:0.0009757328975146966\n",
      "train loss:0.0009911301309522467\n",
      "train loss:0.0012838503128650394\n",
      "train loss:8.848292260495036e-05\n",
      "train loss:0.001980085309887078\n",
      "train loss:0.0004122752323594741\n",
      "train loss:0.0008639646200357363\n",
      "train loss:0.0011350617407071056\n",
      "train loss:6.855255821374278e-05\n",
      "train loss:0.0011315201036601833\n",
      "train loss:0.0011174084135869122\n",
      "train loss:0.0007089372674838877\n",
      "train loss:0.0003116115642207967\n",
      "train loss:2.158000024916153e-05\n",
      "train loss:0.00014746970371647838\n",
      "train loss:0.0003015933255762412\n",
      "train loss:0.000654823044033599\n",
      "train loss:0.0005656025828329255\n",
      "train loss:0.0003173747578611156\n",
      "train loss:9.5792426754191e-05\n",
      "train loss:9.116343404411873e-05\n",
      "train loss:0.0003810632347199968\n",
      "train loss:0.0012592049081067743\n",
      "train loss:0.0005633257807150047\n",
      "train loss:3.739407850473632e-05\n",
      "train loss:0.0008468036315309422\n",
      "train loss:0.004313765856257099\n",
      "train loss:0.0003778838795863006\n",
      "train loss:0.003594896100645471\n",
      "train loss:0.000499570806142549\n",
      "train loss:0.00013539363250712164\n",
      "train loss:0.0002272730964428477\n",
      "train loss:0.00019850456736222245\n",
      "train loss:0.00011693847830943561\n",
      "train loss:0.0009332313220125539\n",
      "train loss:0.0022498373039481798\n",
      "train loss:0.0037220216330146345\n",
      "train loss:0.0010346164569829794\n",
      "train loss:0.00012395922604582652\n",
      "train loss:9.459978771973729e-05\n",
      "train loss:0.0074217473018766975\n",
      "train loss:0.0007932975229670676\n",
      "train loss:8.8866290014021e-05\n",
      "train loss:0.0015036424070011003\n",
      "train loss:0.0002453571447244326\n",
      "train loss:0.0001693539377972137\n",
      "train loss:0.00014399154549510058\n",
      "train loss:8.715027021765476e-05\n",
      "train loss:0.0009445726439840809\n",
      "train loss:0.00012142271073214995\n",
      "train loss:0.000601715090646365\n",
      "train loss:0.0013644726665521176\n",
      "train loss:0.0008436632229488116\n",
      "train loss:0.001961782059242184\n",
      "train loss:0.0005778426897972608\n",
      "train loss:0.0021160361015603903\n",
      "train loss:0.0017351697272951494\n",
      "train loss:0.00022722351476994946\n",
      "train loss:0.0003019271311306385\n",
      "train loss:7.938560895065391e-05\n",
      "train loss:0.0010251656840434631\n",
      "train loss:0.23376844121845725\n",
      "train loss:4.437646994336436e-05\n",
      "train loss:8.694067876079917e-05\n",
      "train loss:0.0012929940237814783\n",
      "train loss:0.0003655889016288711\n",
      "train loss:0.00014824539013497302\n",
      "train loss:4.3812025463609154e-05\n",
      "train loss:0.001263750773778939\n",
      "train loss:4.825993020510752e-06\n",
      "train loss:0.0004927841154517616\n",
      "train loss:0.0012533462660980351\n",
      "train loss:0.0001773247010753703\n",
      "train loss:0.0008412260126284671\n",
      "train loss:0.0006780420956437978\n",
      "train loss:0.0024324449912002622\n",
      "train loss:0.00036910270080623043\n",
      "train loss:0.0006728394762860304\n",
      "train loss:0.0003822670516350185\n",
      "train loss:0.0017765106227720056\n",
      "train loss:0.0013617126942375788\n",
      "train loss:0.00023795345217105096\n",
      "train loss:0.00014113971214250114\n",
      "train loss:0.0011451369361232787\n",
      "train loss:0.0006615152712722541\n",
      "train loss:0.002035900173825538\n",
      "train loss:0.0005967316704324498\n",
      "train loss:0.0004588683236057326\n",
      "train loss:5.90581860659812e-05\n",
      "train loss:0.00048706230246151674\n",
      "train loss:0.00477842792891655\n",
      "train loss:0.002584259425824627\n",
      "train loss:2.8975027487831916e-05\n",
      "train loss:0.00024207859725068788\n",
      "train loss:0.0002167473959143354\n",
      "train loss:0.004153867758030718\n",
      "train loss:0.0013125489450128342\n",
      "train loss:0.00039057439833200896\n",
      "train loss:0.0014281029249283387\n",
      "train loss:0.0014703189403244066\n",
      "train loss:0.0010399529803929526\n",
      "train loss:0.0013206256683316985\n",
      "train loss:0.00011980523589763673\n",
      "train loss:8.98078183179014e-05\n",
      "train loss:0.0019529569092343504\n",
      "train loss:0.00017550967198368808\n",
      "train loss:9.912634744335469e-05\n",
      "train loss:1.4062607742245253e-05\n",
      "train loss:0.00018674524226810324\n",
      "train loss:0.00023844399412157992\n",
      "train loss:0.00024233415936811663\n",
      "train loss:1.937433244948571e-05\n",
      "train loss:4.77958285611895e-05\n",
      "train loss:6.0534996249978144e-05\n",
      "train loss:0.0012127730322587574\n",
      "train loss:0.0005569007969075337\n",
      "train loss:0.0008657680543353952\n",
      "train loss:0.00013545528355696915\n",
      "train loss:2.6786629661207547e-05\n",
      "train loss:0.00374020345787206\n",
      "train loss:0.010481776544304539\n",
      "train loss:0.00010593980540526066\n",
      "train loss:0.00011634992923411631\n",
      "train loss:0.0006025242207298593\n",
      "train loss:0.00253107485638576\n",
      "train loss:0.0006391774274222771\n",
      "train loss:7.233086215227615e-05\n",
      "train loss:2.9280173583549665e-05\n",
      "train loss:0.0005823795089269497\n",
      "train loss:0.0002733871196866448\n",
      "train loss:0.0008953830448112707\n",
      "train loss:3.8988006294673936e-05\n",
      "train loss:0.0005135786747084606\n",
      "train loss:0.0001487594768242721\n",
      "train loss:6.315507287129063e-05\n",
      "train loss:8.20376182017622e-05\n",
      "train loss:0.008056997506921022\n",
      "train loss:0.0002791542026047072\n",
      "train loss:0.00076539924467946\n",
      "train loss:9.941530265963287e-05\n",
      "train loss:0.0036914422264677682\n",
      "train loss:0.002980948560646395\n",
      "train loss:0.0007468845787173322\n",
      "train loss:0.00040564431734956745\n",
      "train loss:0.0018312508644258698\n",
      "train loss:0.0005996704452187289\n",
      "train loss:0.003403458563088391\n",
      "train loss:0.0019037763372599186\n",
      "train loss:0.0006592567015481763\n",
      "train loss:0.0013314981550112536\n",
      "train loss:0.0007004523315103979\n",
      "train loss:0.0012154911969535767\n",
      "train loss:0.021305135361264264\n",
      "train loss:0.0005693027225641038\n",
      "train loss:3.912999270207276e-05\n",
      "train loss:9.547562562378065e-05\n",
      "train loss:0.0008129263151141965\n",
      "train loss:0.0011375933873922614\n",
      "train loss:0.0005619789776502871\n",
      "train loss:9.574937367193894e-05\n",
      "train loss:4.293384899368756e-05\n",
      "train loss:0.0026114231282689778\n",
      "train loss:0.0007615658490460623\n",
      "train loss:0.0004524968030563841\n",
      "train loss:0.00044754505216321053\n",
      "train loss:0.0007838082972433275\n",
      "train loss:0.03649102961794885\n",
      "train loss:0.008691580729313435\n",
      "train loss:0.0014170120043224294\n",
      "train loss:0.00023297743190722823\n",
      "train loss:4.77993279629287e-06\n",
      "train loss:0.0017968258600942458\n",
      "train loss:9.996193843332484e-05\n",
      "train loss:0.0017417036367783393\n",
      "train loss:0.00033480033946247046\n",
      "train loss:6.298073323324434e-05\n",
      "train loss:0.0012750872171863957\n",
      "train loss:0.002994354408402473\n",
      "train loss:0.0010165769594396895\n",
      "train loss:0.0025204885320247662\n",
      "train loss:5.6377570562013786e-05\n",
      "train loss:9.153491119900265e-05\n",
      "train loss:9.020766815444936e-05\n",
      "train loss:0.00013794620879855365\n",
      "train loss:0.00014254875773761668\n",
      "train loss:0.0009301438255704282\n",
      "train loss:0.0002607441506454502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0005092079333847497\n",
      "train loss:0.0006368693803787621\n",
      "train loss:0.0004420649432044792\n",
      "train loss:3.917292025644206e-05\n",
      "train loss:0.0035248999790767482\n",
      "train loss:0.0021001315193084065\n",
      "train loss:0.0004579292441961145\n",
      "train loss:0.00022717775997150666\n",
      "train loss:0.0001489505382273504\n",
      "train loss:0.0002663094912935045\n",
      "train loss:0.0008682696493864323\n",
      "train loss:0.0004750530280783557\n",
      "train loss:5.8981953657202174e-05\n",
      "train loss:0.0002476373310751363\n",
      "train loss:0.0007918317851494627\n",
      "train loss:0.00042733087569021903\n",
      "train loss:4.443312940127471e-05\n",
      "train loss:0.0032811600418668364\n",
      "train loss:0.0008396373961142306\n",
      "train loss:5.8620131433119655e-05\n",
      "train loss:0.0001385070612973171\n",
      "train loss:0.0010614071059702533\n",
      "train loss:0.0007835892814141681\n",
      "train loss:0.00018719610805330078\n",
      "train loss:0.0013596135322591727\n",
      "train loss:0.00032143393300605163\n",
      "train loss:0.00016858342461240097\n",
      "train loss:0.004375318649508126\n",
      "train loss:0.0015165548154921114\n",
      "train loss:0.0005762907684744589\n",
      "train loss:0.001662763125001727\n",
      "train loss:0.00016887284879926737\n",
      "train loss:0.002324706829239717\n",
      "train loss:0.00013174964616943853\n",
      "train loss:2.8923852381377226e-05\n",
      "train loss:9.523617121886277e-05\n",
      "train loss:3.4247302325375495e-05\n",
      "train loss:0.00040056269929168107\n",
      "train loss:0.000965558778440128\n",
      "train loss:0.0008846390696877126\n",
      "train loss:0.00042537864388605945\n",
      "train loss:5.41252746235484e-05\n",
      "train loss:0.0005513910528567771\n",
      "train loss:5.3838767823152615e-05\n",
      "train loss:0.001973547580696949\n",
      "train loss:0.013966543520262583\n",
      "train loss:0.0005367124513240791\n",
      "train loss:0.0009772449037052028\n",
      "train loss:0.0003026665159628361\n",
      "train loss:0.0003741465015168671\n",
      "train loss:0.001575142790817422\n",
      "train loss:0.0013935635823209926\n",
      "train loss:0.00012210422953185147\n",
      "train loss:0.0004533067219787332\n",
      "train loss:0.0013954373681339424\n",
      "train loss:0.00014691839835836133\n",
      "train loss:0.00022773266691658377\n",
      "train loss:0.0003500796889970688\n",
      "train loss:0.00012933795246901646\n",
      "train loss:0.0005886030474150199\n",
      "train loss:0.0009414656954548703\n",
      "train loss:2.7199038852732905e-05\n",
      "train loss:0.00012002161438837692\n",
      "train loss:1.9775493300282766e-05\n",
      "train loss:0.00022420412135309604\n",
      "train loss:0.00048089252586295704\n",
      "train loss:5.058254549505454e-05\n",
      "train loss:0.00014868450446086482\n",
      "train loss:6.79998291861178e-05\n",
      "train loss:0.0005573969848960289\n",
      "train loss:0.0001876399155894724\n",
      "train loss:0.001516025806551133\n",
      "train loss:0.0027588293720767964\n",
      "train loss:0.003406321647129348\n",
      "train loss:4.9480291983586974e-05\n",
      "train loss:7.2471842020692194e-06\n",
      "train loss:0.0017885682004768091\n",
      "train loss:0.0007841550299890965\n",
      "train loss:9.123311314879544e-05\n",
      "train loss:0.00024644392151149287\n",
      "train loss:0.00044777959215826154\n",
      "train loss:1.2626638027543142e-05\n",
      "train loss:0.0007363050518082045\n",
      "train loss:0.00028146136970220764\n",
      "train loss:0.0002003638308365379\n",
      "train loss:1.4157381998043986e-05\n",
      "train loss:5.676121174086885e-05\n",
      "train loss:2.8374947705783227e-05\n",
      "train loss:0.0001036605535714443\n",
      "train loss:5.398755770178443e-05\n",
      "train loss:0.00031345098564647346\n",
      "train loss:5.975714862652549e-05\n",
      "train loss:9.96307920188169e-05\n",
      "train loss:0.0002237557978563707\n",
      "train loss:0.00030631355866788594\n",
      "train loss:0.00010673305971732867\n",
      "train loss:0.00012148179117450831\n",
      "train loss:0.00018839247624631233\n",
      "train loss:0.00013929011169465144\n",
      "train loss:0.00011622209940703709\n",
      "train loss:7.155574381796875e-05\n",
      "train loss:4.39950677815817e-05\n",
      "train loss:0.0003407575577590892\n",
      "train loss:7.990493428169968e-05\n",
      "train loss:6.42223464978465e-05\n",
      "train loss:0.0013349702757278006\n",
      "train loss:0.0002597843493432641\n",
      "train loss:0.00045450473708307437\n",
      "train loss:0.00021604640898517446\n",
      "train loss:0.000862785300107361\n",
      "train loss:0.0005040811585394102\n",
      "train loss:0.0006714695818776645\n",
      "train loss:7.807912192219406e-05\n",
      "train loss:0.0008490738558109691\n",
      "train loss:0.0013244252161247196\n",
      "train loss:3.445775893080614e-05\n",
      "train loss:0.0012051922524233078\n",
      "train loss:0.00032858538197998616\n",
      "train loss:0.0001261218297940197\n",
      "train loss:0.000237144880847625\n",
      "train loss:8.584810927094929e-06\n",
      "train loss:0.0029606041255125855\n",
      "train loss:0.00015330357021035827\n",
      "train loss:0.0002912656199816135\n",
      "train loss:0.0026991722301086337\n",
      "train loss:0.002410639301640794\n",
      "train loss:0.0009163515676702008\n",
      "train loss:0.007974161531729035\n",
      "train loss:0.0012949787617769321\n",
      "train loss:0.0003064083720686269\n",
      "train loss:0.0002647176453646016\n",
      "train loss:9.760317627787131e-05\n",
      "train loss:0.003526519701718298\n",
      "train loss:0.0013636290426278209\n",
      "train loss:0.000681856322718978\n",
      "train loss:4.559254464759917e-05\n",
      "train loss:0.0001356966004137375\n",
      "train loss:0.00030031952132227973\n",
      "train loss:0.0008807181940399969\n",
      "train loss:0.0002529104650836805\n",
      "train loss:0.0018181945310977992\n",
      "train loss:0.001633232954785736\n",
      "train loss:9.915256230309439e-05\n",
      "train loss:0.00011168223002436943\n",
      "train loss:0.0002647690298097712\n",
      "train loss:0.0001995995609030718\n",
      "train loss:0.0005888218724000201\n",
      "train loss:0.0004903016999520203\n",
      "train loss:5.001937749767607e-05\n",
      "train loss:3.768301040763002e-05\n",
      "train loss:0.00042930992866499354\n",
      "train loss:0.003941190999441458\n",
      "train loss:5.485975939908675e-05\n",
      "train loss:3.362061305779862e-05\n",
      "train loss:0.000751804584002614\n",
      "train loss:5.087245158196263e-05\n",
      "train loss:0.00014262402734358223\n",
      "train loss:6.578148745979033e-06\n",
      "train loss:0.00047436112304199\n",
      "train loss:0.00013033239723491278\n",
      "train loss:0.00023962037237677616\n",
      "train loss:7.64962063241598e-05\n",
      "train loss:0.00014070309259736326\n",
      "train loss:0.0006495897506253943\n",
      "train loss:0.0003363712107923121\n",
      "train loss:0.00036853615788210933\n",
      "train loss:0.0002611904053291099\n",
      "train loss:0.0001596873677804449\n",
      "train loss:0.0002204951474108868\n",
      "train loss:0.00032450676194586096\n",
      "train loss:6.455180631292733e-05\n",
      "train loss:0.0004641994890322375\n",
      "train loss:0.0004536223512925803\n",
      "train loss:6.141218286321872e-05\n",
      "train loss:8.916140756604569e-06\n",
      "train loss:0.00023874252226955127\n",
      "train loss:2.5022485030873763e-05\n",
      "train loss:0.0011898206087346181\n",
      "train loss:5.133110038932006e-05\n",
      "train loss:0.0007175590293656023\n",
      "train loss:0.0006071004208718477\n",
      "train loss:0.0011426656982374294\n",
      "train loss:4.330778748192616e-05\n",
      "train loss:3.73468178323588e-05\n",
      "train loss:2.3715818075254764e-05\n",
      "train loss:0.00024711619593520873\n",
      "train loss:0.0006377795744532525\n",
      "train loss:1.1660931151807866e-05\n",
      "train loss:0.0007620102581018484\n",
      "train loss:0.00035267772521294544\n",
      "train loss:1.0181494399339825e-05\n",
      "train loss:6.886235455312687e-05\n",
      "train loss:0.00012385575080434664\n",
      "train loss:0.0007202513239798228\n",
      "train loss:0.0008378787965579705\n",
      "train loss:0.00020226748728935297\n",
      "train loss:0.0002852095072447498\n",
      "train loss:0.00047318754502601306\n",
      "train loss:0.00018528338640976357\n",
      "train loss:2.548038657347049e-05\n",
      "train loss:0.0003282109168871139\n",
      "train loss:0.00023383482022882742\n",
      "train loss:5.754571229889233e-05\n",
      "train loss:0.0003876205137178678\n",
      "train loss:4.5685938489135235e-05\n",
      "train loss:0.0004647643595862216\n",
      "train loss:0.0004075372371534454\n",
      "train loss:2.183922163189092e-06\n",
      "train loss:4.782212117094084e-05\n",
      "train loss:0.0007101395285551436\n",
      "train loss:0.00018160579611376708\n",
      "train loss:7.131373068605941e-06\n",
      "train loss:0.00013419563272246647\n",
      "train loss:0.00013147524399940636\n",
      "train loss:0.0006411000911793444\n",
      "train loss:2.2389503361692445e-05\n",
      "train loss:0.0003028212889543207\n",
      "train loss:5.9602741022346935e-05\n",
      "train loss:0.0009247061473444885\n",
      "train loss:0.00014934186293497735\n",
      "train loss:6.044602559114028e-05\n",
      "train loss:2.351541032713442e-05\n",
      "train loss:8.083200343068107e-05\n",
      "train loss:0.0007946112473841951\n",
      "train loss:0.0007005702474467066\n",
      "train loss:1.7819648984758613e-05\n",
      "train loss:0.0003662078508567048\n",
      "train loss:4.16129239423244e-05\n",
      "train loss:4.2989657111529006e-05\n",
      "train loss:0.0006048999341670218\n",
      "train loss:0.0002525162224901118\n",
      "train loss:0.0005016374511026511\n",
      "train loss:4.509168578678035e-05\n",
      "train loss:0.00012210838119614418\n",
      "train loss:0.0003994772186802086\n",
      "train loss:0.0002747363730799911\n",
      "train loss:0.0007242991480396657\n",
      "train loss:0.00044208895068424014\n",
      "train loss:0.00011980915659045259\n",
      "train loss:0.00010100955846443578\n",
      "train loss:0.00013814868019304386\n",
      "train loss:0.0013641974612595292\n",
      "train loss:0.0069421413750771896\n",
      "train loss:0.00046111858645043476\n",
      "train loss:0.0005567217818072464\n",
      "train loss:2.179707425975968e-05\n",
      "train loss:0.0003203394539510165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:4.009529781387882e-05\n",
      "train loss:0.0063962586782430695\n",
      "train loss:0.0010859602975080971\n",
      "train loss:0.0007940874833791987\n",
      "train loss:0.006488948483328657\n",
      "train loss:9.246262523285987e-05\n",
      "train loss:0.00044897719578741545\n",
      "train loss:0.014090760932465586\n",
      "train loss:8.870731452566791e-05\n",
      "train loss:0.0007163057644345755\n",
      "train loss:0.0009072499104384312\n",
      "train loss:0.001698120642901956\n",
      "train loss:0.004198145745453529\n",
      "train loss:9.192030510112981e-05\n",
      "train loss:6.705833881331755e-05\n",
      "train loss:0.0003410492577170062\n",
      "train loss:1.699693152249775e-05\n",
      "train loss:6.557380648737508e-05\n",
      "train loss:0.0005483318063853194\n",
      "train loss:0.0030493730617053142\n",
      "train loss:0.0030406035935962687\n",
      "train loss:0.0008820322281957914\n",
      "train loss:3.031755255569075e-05\n",
      "train loss:0.0012180995128658228\n",
      "train loss:0.00017158112936550577\n",
      "train loss:8.814940779769135e-05\n",
      "train loss:1.1315642107292165e-05\n",
      "train loss:0.0004200259145634969\n",
      "train loss:0.004865125374612536\n",
      "train loss:0.0006441985208319773\n",
      "train loss:0.0006769697663723425\n",
      "train loss:0.0015837509045650475\n",
      "train loss:0.00014923437986488387\n",
      "train loss:0.00016399468083678353\n",
      "train loss:0.0006357817600544348\n",
      "train loss:0.004018142110411627\n",
      "train loss:0.0037243623742658173\n",
      "train loss:2.3906537998368912e-05\n",
      "train loss:0.00018941297282211667\n",
      "train loss:4.151438069444102e-05\n",
      "train loss:0.0003882026379005705\n",
      "train loss:0.00010480213407630933\n",
      "train loss:0.0007118623094486856\n",
      "train loss:0.002760730958851298\n",
      "train loss:0.0008968461193368185\n",
      "train loss:0.001533654730854544\n",
      "train loss:1.1799601616618833e-05\n",
      "train loss:0.0018925472392963755\n",
      "train loss:0.0004520496390553724\n",
      "train loss:0.0021337137598220835\n",
      "train loss:0.0017007931034326263\n",
      "train loss:0.0003176014403531431\n",
      "train loss:0.00023287113862031418\n",
      "train loss:0.0005025229790200679\n",
      "train loss:0.0001863896766371958\n",
      "train loss:0.00040096179783494934\n",
      "train loss:0.0008032670515778937\n",
      "train loss:0.00016656064932677646\n",
      "train loss:0.0002938775962147204\n",
      "train loss:0.0008276939603940148\n",
      "train loss:0.0012479134025322844\n",
      "train loss:0.0011746087754381095\n",
      "train loss:0.000505115250552886\n",
      "train loss:0.0010465962151019542\n",
      "train loss:0.0016078132038195045\n",
      "train loss:0.00036110792577882\n",
      "train loss:8.232860474787491e-05\n",
      "train loss:9.930774083909905e-05\n",
      "train loss:2.2705138508366608e-05\n",
      "train loss:0.0003103287252409243\n",
      "train loss:0.0005793302696595631\n",
      "train loss:0.0005135154886747941\n",
      "train loss:4.933797177812659e-05\n",
      "train loss:5.069624249489158e-05\n",
      "train loss:1.2327007968514812e-05\n",
      "train loss:9.356642769310011e-05\n",
      "train loss:0.00011178428548118428\n",
      "train loss:0.0020588035147339253\n",
      "train loss:0.001978454799302688\n",
      "train loss:0.00012913205071826686\n",
      "train loss:0.0009711726958411414\n",
      "train loss:0.0011721836730749949\n",
      "train loss:0.0020581329099557564\n",
      "train loss:0.00019775533116105453\n",
      "train loss:0.0002486134464573213\n",
      "train loss:0.0009387929232133336\n",
      "train loss:0.0011942223960345357\n",
      "train loss:4.804775918879872e-05\n",
      "train loss:0.0009802172243675912\n",
      "train loss:0.0018114713349839757\n",
      "train loss:0.0009650277923805832\n",
      "train loss:0.00017397609218635583\n",
      "train loss:8.413826366768306e-05\n",
      "train loss:0.002007285321421673\n",
      "train loss:0.0005964595134087252\n",
      "train loss:4.4956757805068596e-05\n",
      "train loss:0.001375520533941674\n",
      "train loss:0.000263790356026743\n",
      "train loss:0.0001325315270935381\n",
      "train loss:0.002053163428483436\n",
      "train loss:6.25056759744591e-05\n",
      "train loss:0.0012882429944283192\n",
      "train loss:0.00023260334385127401\n",
      "train loss:0.0004061038712099334\n",
      "train loss:5.312903534255127e-05\n",
      "train loss:4.860488114260571e-05\n",
      "train loss:0.00021582150836616673\n",
      "train loss:3.0154946398606848e-05\n",
      "train loss:0.0004650161293451636\n",
      "=== epoch:19, train acc:1.0, test acc:0.987 ===\n",
      "train loss:0.00035368243108409504\n",
      "train loss:0.0007101833331859585\n",
      "train loss:0.00024181639429439935\n",
      "train loss:2.1526681498709533e-05\n",
      "train loss:3.993152084549938e-05\n",
      "train loss:1.872676581803338e-05\n",
      "train loss:0.00028000021653587173\n",
      "train loss:0.001033118037447392\n",
      "train loss:0.0009336952122187641\n",
      "train loss:6.403744263780805e-05\n",
      "train loss:0.00044755488191622545\n",
      "train loss:0.003494306103510921\n",
      "train loss:0.0004652838685742862\n",
      "train loss:7.640838300062096e-05\n",
      "train loss:4.325607471800939e-05\n",
      "train loss:0.0007207925456933371\n",
      "train loss:0.00044165576654475036\n",
      "train loss:0.000219182404279214\n",
      "train loss:0.000519526495993696\n",
      "train loss:1.5500319626327322e-05\n",
      "train loss:0.0007892124649097254\n",
      "train loss:0.004097757563475962\n",
      "train loss:0.000675718267105791\n",
      "train loss:0.0004353035972820182\n",
      "train loss:5.396145151708531e-05\n",
      "train loss:1.4231836961099744e-05\n",
      "train loss:0.004008242593008131\n",
      "train loss:4.770575059404508e-05\n",
      "train loss:0.00013233785015331933\n",
      "train loss:3.610144222349033e-05\n",
      "train loss:2.1277282601505517e-05\n",
      "train loss:0.00033313938442785574\n",
      "train loss:0.0004293225750342076\n",
      "train loss:0.0008274911100675995\n",
      "train loss:0.00010180990419194135\n",
      "train loss:0.0003621137591547189\n",
      "train loss:9.598595281603558e-05\n",
      "train loss:1.3461832026905886e-05\n",
      "train loss:0.0009949839388875962\n",
      "train loss:0.00014604535421387915\n",
      "train loss:0.0007002906726345387\n",
      "train loss:0.0001846697331742556\n",
      "train loss:0.0006081287708375229\n",
      "train loss:0.00374655616624655\n",
      "train loss:0.0012884103488949947\n",
      "train loss:5.8297734351644165e-05\n",
      "train loss:0.004595653882568146\n",
      "train loss:0.000796831706701693\n",
      "train loss:6.656290229472015e-05\n",
      "train loss:3.5572911168679676e-05\n",
      "train loss:5.2746864496291176e-05\n",
      "train loss:0.00032010984588826347\n",
      "train loss:9.064367266897478e-05\n",
      "train loss:7.254321253641688e-05\n",
      "train loss:0.0002500662929042752\n",
      "train loss:4.743065030017602e-05\n",
      "train loss:0.0003322167568426119\n",
      "train loss:0.0005103130500316489\n",
      "train loss:0.0007198217579206715\n",
      "train loss:5.9702746603200316e-05\n",
      "train loss:0.00012055578345084095\n",
      "train loss:0.0010939907856257329\n",
      "train loss:0.00018430845484468915\n",
      "train loss:3.4467080024119134e-05\n",
      "train loss:0.0001141880738992707\n",
      "train loss:5.048608559409589e-05\n",
      "train loss:1.7494818868116834e-05\n",
      "train loss:0.00141213469608306\n",
      "train loss:0.00013268507218572117\n",
      "train loss:0.0015456893970860577\n",
      "train loss:0.001418840482335621\n",
      "train loss:6.920918813668087e-06\n",
      "train loss:0.1550046063722789\n",
      "train loss:7.151856886332446e-05\n",
      "train loss:0.03578768765760537\n",
      "train loss:0.00012706520860147056\n",
      "train loss:0.0014713392465988398\n",
      "train loss:4.7564430164401624e-05\n",
      "train loss:0.0021445155190173816\n",
      "train loss:0.0002509065974617545\n",
      "train loss:1.0645953372395243e-05\n",
      "train loss:0.00015781290065620837\n",
      "train loss:0.0018749082400539234\n",
      "train loss:0.002598473479311779\n",
      "train loss:0.0003986744138366265\n",
      "train loss:0.0010682735439424748\n",
      "train loss:0.00019532477219311444\n",
      "train loss:0.00042975293128325265\n",
      "train loss:8.803672426572003e-05\n",
      "train loss:5.055811737870621e-05\n",
      "train loss:0.005988020804101356\n",
      "train loss:0.011000660873865014\n",
      "train loss:0.00015519041042838885\n",
      "train loss:0.0005517112930346717\n",
      "train loss:0.00019245475351667397\n",
      "train loss:0.0008296684071334616\n",
      "train loss:2.3840950062515843e-05\n",
      "train loss:0.0002565768242340908\n",
      "train loss:0.0013141109057117194\n",
      "train loss:0.0002484548780028854\n",
      "train loss:0.010618164058796625\n",
      "train loss:0.002283448623348754\n",
      "train loss:4.40069977240581e-05\n",
      "train loss:0.0011693008385608155\n",
      "train loss:0.0008179597414615533\n",
      "train loss:0.0002860868056889765\n",
      "train loss:0.002701156885596379\n",
      "train loss:0.00014827374524186747\n",
      "train loss:0.0012563630472774267\n",
      "train loss:0.00021677824440977434\n",
      "train loss:0.00021804741769107457\n",
      "train loss:0.001892032453169519\n",
      "train loss:6.905241271738386e-05\n",
      "train loss:0.00011876376908025574\n",
      "train loss:0.0026862773179168386\n",
      "train loss:2.3137781211756728e-05\n",
      "train loss:0.00033039300363555145\n",
      "train loss:0.00049854254064379\n",
      "train loss:0.00035490219854714074\n",
      "train loss:1.7317380026450423e-05\n",
      "train loss:0.0017972025785318673\n",
      "train loss:0.00039208117191454436\n",
      "train loss:0.0006304539327858104\n",
      "train loss:0.0001023094769857149\n",
      "train loss:8.840721865750917e-05\n",
      "train loss:1.3694490711272251e-05\n",
      "train loss:0.0001410731074520785\n",
      "train loss:0.0002863511914248557\n",
      "train loss:7.269674601958657e-05\n",
      "train loss:0.00017674830802473846\n",
      "train loss:0.0008704000310532597\n",
      "train loss:0.0009583661681744721\n",
      "train loss:0.0010759020789356177\n",
      "train loss:7.763209754110176e-05\n",
      "train loss:0.00021078872629305223\n",
      "train loss:0.004151595920581404\n",
      "train loss:0.00033371692808005835\n",
      "train loss:0.04089651657254655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2508550395907856e-06\n",
      "train loss:0.000369778822799941\n",
      "train loss:0.00028828698969160646\n",
      "train loss:0.0006821387764337942\n",
      "train loss:0.0033559914981512854\n",
      "train loss:4.572529881477444e-05\n",
      "train loss:0.009798771242133276\n",
      "train loss:0.0005120025088196099\n",
      "train loss:0.0007289959635731315\n",
      "train loss:0.001568261477348752\n",
      "train loss:0.0003735192384001364\n",
      "train loss:5.749523594457054e-05\n",
      "train loss:0.00011201357951476369\n",
      "train loss:0.002554378191448658\n",
      "train loss:0.00027227498740723055\n",
      "train loss:0.0002312738734390562\n",
      "train loss:0.0009533242912305833\n",
      "train loss:6.226916153238242e-05\n",
      "train loss:0.00018100227815811325\n",
      "train loss:2.3174884041978763e-05\n",
      "train loss:0.000322462060026538\n",
      "train loss:0.0008938858752279555\n",
      "train loss:0.0019458504885829125\n",
      "train loss:0.000290648289730526\n",
      "train loss:0.0003064607520774322\n",
      "train loss:0.0005680432860340077\n",
      "train loss:0.001991815041257664\n",
      "train loss:0.010269640616707392\n",
      "train loss:0.0006429912915157271\n",
      "train loss:0.00025564712475182813\n",
      "train loss:2.2332268836322567e-05\n",
      "train loss:0.0004906358082123731\n",
      "train loss:0.0007310463762992496\n",
      "train loss:0.0001937004442435668\n",
      "train loss:0.0009192116190486846\n",
      "train loss:0.0005858208673680273\n",
      "train loss:0.0008341038726405224\n",
      "train loss:0.00017614729730456362\n",
      "train loss:0.002533196681112926\n",
      "train loss:0.001333474805208643\n",
      "train loss:1.6792225778409353e-05\n",
      "train loss:0.00031597042269649873\n",
      "train loss:7.073702994711872e-05\n",
      "train loss:0.0002994024887529953\n",
      "train loss:5.866553787978008e-05\n",
      "train loss:0.00012211666249800488\n",
      "train loss:0.0013213808794990507\n",
      "train loss:0.0024875965104159377\n",
      "train loss:7.875194622040869e-05\n",
      "train loss:3.3805429679402856e-05\n",
      "train loss:2.443893379430893e-05\n",
      "train loss:2.2031032144338305e-06\n",
      "train loss:0.00031328518183339123\n",
      "train loss:1.979133594244533e-05\n",
      "train loss:0.001162847501197269\n",
      "train loss:4.6891603954126486e-05\n",
      "train loss:0.0011491585987579475\n",
      "train loss:0.0006241958265871035\n",
      "train loss:0.00021560027007509557\n",
      "train loss:0.0007554231200445534\n",
      "train loss:0.0005380905772706787\n",
      "train loss:0.00047318983555632877\n",
      "train loss:0.0004730160839195307\n",
      "train loss:0.0007394883496684856\n",
      "train loss:1.5170821285350585e-05\n",
      "train loss:4.526728380227419e-05\n",
      "train loss:0.00019063111981720482\n",
      "train loss:0.0003557797226978248\n",
      "train loss:0.0001675214089868712\n",
      "train loss:0.00029662658487693733\n",
      "train loss:0.0004784561547329142\n",
      "train loss:0.0017334593114305841\n",
      "train loss:0.0005816150653397582\n",
      "train loss:6.9826414038006e-05\n",
      "train loss:0.0005180775224544097\n",
      "train loss:3.953711037594542e-05\n",
      "train loss:5.547545160030992e-05\n",
      "train loss:4.2745496574046705e-05\n",
      "train loss:0.00023694061133874606\n",
      "train loss:8.134181754962182e-05\n",
      "train loss:9.33906964560511e-06\n",
      "train loss:3.50445921787423e-05\n",
      "train loss:0.0003579927883581106\n",
      "train loss:6.114318732431203e-05\n",
      "train loss:0.0004089484392591288\n",
      "train loss:0.002253930627670541\n",
      "train loss:0.00026890628382995055\n",
      "train loss:0.0002561003080402386\n",
      "train loss:0.00012563962594980838\n",
      "train loss:0.00011820759966645086\n",
      "train loss:0.00014745527867939315\n",
      "train loss:4.713603632792835e-05\n",
      "train loss:7.703878863967307e-06\n",
      "train loss:0.0010929337819182592\n",
      "train loss:8.653314684674397e-05\n",
      "train loss:9.983952131462752e-05\n",
      "train loss:0.00033899362255863615\n",
      "train loss:8.114471375075402e-05\n",
      "train loss:0.0003587726831808395\n",
      "train loss:0.0002399507573148054\n",
      "train loss:0.00023401914721137194\n",
      "train loss:0.0010987023201168154\n",
      "train loss:6.507358185697153e-05\n",
      "train loss:0.00010268920829294795\n",
      "train loss:6.104926297107813e-05\n",
      "train loss:0.0002128881555321413\n",
      "train loss:0.0004467366841356521\n",
      "train loss:0.0003386315034076945\n",
      "train loss:0.00015073015651554422\n",
      "train loss:8.23958237131699e-05\n",
      "train loss:2.6759698955634824e-05\n",
      "train loss:4.169519861708039e-06\n",
      "train loss:0.000990088868673792\n",
      "train loss:8.71868167723659e-05\n",
      "train loss:0.004274528641293048\n",
      "train loss:0.0007793364673152371\n",
      "train loss:0.0008180561818930841\n",
      "train loss:0.0007063043028441511\n",
      "train loss:0.00047775297947935425\n",
      "train loss:6.330355280342698e-05\n",
      "train loss:0.00014444848929041521\n",
      "train loss:0.0008357398470781528\n",
      "train loss:0.0010010536434008023\n",
      "train loss:0.0002073770729296197\n",
      "train loss:0.00024099807010922282\n",
      "train loss:0.0014368674664342526\n",
      "train loss:0.0022918959604562913\n",
      "train loss:1.423795137736736e-05\n",
      "train loss:0.0017632819096699837\n",
      "train loss:0.00035833994471529124\n",
      "train loss:0.0006406884900739061\n",
      "train loss:0.0002022050506859514\n",
      "train loss:0.00015646934986361907\n",
      "train loss:5.3663019860575855e-05\n",
      "train loss:0.02791360148541656\n",
      "train loss:0.00024667159279443096\n",
      "train loss:9.033497251390944e-05\n",
      "train loss:0.00014983662841369332\n",
      "train loss:0.00010231935906256222\n",
      "train loss:0.0005314688661350313\n",
      "train loss:8.701319126588779e-05\n",
      "train loss:1.663724589183932e-05\n",
      "train loss:0.0004215482765539774\n",
      "train loss:4.28596921100704e-05\n",
      "train loss:0.00021910332219849247\n",
      "train loss:0.0008733899957882051\n",
      "train loss:1.1590968017915974e-05\n",
      "train loss:0.0013685173489880215\n",
      "train loss:0.0015989983916500949\n",
      "train loss:0.0006439247332514513\n",
      "train loss:0.0008064522708992056\n",
      "train loss:5.333305541854132e-05\n",
      "train loss:0.000994348942269439\n",
      "train loss:0.0004706101650173081\n",
      "train loss:0.0009046903815331108\n",
      "train loss:0.00017656941905846974\n",
      "train loss:0.0002223544551796755\n",
      "train loss:3.836138042800314e-05\n",
      "train loss:0.0016345111365418006\n",
      "train loss:4.138567494912899e-05\n",
      "train loss:2.1915902405832896e-05\n",
      "train loss:3.9732511324359256e-05\n",
      "train loss:0.0005465815273682203\n",
      "train loss:0.001325503626643452\n",
      "train loss:0.00015152725353121115\n",
      "train loss:0.002252823992161833\n",
      "train loss:5.272458289354617e-05\n",
      "train loss:0.001223643847032533\n",
      "train loss:8.763466145544156e-06\n",
      "train loss:0.0002404902171491762\n",
      "train loss:0.0001286155970776877\n",
      "train loss:0.0043530854721963745\n",
      "train loss:6.842544591046499e-05\n",
      "train loss:0.0001179951700682521\n",
      "train loss:0.0016377131278777282\n",
      "train loss:9.246998288138877e-05\n",
      "train loss:0.00017170991926362595\n",
      "train loss:4.437959781818755e-05\n",
      "train loss:0.0006768448830924565\n",
      "train loss:7.759649669176269e-05\n",
      "train loss:0.0003363731397257453\n",
      "train loss:0.00022505197983696703\n",
      "train loss:0.0011561616473631216\n",
      "train loss:0.0006672786752407911\n",
      "train loss:0.0009196223020166468\n",
      "train loss:0.00019843014814124453\n",
      "train loss:0.0008477451362728998\n",
      "train loss:4.3503558206252604e-05\n",
      "train loss:3.211058891781098e-05\n",
      "train loss:0.0003316270223860534\n",
      "train loss:0.0004259311302847118\n",
      "train loss:1.534147329001525e-05\n",
      "train loss:1.750321841920106e-05\n",
      "train loss:6.986072858975863e-05\n",
      "train loss:0.00011459623486143547\n",
      "train loss:0.00017853075906528842\n",
      "train loss:0.0007926142293417325\n",
      "train loss:0.0006791718332061887\n",
      "train loss:0.0017106633889886244\n",
      "train loss:0.00017285460422449765\n",
      "train loss:0.008629819234014548\n",
      "train loss:0.0016288232272918819\n",
      "train loss:0.0007967556272452347\n",
      "train loss:3.795367189742065e-05\n",
      "train loss:0.0016061155563568041\n",
      "train loss:9.538112756615378e-05\n",
      "train loss:0.00467684285349947\n",
      "train loss:0.00039657357786264516\n",
      "train loss:0.0007178582424415685\n",
      "train loss:0.0001253922204203628\n",
      "train loss:0.0003433473490435704\n",
      "train loss:9.992889270896474e-05\n",
      "train loss:0.00014781925980392348\n",
      "train loss:0.00014194734392905463\n",
      "train loss:0.0012636994209160805\n",
      "train loss:0.00034633154949798484\n",
      "train loss:1.4681533581278607e-05\n",
      "train loss:0.0030829456200321547\n",
      "train loss:0.0003164058605621148\n",
      "train loss:1.50351970978247e-05\n",
      "train loss:3.3933957126080474e-05\n",
      "train loss:0.0005159929965916906\n",
      "train loss:8.186273632442389e-05\n",
      "train loss:2.499882385087638e-05\n",
      "train loss:0.00186719090285395\n",
      "train loss:7.958400289362757e-05\n",
      "train loss:3.6901436001835125e-05\n",
      "train loss:3.210051836990545e-05\n",
      "train loss:0.0002922194472650184\n",
      "train loss:4.155048666352651e-05\n",
      "train loss:8.106416513241289e-05\n",
      "train loss:0.007632638617768584\n",
      "train loss:1.081890959966075e-05\n",
      "train loss:0.0022018318604949774\n",
      "train loss:9.621882632693984e-05\n",
      "train loss:0.0001814342881162867\n",
      "train loss:0.0004436729825143482\n",
      "train loss:0.008030561728782158\n",
      "train loss:0.0003384923854774427\n",
      "train loss:0.0031576132900248536\n",
      "train loss:0.0007883995675626301\n",
      "train loss:9.206540262234641e-05\n",
      "train loss:0.00026948805683438325\n",
      "train loss:0.0015882676757321165\n",
      "train loss:0.00021393895871996612\n",
      "train loss:0.00035813782082077535\n",
      "train loss:0.0009124267972285221\n",
      "train loss:0.0007559751851949052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.004226544388789948\n",
      "train loss:0.0032727877479566724\n",
      "train loss:0.0002546798417954435\n",
      "train loss:1.9274295454649795e-05\n",
      "train loss:0.0011759369611681445\n",
      "train loss:0.007215960424414267\n",
      "train loss:0.0005196761795530723\n",
      "train loss:0.00013961640849320057\n",
      "train loss:0.001645360941449956\n",
      "train loss:0.0005723180123620327\n",
      "train loss:0.0006516474660261562\n",
      "train loss:0.002793795970568938\n",
      "train loss:0.026248453612665427\n",
      "train loss:0.001473470690541136\n",
      "train loss:4.1868547614389943e-05\n",
      "train loss:0.00037187268596905834\n",
      "train loss:0.002746747181515483\n",
      "train loss:0.00036882477812045426\n",
      "train loss:0.00017346952230562636\n",
      "train loss:0.001399275193624175\n",
      "train loss:0.00048305403407688635\n",
      "train loss:0.00047545724875461823\n",
      "train loss:0.003962665867547176\n",
      "train loss:0.0002153251080822894\n",
      "train loss:1.660833386474237e-05\n",
      "train loss:0.00011898049229030702\n",
      "train loss:0.00010831280678209013\n",
      "train loss:7.087821013299069e-06\n",
      "train loss:0.0010845243343108107\n",
      "train loss:1.022491109044016e-05\n",
      "train loss:0.0002790914363158651\n",
      "train loss:0.001733902821201309\n",
      "train loss:8.731570406240908e-05\n",
      "train loss:6.536139753625512e-05\n",
      "train loss:2.4089720512618924e-05\n",
      "train loss:0.00041143144276556735\n",
      "train loss:0.00020328421054743184\n",
      "train loss:0.00025637137089809124\n",
      "train loss:0.00034907099458907916\n",
      "train loss:7.463456888658097e-05\n",
      "train loss:0.0009106339334309467\n",
      "train loss:2.1387640380608808e-05\n",
      "train loss:0.0009353560781877497\n",
      "train loss:0.00024770605771363746\n",
      "train loss:2.379425352616911e-05\n",
      "train loss:0.0010277417119319192\n",
      "train loss:0.00014762260344860226\n",
      "train loss:0.0003344011648024324\n",
      "train loss:0.00010606110093175205\n",
      "train loss:0.02286928395200744\n",
      "train loss:0.005116666705256663\n",
      "train loss:0.00027444549007941895\n",
      "train loss:0.00020799885190215901\n",
      "train loss:0.00016897797886345552\n",
      "train loss:7.619269994672557e-05\n",
      "train loss:0.0021941705397088484\n",
      "train loss:6.707900016158464e-05\n",
      "train loss:0.00014589759376056193\n",
      "train loss:6.804683327527377e-05\n",
      "train loss:0.0027775720078602965\n",
      "train loss:0.002950081184526493\n",
      "train loss:0.0010907240177540829\n",
      "train loss:0.0006492121205238302\n",
      "train loss:0.0002127258596034738\n",
      "train loss:0.0012055678582223394\n",
      "train loss:6.29766642277027e-05\n",
      "train loss:0.002738418794546354\n",
      "train loss:1.0781774600245952e-05\n",
      "train loss:0.0017556171342069668\n",
      "train loss:0.0003081805694402599\n",
      "train loss:8.480801585870997e-05\n",
      "train loss:0.0007857384429146828\n",
      "train loss:0.00013835441211586683\n",
      "train loss:0.0001084665541109262\n",
      "train loss:0.0001665745249372673\n",
      "train loss:0.00037442744483132313\n",
      "train loss:2.8371021557073187e-05\n",
      "train loss:0.0001310959960824074\n",
      "train loss:0.0009042193155436957\n",
      "train loss:0.00042140909385824364\n",
      "train loss:9.05895120364593e-05\n",
      "train loss:0.0003042409889968013\n",
      "train loss:0.0012366359312870917\n",
      "train loss:0.001385158425517989\n",
      "train loss:0.0005039314740615189\n",
      "train loss:0.0007440988261391915\n",
      "train loss:0.00024339968356139666\n",
      "train loss:0.0018497615813330922\n",
      "train loss:4.7348127998420085e-05\n",
      "train loss:2.9874913252051516e-05\n",
      "train loss:0.00030122033840050395\n",
      "train loss:8.401129049992131e-05\n",
      "train loss:0.00029655746547102785\n",
      "train loss:7.552348981666247e-05\n",
      "train loss:0.0009931003054656571\n",
      "train loss:5.1450813929483555e-05\n",
      "train loss:0.0005499863803475569\n",
      "train loss:1.7623112304199425e-05\n",
      "train loss:0.00022562236416144232\n",
      "train loss:0.001949079227269846\n",
      "train loss:1.6408913454483107e-05\n",
      "train loss:0.0005851431812584323\n",
      "train loss:0.00027363053604530636\n",
      "train loss:0.000277321019221975\n",
      "train loss:6.504382939337193e-05\n",
      "train loss:0.00010957882628673874\n",
      "train loss:0.00017557945938142253\n",
      "train loss:0.0012373858672361706\n",
      "train loss:0.002142714056296925\n",
      "train loss:0.0001500709349073852\n",
      "train loss:0.0007779893661447359\n",
      "train loss:0.0006996855989792362\n",
      "train loss:0.0002540211344467135\n",
      "train loss:7.692961502663819e-06\n",
      "train loss:0.0005273149213625276\n",
      "train loss:0.0006696521128024808\n",
      "train loss:1.6613234322083856e-05\n",
      "train loss:0.0032503923125338325\n",
      "train loss:0.00010189117987936466\n",
      "train loss:9.672527411940755e-05\n",
      "train loss:0.001586443688329209\n",
      "train loss:1.3906169745863028e-05\n",
      "train loss:0.0009997941884250636\n",
      "train loss:0.0008771818194690344\n",
      "train loss:1.6236002114278586e-05\n",
      "train loss:0.00013433591539875327\n",
      "train loss:0.0002247918202706854\n",
      "train loss:0.0019312831449220008\n",
      "train loss:0.00021612958035358606\n",
      "train loss:0.0003184449324742558\n",
      "train loss:0.00020387455269959946\n",
      "train loss:0.00019191465795990014\n",
      "train loss:9.085233169206064e-05\n",
      "train loss:0.0008712191244709796\n",
      "train loss:0.00023900977931193328\n",
      "train loss:0.0010420674323584952\n",
      "train loss:0.006646740419621297\n",
      "train loss:0.0008305424976807565\n",
      "train loss:0.0009035145761115143\n",
      "train loss:0.0007441719312435375\n",
      "train loss:0.011573551616208811\n",
      "train loss:0.0007830511880368985\n",
      "train loss:2.0897177003906957e-05\n",
      "train loss:0.0006298990910425258\n",
      "train loss:1.5833240956497704e-05\n",
      "train loss:0.00011611388831949573\n",
      "train loss:0.0010275914518993188\n",
      "train loss:0.0031405232307659446\n",
      "train loss:0.0005860911014857718\n",
      "train loss:0.005407517689501457\n",
      "train loss:0.0006678444415325051\n",
      "train loss:0.001274791214559983\n",
      "train loss:0.0018146107665926747\n",
      "train loss:0.0005583345323071842\n",
      "train loss:0.0007464242823343157\n",
      "train loss:0.0042221941630076485\n",
      "train loss:0.00013399796504371837\n",
      "train loss:0.0007308964973403656\n",
      "train loss:0.0004489791128224957\n",
      "train loss:0.0005208016057004636\n",
      "train loss:0.000830617022828216\n",
      "train loss:0.0009670619012671923\n",
      "train loss:8.061500617974423e-05\n",
      "train loss:0.000298175255109603\n",
      "train loss:8.431317309657011e-05\n",
      "train loss:0.000978890698144612\n",
      "train loss:4.7466090357609154e-05\n",
      "train loss:2.723213383048602e-05\n",
      "train loss:0.0011020600011818291\n",
      "train loss:2.190559312168193e-05\n",
      "train loss:0.00043699705000684036\n",
      "train loss:0.00013759241610406637\n",
      "train loss:0.0004425134878277343\n",
      "train loss:0.00030394260975794967\n",
      "train loss:0.0006754452471113407\n",
      "train loss:0.00025229570087508603\n",
      "train loss:0.00020311139298478004\n",
      "train loss:0.0015564039457856063\n",
      "train loss:6.708142849405322e-05\n",
      "train loss:0.00026897168889751614\n",
      "train loss:0.00022711670506726726\n",
      "train loss:3.83429309806077e-05\n",
      "train loss:0.0002626339588800767\n",
      "train loss:0.0003065803293261704\n",
      "train loss:0.0001626742407953495\n",
      "train loss:0.0004420253142899546\n",
      "train loss:1.0738535863795988e-05\n",
      "train loss:0.0007302762565687174\n",
      "train loss:0.00021126188630846002\n",
      "train loss:0.0012017127956991552\n",
      "train loss:0.003168146026831199\n",
      "train loss:2.266335866178102e-05\n",
      "train loss:8.578310770135416e-05\n",
      "train loss:0.0004797411758884131\n",
      "train loss:6.859648173110206e-05\n",
      "train loss:0.0006037395721574416\n",
      "train loss:0.0005646863808459475\n",
      "train loss:0.0005119390156501546\n",
      "train loss:0.0009481799194717415\n",
      "train loss:5.778311949700219e-05\n",
      "train loss:5.259568492906379e-05\n",
      "train loss:3.833730240880159e-05\n",
      "train loss:0.0006604296032427967\n",
      "train loss:0.0007675128202947266\n",
      "train loss:0.00015004840251643133\n",
      "train loss:8.386897954821841e-05\n",
      "train loss:0.00011111891240918686\n",
      "train loss:0.0035695460764401323\n",
      "train loss:0.00015779513109116488\n",
      "train loss:0.00035292321588697924\n",
      "train loss:5.3494503354489816e-05\n",
      "train loss:8.881166282817416e-06\n",
      "train loss:9.968042207377325e-05\n",
      "train loss:9.74091888731938e-05\n",
      "=== epoch:20, train acc:1.0, test acc:0.986 ===\n",
      "train loss:0.00038857323179607476\n",
      "train loss:0.0006997397438034949\n",
      "train loss:1.820926047989336e-05\n",
      "train loss:0.00030101668676160503\n",
      "train loss:0.0009420635064673174\n",
      "train loss:9.17164975082575e-05\n",
      "train loss:1.2681477582665558e-05\n",
      "train loss:3.997343588209878e-05\n",
      "train loss:0.00038469325674371263\n",
      "train loss:0.0005485571197426662\n",
      "train loss:0.00026998405615568153\n",
      "train loss:4.093787008326978e-05\n",
      "train loss:0.0007024245415737681\n",
      "train loss:0.00013865101120006217\n",
      "train loss:0.0005476177194687603\n",
      "train loss:0.00016188342046220805\n",
      "train loss:0.001189273258106287\n",
      "train loss:0.00010810266178032111\n",
      "train loss:0.0007524422879653514\n",
      "train loss:0.00039518734278854003\n",
      "train loss:0.001024418083309049\n",
      "train loss:0.0002554338996184182\n",
      "train loss:0.0005523130061401757\n",
      "train loss:3.2334400240916255e-05\n",
      "train loss:9.960712572363296e-05\n",
      "train loss:4.519056304908053e-05\n",
      "train loss:8.84425984288966e-05\n",
      "train loss:6.565907797102978e-05\n",
      "train loss:2.8813468007125238e-05\n",
      "train loss:0.0005545645236105766\n",
      "train loss:0.0007142362751833012\n",
      "train loss:0.00029311045399640413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0013956567880068248\n",
      "train loss:9.743366049721008e-05\n",
      "train loss:4.831402200255804e-06\n",
      "train loss:0.00010677462067342295\n",
      "train loss:4.849886826123949e-05\n",
      "train loss:0.00046618219182533745\n",
      "train loss:0.00026205249744948994\n",
      "train loss:2.790682239273315e-05\n",
      "train loss:4.38000349855693e-05\n",
      "train loss:0.0003848918530709998\n",
      "train loss:0.0007556924415359961\n",
      "train loss:0.0001743939116361945\n",
      "train loss:0.00012559379440112316\n",
      "train loss:0.00012265580118146036\n",
      "train loss:0.00022989234675354427\n",
      "train loss:4.140078454291902e-05\n",
      "train loss:1.9465028068841712e-05\n",
      "train loss:0.0004607609823719314\n",
      "train loss:0.001103181862043786\n",
      "train loss:3.74336244557963e-05\n",
      "train loss:1.0611827923259114e-05\n",
      "train loss:8.786840869180415e-05\n",
      "train loss:0.00037242887348093944\n",
      "train loss:0.0017731827385190583\n",
      "train loss:3.617942194211888e-05\n",
      "train loss:1.9752035918400955e-06\n",
      "train loss:0.00013667294011863442\n",
      "train loss:0.0001695655841541327\n",
      "train loss:0.00021640292377750065\n",
      "train loss:0.00016920207620040042\n",
      "train loss:1.8240983777208976e-05\n",
      "train loss:5.099926434077821e-05\n",
      "train loss:0.0010809884554385998\n",
      "train loss:9.130302284323233e-05\n",
      "train loss:0.0002413764763578946\n",
      "train loss:4.196864094208123e-05\n",
      "train loss:4.3867519225882117e-05\n",
      "train loss:0.00010019125058885636\n",
      "train loss:6.265969984192281e-05\n",
      "train loss:9.120027160031215e-05\n",
      "train loss:0.0006393626101610718\n",
      "train loss:0.0003186433264557186\n",
      "train loss:0.00010608864018258554\n",
      "train loss:0.0007976520225563702\n",
      "train loss:0.0001255070800379948\n",
      "train loss:4.892572858767814e-05\n",
      "train loss:0.00015283889962962096\n",
      "train loss:0.0006618445595555435\n",
      "train loss:2.1558459856024546e-05\n",
      "train loss:4.065463114520758e-05\n",
      "train loss:0.00032030658787316526\n",
      "train loss:0.00018779094729129176\n",
      "train loss:0.0007559371467416823\n",
      "train loss:0.0003677837309115925\n",
      "train loss:0.0001427039067582063\n",
      "train loss:0.0007638213570657422\n",
      "train loss:0.0003834105236072451\n",
      "train loss:8.799634656752818e-05\n",
      "train loss:3.787232150787409e-05\n",
      "train loss:0.00012566502162378117\n",
      "train loss:3.0512935537149264e-06\n",
      "train loss:6.839793249151305e-05\n",
      "train loss:0.00037526633485273784\n",
      "train loss:2.7635062710491167e-05\n",
      "train loss:5.72216623114602e-05\n",
      "train loss:7.682461322315666e-05\n",
      "train loss:4.7442358932589e-05\n",
      "train loss:0.0006412298570767633\n",
      "train loss:0.00033834146650639453\n",
      "train loss:0.00030002439832666394\n",
      "train loss:0.0033843298001118905\n",
      "train loss:0.0001002511951190856\n",
      "train loss:0.0001338005756957169\n",
      "train loss:2.2604634144785358e-06\n",
      "train loss:0.00011205921523617459\n",
      "train loss:0.0038375429281251746\n",
      "train loss:0.00017077953956280576\n",
      "train loss:1.1119058469550448e-05\n",
      "train loss:0.0002693269238959196\n",
      "train loss:2.6748853907171445e-05\n",
      "train loss:0.0010188979360684356\n",
      "train loss:0.00046649085535852984\n",
      "train loss:0.0012575782821228564\n",
      "train loss:0.0015538258537370637\n",
      "train loss:0.0003038064830790852\n",
      "train loss:9.503743030858644e-05\n",
      "train loss:0.00019646630366285513\n",
      "train loss:0.0014895334495560197\n",
      "train loss:0.0006041490254088383\n",
      "train loss:9.081512969337079e-05\n",
      "train loss:0.000601042710496581\n",
      "train loss:0.0008162360848697009\n",
      "train loss:0.00030880315016779754\n",
      "train loss:0.0001498333772447946\n",
      "train loss:0.0008574690190433637\n",
      "train loss:0.0004812386939819885\n",
      "train loss:7.38146603216693e-05\n",
      "train loss:0.00022437493143055514\n",
      "train loss:0.0003100629511259324\n",
      "train loss:0.00040437253107584434\n",
      "train loss:0.0005391076377376536\n",
      "train loss:6.092908836884272e-05\n",
      "train loss:3.065559904299335e-05\n",
      "train loss:0.00012975290410486545\n",
      "train loss:3.039535491293079e-05\n",
      "train loss:0.0003232304015707219\n",
      "train loss:0.00039753222691850116\n",
      "train loss:2.6501268608171346e-05\n",
      "train loss:0.00037438001864881275\n",
      "train loss:0.00015835450119366365\n",
      "train loss:0.00030114877199670175\n",
      "train loss:9.379999143793807e-05\n",
      "train loss:3.5374737704716206e-05\n",
      "train loss:0.000496117391839345\n",
      "train loss:0.0003498696326587694\n",
      "train loss:0.00023678312342847695\n",
      "train loss:0.00025059713381494163\n",
      "train loss:8.80929653044845e-06\n",
      "train loss:6.9205164856396205e-06\n",
      "train loss:7.863771444983412e-05\n",
      "train loss:5.9367507644465346e-05\n",
      "train loss:0.00012297750527322137\n",
      "train loss:0.00011144018313847509\n",
      "train loss:8.636671900366183e-05\n",
      "train loss:0.00023979377980280187\n",
      "train loss:0.00039025139133283684\n",
      "train loss:8.722230675449862e-05\n",
      "train loss:2.8461913364982833e-05\n",
      "train loss:0.0008460189770095504\n",
      "train loss:0.0011256049482188078\n",
      "train loss:0.00019529245199289348\n",
      "train loss:0.00027321183927633683\n",
      "train loss:0.00020608640132681287\n",
      "train loss:0.00020389562552788475\n",
      "train loss:0.0021633734654728366\n",
      "train loss:6.619808751240297e-05\n",
      "train loss:0.0002197770633587583\n",
      "train loss:0.001847616936069958\n",
      "train loss:0.00016619907331790113\n",
      "train loss:4.6517357658341754e-05\n",
      "train loss:0.000130028672522963\n",
      "train loss:0.0004171771200554248\n",
      "train loss:0.00012761986703422408\n",
      "train loss:0.0006405846224999745\n",
      "train loss:0.0006233693597294143\n",
      "train loss:8.410433222889898e-05\n",
      "train loss:0.00012313054862530902\n",
      "train loss:0.00027048595056071466\n",
      "train loss:0.00024381114111372146\n",
      "train loss:0.0003575832826322384\n",
      "train loss:0.014677363829094117\n",
      "train loss:0.0004890821691917865\n",
      "train loss:0.0005364742425105791\n",
      "train loss:0.0006761812488623211\n",
      "train loss:0.00026407511832331735\n",
      "train loss:5.78826940574061e-05\n",
      "train loss:0.0006096613465418306\n",
      "train loss:0.00023239113027320713\n",
      "train loss:0.0006533695443128148\n",
      "train loss:0.0001715154723620954\n",
      "train loss:0.00039322206752042\n",
      "train loss:0.00018673805907895894\n",
      "train loss:4.3401080590009235e-05\n",
      "train loss:6.168341454421452e-05\n",
      "train loss:5.378970760026059e-05\n",
      "train loss:9.117620730334658e-05\n",
      "train loss:0.0006065617607236957\n",
      "train loss:9.794917643958352e-05\n",
      "train loss:0.00027880874848082335\n",
      "train loss:0.00014487465743565024\n",
      "train loss:0.00047974359999975174\n",
      "train loss:1.8494961284003742e-05\n",
      "train loss:0.0017154393417021224\n",
      "train loss:0.0004944741749421219\n",
      "train loss:0.0006709496031269512\n",
      "train loss:9.497479631894594e-06\n",
      "train loss:7.680265843474695e-05\n",
      "train loss:0.00032439769921866493\n",
      "train loss:0.0024115204390219797\n",
      "train loss:0.0019079531770511814\n",
      "train loss:0.0007320235204717842\n",
      "train loss:2.1152542820879195e-05\n",
      "train loss:2.4613226159838012e-05\n",
      "train loss:8.249574715376522e-05\n",
      "train loss:0.0002796084744112182\n",
      "train loss:0.00021303957303859527\n",
      "train loss:0.001423757419774478\n",
      "train loss:0.0002066446970885772\n",
      "train loss:6.0874777353902236e-05\n",
      "train loss:0.0006934308828078341\n",
      "train loss:5.5267638666710546e-05\n",
      "train loss:8.754622666336236e-05\n",
      "train loss:0.00035887014573945497\n",
      "train loss:8.854156024080024e-05\n",
      "train loss:8.755391481152707e-05\n",
      "train loss:8.514554693540114e-05\n",
      "train loss:0.0010036545592106564\n",
      "train loss:0.0004417230874854712\n",
      "train loss:0.00053811281563372\n",
      "train loss:0.00023683359854105059\n",
      "train loss:0.0005995063564545435\n",
      "train loss:4.4802614345100645e-05\n",
      "train loss:3.965318032635878e-05\n",
      "train loss:0.00033615486639493\n",
      "train loss:0.0008156417159870519\n",
      "train loss:0.0008430963585327357\n",
      "train loss:4.461423892126019e-05\n",
      "train loss:0.000184705432065214\n",
      "train loss:2.3123814793845213e-05\n",
      "train loss:2.5416163294360627e-05\n",
      "train loss:0.0002458769281554797\n",
      "train loss:0.00047439763367452425\n",
      "train loss:1.608330674067908e-05\n",
      "train loss:8.124598235318289e-05\n",
      "train loss:0.0015309167735545407\n",
      "train loss:0.00017761528495214985\n",
      "train loss:0.00010168829555700269\n",
      "train loss:4.423750313228613e-05\n",
      "train loss:0.00022500315040713506\n",
      "train loss:1.9590560054498387e-05\n",
      "train loss:9.941310074022923e-05\n",
      "train loss:0.001347668363250403\n",
      "train loss:8.390813538050514e-06\n",
      "train loss:0.00011647695969236814\n",
      "train loss:0.00036444171137971367\n",
      "train loss:2.9648365246693355e-05\n",
      "train loss:0.00014984488331691578\n",
      "train loss:0.00034979115255374157\n",
      "train loss:0.0005095904391197694\n",
      "train loss:0.0009836548837971744\n",
      "train loss:0.0006331024381684118\n",
      "train loss:0.0012672047708619655\n",
      "train loss:6.300799725159825e-05\n",
      "train loss:0.001277995915758162\n",
      "train loss:0.001690265800841485\n",
      "train loss:5.044741840928422e-06\n",
      "train loss:3.428622579327892e-05\n",
      "train loss:0.00018477131766555697\n",
      "train loss:0.00020327566665233378\n",
      "train loss:5.568270587166173e-05\n",
      "train loss:5.6720573728335815e-05\n",
      "train loss:0.0007990841395307757\n",
      "train loss:0.00040163810912150275\n",
      "train loss:0.0008002514805227886\n",
      "train loss:9.306098939349319e-05\n",
      "train loss:0.00012714720644516715\n",
      "train loss:0.0055643459372743675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.1425674684424705e-05\n",
      "train loss:0.0006471388554376853\n",
      "train loss:0.00045789164648512236\n",
      "train loss:0.000571143788300163\n",
      "train loss:0.00020377740891459032\n",
      "train loss:0.00012330735946656756\n",
      "train loss:1.7210897245059967e-05\n",
      "train loss:0.0005466572898182149\n",
      "train loss:8.73873843282775e-05\n",
      "train loss:0.001168259457344028\n",
      "train loss:0.0006364917605495572\n",
      "train loss:0.001226477034073025\n",
      "train loss:0.00012970697055691107\n",
      "train loss:0.0010312742755377283\n",
      "train loss:0.0018607341488726273\n",
      "train loss:0.00016954649654157613\n",
      "train loss:0.00028187045781011445\n",
      "train loss:9.750791391332741e-05\n",
      "train loss:0.0004236469385199618\n",
      "train loss:2.571739588356094e-05\n",
      "train loss:0.00011031376329214397\n",
      "train loss:0.0007685079282112182\n",
      "train loss:4.476610242312865e-05\n",
      "train loss:1.848616462517016e-05\n",
      "train loss:0.0005945919559895448\n",
      "train loss:0.0008482546444449034\n",
      "train loss:0.00032775421490652235\n",
      "train loss:0.00018850100854982254\n",
      "train loss:0.0015425692054279263\n",
      "train loss:5.4418083700526104e-05\n",
      "train loss:0.00044846939196227503\n",
      "train loss:0.0004623592528485699\n",
      "train loss:0.000506540424090896\n",
      "train loss:6.519506758486151e-05\n",
      "train loss:0.0004896067429145537\n",
      "train loss:4.3274700645533365e-06\n",
      "train loss:6.262338710656107e-05\n",
      "train loss:0.01836258819571914\n",
      "train loss:0.004537563901425779\n",
      "train loss:0.004659355600091743\n",
      "train loss:0.0019618275556410937\n",
      "train loss:4.997099380777649e-05\n",
      "train loss:6.208906085116048e-05\n",
      "train loss:0.0007405583211269451\n",
      "train loss:8.698002437161039e-05\n",
      "train loss:5.750936321366601e-05\n",
      "train loss:0.0010959331451474825\n",
      "train loss:0.00013098161983553146\n",
      "train loss:1.5490165766600457e-05\n",
      "train loss:0.0005195758068434976\n",
      "train loss:0.0007927592509102152\n",
      "train loss:0.0006585424968891751\n",
      "train loss:7.217404410699569e-06\n",
      "train loss:0.000622769315580704\n",
      "train loss:0.0018971047682279083\n",
      "train loss:0.0002693105774554881\n",
      "train loss:0.00025883976834968213\n",
      "train loss:0.01332390226633844\n",
      "train loss:0.0018890142514876318\n",
      "train loss:0.0002531415928844827\n",
      "train loss:1.7144012692603994e-05\n",
      "train loss:0.00048734710182447556\n",
      "train loss:9.296117187309783e-06\n",
      "train loss:2.430796720809114e-05\n",
      "train loss:0.00010234288487435432\n",
      "train loss:0.0005643575408461449\n",
      "train loss:0.00026644850744031635\n",
      "train loss:0.0004557017203082603\n",
      "train loss:0.0049978841380510656\n",
      "train loss:0.00014843253978625518\n",
      "train loss:4.7970668460166455e-05\n",
      "train loss:3.7307488818065046e-05\n",
      "train loss:0.0037393712796069106\n",
      "train loss:1.0186079614148335e-05\n",
      "train loss:0.0009671258110418892\n",
      "train loss:1.077620091367959e-05\n",
      "train loss:4.031916969866287e-05\n",
      "train loss:0.0003320530136165935\n",
      "train loss:0.0008997600695019154\n",
      "train loss:0.00012519515782845968\n",
      "train loss:5.842679896328245e-05\n",
      "train loss:5.59158304040344e-06\n",
      "train loss:0.015128895289508558\n",
      "train loss:0.0021339811595638123\n",
      "train loss:0.0009948240698376826\n",
      "train loss:0.0013458897416826331\n",
      "train loss:0.00013482095528539434\n",
      "train loss:0.004945442220549705\n",
      "train loss:0.001652031049369445\n",
      "train loss:0.0004685132815701231\n",
      "train loss:0.001156949936987395\n",
      "train loss:2.2457415322416665e-05\n",
      "train loss:0.00012063464517281862\n",
      "train loss:6.460171354842121e-05\n",
      "train loss:9.60375155466797e-05\n",
      "train loss:0.0009724429280095783\n",
      "train loss:0.020394145426133044\n",
      "train loss:3.92583124903487e-05\n",
      "train loss:0.00043381159239007415\n",
      "train loss:0.0002113234271598628\n",
      "train loss:0.00035176589082737777\n",
      "train loss:0.0004926291876762005\n",
      "train loss:0.00013156361428009076\n",
      "train loss:0.004988881775554611\n",
      "train loss:0.00034089460121600553\n",
      "train loss:0.00014297965490331309\n",
      "train loss:0.0003729144017584694\n",
      "train loss:0.0009012018923344581\n",
      "train loss:0.0015794780337301036\n",
      "train loss:7.828070535439488e-05\n",
      "train loss:0.00010629430205430153\n",
      "train loss:2.2639985273116967e-05\n",
      "train loss:0.00018067972363394473\n",
      "train loss:0.00015428811846779852\n",
      "train loss:6.940232026077006e-05\n",
      "train loss:0.0010424364621927369\n",
      "train loss:0.0010103503462157593\n",
      "train loss:2.5896378819180262e-05\n",
      "train loss:5.11824812501377e-05\n",
      "train loss:0.0013589640024043905\n",
      "train loss:0.003309661542799985\n",
      "train loss:0.005076515603052251\n",
      "train loss:0.0004914068712666218\n",
      "train loss:0.0014993634663609786\n",
      "train loss:5.9246114848147975e-05\n",
      "train loss:0.00011922697422104944\n",
      "train loss:0.00014918624312616047\n",
      "train loss:2.068745675020896e-05\n",
      "train loss:0.00016105567745089934\n",
      "train loss:0.004280319013448592\n",
      "train loss:0.001118148987413069\n",
      "train loss:0.000594734546771839\n",
      "train loss:0.0011585689631311452\n",
      "train loss:0.001909712835026224\n",
      "train loss:0.0003050713129633164\n",
      "train loss:0.00010600238367548955\n",
      "train loss:0.00033264945315761415\n",
      "train loss:0.0014678796576098091\n",
      "train loss:0.0004951795168031299\n",
      "train loss:0.00045467971849235533\n",
      "train loss:0.0013622271378698329\n",
      "train loss:5.987753356998149e-06\n",
      "train loss:0.0006259116495726404\n",
      "train loss:0.0004312978135943729\n",
      "train loss:8.030232743989977e-05\n",
      "train loss:4.2309926452077985e-06\n",
      "train loss:0.0002553187236671375\n",
      "train loss:0.004283819950035994\n",
      "train loss:3.48188411427796e-05\n",
      "train loss:0.00015003569199336248\n",
      "train loss:3.434753556338514e-05\n",
      "train loss:5.028427463440212e-06\n",
      "train loss:0.007918273925487728\n",
      "train loss:0.0007329573480360281\n",
      "train loss:0.0006556551524282158\n",
      "train loss:0.00033842438684852537\n",
      "train loss:0.00027975337643449993\n",
      "train loss:0.0016163022665657374\n",
      "train loss:0.0010387421632094104\n",
      "train loss:6.795846193945078e-05\n",
      "train loss:0.005524895608798425\n",
      "train loss:9.711651755241013e-06\n",
      "train loss:0.0002504597163897068\n",
      "train loss:0.0007471054805237933\n",
      "train loss:0.00019683409529676975\n",
      "train loss:3.685007730129703e-05\n",
      "train loss:0.00022250524316195385\n",
      "train loss:0.0004684187938798413\n",
      "train loss:0.00022213492284895619\n",
      "train loss:0.00011448359121766255\n",
      "train loss:0.00047514952471861357\n",
      "train loss:3.956091997504207e-05\n",
      "train loss:0.0002209092150621694\n",
      "train loss:7.854659929711232e-05\n",
      "train loss:0.0019465171837068588\n",
      "train loss:5.9202522063519835e-05\n",
      "train loss:5.3000113615719535e-05\n",
      "train loss:0.0006611735487323616\n",
      "train loss:0.000529363789080106\n",
      "train loss:0.0004418465473261218\n",
      "train loss:0.0008715580806465747\n",
      "train loss:0.0017458997767809028\n",
      "train loss:0.0002043280993792102\n",
      "train loss:0.0003090726945399069\n",
      "train loss:0.0016308510036963655\n",
      "train loss:0.000864978271732373\n",
      "train loss:0.00022863158188744876\n",
      "train loss:0.00021827740998131163\n",
      "train loss:0.00010662922674774397\n",
      "train loss:7.500151401145935e-05\n",
      "train loss:0.00021377044860687851\n",
      "train loss:3.997300180073333e-05\n",
      "train loss:0.0003990676104118265\n",
      "train loss:0.001214033038553458\n",
      "train loss:0.0009824164069266678\n",
      "train loss:2.74316826584771e-05\n",
      "train loss:0.0014033268638804447\n",
      "train loss:9.83299211354673e-05\n",
      "train loss:7.418695119649528e-05\n",
      "train loss:0.0005820071141145517\n",
      "train loss:0.001152311781817484\n",
      "train loss:0.0016632940995205594\n",
      "train loss:1.6419879927481478e-05\n",
      "train loss:3.863676153898611e-05\n",
      "train loss:0.028442032452672263\n",
      "train loss:0.0007109863320919088\n",
      "train loss:5.4291023296913615e-06\n",
      "train loss:0.0001376443815203032\n",
      "train loss:0.0028083080817382557\n",
      "train loss:6.610221216347691e-07\n",
      "train loss:6.329380193663594e-05\n",
      "train loss:0.00014931594295815138\n",
      "train loss:0.00016518498814408476\n",
      "train loss:0.00017614602540484963\n",
      "train loss:0.0002693112469645736\n",
      "train loss:0.0001761337467873046\n",
      "train loss:4.571565269563498e-05\n",
      "train loss:0.0001719059297637679\n",
      "train loss:0.0007530808827411438\n",
      "train loss:9.874192481842111e-05\n",
      "train loss:0.0008091989042727321\n",
      "train loss:0.0005348598650138402\n",
      "train loss:0.00040065565602480065\n",
      "train loss:0.0006060214444562464\n",
      "train loss:1.3780496584247406e-05\n",
      "train loss:0.00010886028242381982\n",
      "train loss:7.9021829326492e-05\n",
      "train loss:1.2064764514489606e-05\n",
      "train loss:7.527354003023652e-05\n",
      "train loss:0.0011252819540304181\n",
      "train loss:0.0001466522747696758\n",
      "train loss:0.00010219438937467121\n",
      "train loss:0.0004090703269849727\n",
      "train loss:0.00019630291776195536\n",
      "train loss:0.0018212433978416886\n",
      "train loss:0.00046449594274412337\n",
      "train loss:2.4428823897261426e-06\n",
      "train loss:0.0018930118179346042\n",
      "train loss:4.614334937047302e-05\n",
      "train loss:0.0002460259060922262\n",
      "train loss:9.097614249596828e-05\n",
      "train loss:0.0017311351038665427\n",
      "train loss:0.0005123247461096953\n",
      "train loss:2.030393249981032e-06\n",
      "train loss:0.0014942568249988015\n",
      "train loss:9.385058300459052e-06\n",
      "train loss:0.00012431613667558022\n",
      "train loss:0.0005076067298101974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:3.1198785233889164e-05\n",
      "train loss:0.0007291276684691076\n",
      "train loss:9.317412622564823e-05\n",
      "train loss:0.00011960472358412358\n",
      "train loss:0.0008352300012248613\n",
      "train loss:0.0005523995398498076\n",
      "train loss:0.00013381746217111819\n",
      "train loss:0.0006758835212563598\n",
      "train loss:0.0003244550707579278\n",
      "train loss:0.0002528835834240227\n",
      "train loss:1.6399935882698582e-05\n",
      "train loss:0.00013901505832421607\n",
      "train loss:0.000656826494159262\n",
      "train loss:5.339493406303099e-06\n",
      "train loss:2.2102140739193814e-05\n",
      "train loss:0.0004929851188606423\n",
      "train loss:9.111835151640705e-05\n",
      "train loss:5.0089230046377185e-05\n",
      "train loss:0.0005949417170417238\n",
      "train loss:0.00010437819748934154\n",
      "train loss:6.842427388930304e-05\n",
      "train loss:1.552446772804422e-05\n",
      "train loss:2.7861584260865735e-05\n",
      "train loss:0.00011515132550429537\n",
      "train loss:7.735234000223681e-05\n",
      "train loss:0.000825560457704204\n",
      "train loss:0.0014118156327890383\n",
      "train loss:0.00023964089158555333\n",
      "train loss:0.00035548992129462805\n",
      "train loss:6.898040568387729e-05\n",
      "train loss:3.699217589284446e-05\n",
      "train loss:1.676554465857366e-05\n",
      "train loss:0.0001405438977281322\n",
      "train loss:0.0003077124513768401\n",
      "train loss:0.0003185697657667749\n",
      "train loss:0.0003502754336074271\n",
      "train loss:1.2552245499827167e-05\n",
      "train loss:0.0005779277121358431\n",
      "train loss:0.0005844859515638299\n",
      "train loss:0.0035461010677758425\n",
      "train loss:1.0886940221821056e-05\n",
      "train loss:4.537111219743935e-05\n",
      "train loss:3.3869433089515607e-06\n",
      "train loss:3.3237211724760764e-05\n",
      "train loss:0.0005567941070832403\n",
      "train loss:2.1728974792654066e-05\n",
      "train loss:0.0009102614109097979\n",
      "train loss:0.0022190174502281054\n",
      "train loss:0.0004078437038722755\n",
      "train loss:0.00023122049343668525\n",
      "train loss:0.0002893993685837096\n",
      "train loss:0.004412478886047008\n",
      "train loss:8.300568484370647e-05\n",
      "train loss:2.735406048360687e-05\n",
      "train loss:0.0015371039868497466\n",
      "train loss:0.00036644999305939294\n",
      "train loss:4.656940438303901e-05\n",
      "train loss:6.472013874523907e-05\n",
      "train loss:0.0012214715139421423\n",
      "train loss:0.0006563957355640434\n",
      "train loss:0.0007216978963825424\n",
      "train loss:6.45560606164438e-05\n",
      "train loss:3.979215189457924e-06\n",
      "train loss:0.0010318918597196976\n",
      "train loss:0.0008283744076148262\n",
      "train loss:2.9334983982107323e-05\n",
      "train loss:0.00018103822992004547\n",
      "train loss:0.0028195805061870176\n",
      "train loss:8.389553747759314e-05\n",
      "train loss:0.0006044534518962714\n",
      "train loss:2.323038476021874e-06\n",
      "train loss:0.0011986670362422623\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.9885\n"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1, 28, 28),\n",
    "                        conv_param={'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.1)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.save_params('CNNparams.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def filter_show(filters, nx=8, margin=3, scale=10):\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(FN / nx))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i in range(FN):\n",
    "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "network = SimpleConvNet()\n",
    "filter_show(network.params['W1'])\n",
    "\n",
    "network.load_params(\"CNNparams.pkl\")\n",
    "filter_show(network.params['W1'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
