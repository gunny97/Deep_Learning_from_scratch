{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ana3\\envs\\t_f2.2\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ana3\\envs\\t_f2.2\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\ana3\\envs\\t_f2.2\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "from np import *\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "\n",
    "    word_to_id = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id =  len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            \n",
    "    id_to_word = {id_: word for word, id_ in word_to_id.items()}\n",
    "    \n",
    "    corpus = np.array([word_to_id[word] for word in words])\n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n",
    "\n",
    "def cos_similarity(x, y, eps=1e-8):\n",
    "\n",
    "    nx = x / np.sqrt(np.sum(x**2) + eps)\n",
    "    ny = y / np.sqrt(np.sum(y**2) + eps) \n",
    "    return np.dot(nx, ny)\n",
    "\n",
    "\n",
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "\n",
    "    if query not in word_to_id:\n",
    "        print(f'{query}(을)를 찾을 수 없습니다.')\n",
    "        return \n",
    "    \n",
    "    print(f'\\n[query] {query}')\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "    \n",
    "    vocab_size = len(id_to_word)\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "        \n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(f' {id_to_word[i]}: {similarity[i]}')\n",
    "        \n",
    "        count +=1\n",
    "        if count >= top:\n",
    "            return\n",
    "\n",
    "\n",
    "def convert_one_hot(corpus, vocab_size):\n",
    "\n",
    "    N = corpus.shape[0]\n",
    "    if corpus.ndim == 1:\n",
    "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
    "        for idx, word_id in enumerate(corpus):\n",
    "            one_hot[idx, word_id] = 1\n",
    "            \n",
    "    elif corpus.ndim == 2:\n",
    "        C = corpus.shape[1]\n",
    "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
    "        for idx_0, word_ids in enumerate(corpus):\n",
    "            for idx_1, word_id in enumerate(word_ids):\n",
    "                one_hot[idx_0, idx_1, word_id] = 1\n",
    "                \n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "    \n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i  \n",
    "            right_idx = idx + i \n",
    "\n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "                \n",
    "    return co_matrix\n",
    "\n",
    "\n",
    "def ppmi(C, verbose=False, eps=1e-8):\n",
    "\n",
    "    M = np.zeros_like(C, dtype=np.float32)\n",
    "    N = np.sum(C) \n",
    "    S = np.sum(C, axis=0) \n",
    "    total = C.shape[0] * C.shape[1]\n",
    "    cnt = 0\n",
    "    \n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            pmi = np.log2(C[i, j] * N / (S[i]*S[j]) + eps)\n",
    "            M[i, j] = max(0, pmi)\n",
    "            \n",
    "            if verbose:\n",
    "                cnt += 1\n",
    "                if cnt % (total//100) == 0:\n",
    "                    print(f'{(100*cnt/total):.2f} 완료')\n",
    "    return M\n",
    "\n",
    "\n",
    "def eval_perplexity(model, corpus, batch_size=10, time_size=35):\n",
    "    print('퍼플렉서티 평가 중 ...')\n",
    "    corpus_size = len(corpus)\n",
    "    total_loss, loss_cnt = 0, 0\n",
    "    max_iters = (corpus_size - 1) // (batch_size * time_size)\n",
    "    jump = (corpus_size - 1) // batch_size\n",
    "\n",
    "    for iters in range(max_iters):\n",
    "        xs = np.zeros((batch_size, time_size), dtype=np.int32)\n",
    "        ts = np.zeros((batch_size, time_size), dtype=np.int32)\n",
    "        time_offset = iters * time_size\n",
    "        offsets = [time_offset + (i * jump) for i in range(batch_size)]\n",
    "        for t in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                xs[i, t] = corpus[(offset + t) % corpus_size]\n",
    "                ts[i, t] = corpus[(offset + t + 1) % corpus_size]\n",
    "\n",
    "        try:\n",
    "            loss = model.forward(xs, ts, train_flg=False)\n",
    "        except TypeError:\n",
    "            loss = model.forward(xs, ts)\n",
    "        total_loss += loss\n",
    "\n",
    "        sys.stdout.write('\\r%d / %d' % (iters, max_iters))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    print('')\n",
    "    ppl = np.exp(total_loss / max_iters)\n",
    "    return ppl\n",
    "\n",
    "def eval_seq2seq(model, question, correct, id_to_char,\n",
    "                 verbos=False, is_reverse=False):\n",
    "    correct = correct.flatten()\n",
    "    start_id = correct[0]\n",
    "    correct = correct[1:]\n",
    "    guess = model.generate(question, start_id, len(correct))\n",
    "\n",
    "    question = ''.join([id_to_char[int(c)] for c in question.flatten()])\n",
    "    correct = ''.join([id_to_char[int(c)] for c in correct])\n",
    "    guess = ''.join([id_to_char[int(c)] for c in guess])\n",
    "\n",
    "    if verbos:\n",
    "        if is_reverse:\n",
    "            question = question[::-1]\n",
    "\n",
    "        colors = {'ok': '\\033[92m', 'fail': '\\033[91m', 'close': '\\033[0m'}\n",
    "        print('Q', question)\n",
    "        print('T', correct)\n",
    "\n",
    "        is_windows = os.name == 'nt'\n",
    "\n",
    "        if correct == guess:\n",
    "            mark = colors['ok'] + '☑' + colors['close']\n",
    "            if is_windows:\n",
    "                mark = 'O'\n",
    "            print(mark + ' ' + guess)\n",
    "        else:\n",
    "            mark = colors['fail'] + '☒' + colors['close']\n",
    "            if is_windows:\n",
    "                mark = 'X'\n",
    "            print(mark + ' ' + guess)\n",
    "        print('---')\n",
    "\n",
    "    return 1 if guess == correct else 0  \n",
    "\n",
    "def create_contexts_target(corpus, window_size=1):\n",
    "\n",
    "    target = corpus[window_size:-window_size]\n",
    "    contexts = []\n",
    "    \n",
    "    for idx in range(window_size, len(corpus)-window_size):\n",
    "        cs = []\n",
    "        for t in range(-window_size, window_size+1):\n",
    "            if t != 0:\n",
    "                cs.append(corpus[idx + t])\n",
    "        contexts.append(cs)\n",
    "        \n",
    "    return np.array(contexts), np.array(target)\n",
    "\n",
    "\n",
    "def to_cpu(x):\n",
    "    import numpy\n",
    "    if type(x) == numpy.ndarray:\n",
    "        return x\n",
    "    return np.asnumpy(x)\n",
    "\n",
    "\n",
    "def to_gpu(x):\n",
    "    import cupy\n",
    "    if type(x) == cupy.ndarray:\n",
    "        return x\n",
    "    return cupy.asarray(x)\n",
    "    \n",
    "\n",
    "def clip_grads(grads, max_norm):\n",
    "    total_norm = 0\n",
    "    for grad in grads:\n",
    "        total_norm += np.sum(grad ** 2)\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "\n",
    "    rate = max_norm / (total_norm + 1e-6)\n",
    "    if rate < 1:\n",
    "        for grad in grads:\n",
    "            grad *= rate\n",
    "\n",
    "\n",
    "def analogy(a, b, c, word_to_id, id_to_word, word_matrix, top=5, answer=None):\n",
    "    for word in (a, b, c):\n",
    "        if word not in word_to_id:\n",
    "            print(f'{word}(을)를 찾을 수 없습니다.')\n",
    "            return\n",
    "    \n",
    "    print(f'\\n[analogy] {a}:{b} = {c}:?')\n",
    "    a_vec, b_vec, c_vec = word_matrix[word_to_id[a]], word_matrix[word_to_id[b]], word_matrix[word_to_id[c]]\n",
    "    query_vec = b_vec - a_vec + c_vec\n",
    "    query_vec = normalize(query_vec)\n",
    "\n",
    "    similarity = np.dot(word_matrix, query_vec)\n",
    "\n",
    "    if answer is not None:\n",
    "        print(f\"==>{answer}:{str(np.dot(word_matrix[word_to_id[answer]], query_vec))}\")\n",
    "    \n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if np.isnan(similarity[i]):\n",
    "            continue\n",
    "        if id_to_word[i] in (a, b, c):\n",
    "            continue\n",
    "        print(' {0}: {1}'.format(id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    if x.ndim == 2:\n",
    "        s = np.sqrt((x * x).sum(1))\n",
    "        x /= s.reshape((s.shape[0], 1))\n",
    "    elif x.ndim == 1:\n",
    "        s = np.sqrt((x * x).sum())\n",
    "        x /= s\n",
    "    return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
