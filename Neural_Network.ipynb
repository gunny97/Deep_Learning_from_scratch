{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ana3\\envs\\t_f2.2\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ana3\\envs\\t_f2.2\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\ana3\\envs\\t_f2.2\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from layers_collection.ipynb\n"
     ]
    }
   ],
   "source": [
    "# needed library\n",
    "\n",
    "import sys, os\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "import import_ipynb\n",
    "from layers_collection import *\n",
    "\n",
    "class neural_network:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, \n",
    "                 activation='relu', weight_init_std = 'relu', weight_decay_lambda=0,\n",
    "                 dropout = False, dropout_p = 0.3, batchnorm = False):\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_layer = len(hidden_size)\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.batchnorm = batchnorm\n",
    "        \n",
    "        self.weight_decay_lambda = weight_decay_lambda\n",
    "        self.params = {}\n",
    "        \n",
    "        self.weight_init(weight_init_std)\n",
    "        \n",
    "        activation_layer = {\n",
    "            'relu' : Relu,\n",
    "            'sigmoid' : Sigmoid\n",
    "        }\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        \n",
    "        for i in range(1, self.hidden_layer+1):\n",
    "            self.layers['Affine'+str(i)] = Affine(self.params['W'+str(i)], \n",
    "                                                  self.params['b'+str(i)])\n",
    "            \n",
    "            if self.batchnorm:\n",
    "                self.params['gamma'+str(i)] = np.ones(hidden_size[i-1])\n",
    "                self.params['beta'+str(i)] = np.zeros(hidden_size[i-1])\n",
    "                self.layers['BN'+str(i)] = BatchNormalization(self.params['gamma'+str(i)],\n",
    "                                                              self.params['beta'+str(i)])\n",
    "            \n",
    "            self.layers['activation function'+str(i)] = activation_layer[activation]()\n",
    "            \n",
    "            if self.dropout:\n",
    "                self.layers['Dropout'+str(i)] = Dropout(dropout_p)\n",
    "        \n",
    "        i = self.hidden_layer + 1\n",
    "        self.layers['Affine'+str(i)] = Affine(self.params['W'+str(i)], self.params['b'+str(i)])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "    \n",
    "    def weight_init(self, weight_init_std):\n",
    "        total_size = [self.input_size] + self.hidden_size + [self.output_size]\n",
    "        for i in range(1, len(total_size)):\n",
    "            \n",
    "            scale = weight_init_std\n",
    "            \n",
    "            if weight_init_std == 'relu':\n",
    "                scale = np.sqrt(2.0/total_size[i-1])\n",
    "            elif weight_init_std == 'sigmoid':\n",
    "                scale = np.sqrt(1.0 / total_size[i - 1]) \n",
    "            \n",
    "            self.params['W'+str(i)] = scale * np.random.randn(total_size[i-1], total_size[i])\n",
    "            self.params['b'+str(i)] = np.zeros(total_size[i])\n",
    "            \n",
    "    def predict(self, x, flg = False):\n",
    "        for k, layer in self.layers.items():\n",
    "            if 'Dropout' in k or 'BN' in k:\n",
    "                x = layer.forward(x, flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t, flg=False):\n",
    "        y = self.predict(x, flg)\n",
    "        \n",
    "        weight_decay = 0\n",
    "        for i in range(1,self.hidden_layer+2):\n",
    "            W = self.params['W'+str(i)]\n",
    "            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\n",
    "        \n",
    "        return self.last_layer.forward(y,t) + weight_decay\n",
    "        \n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x, flg=False)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t,axis=1)\n",
    "        accuracy = np.sum(y == t)/ float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "\n",
    "        self.loss(x, t, flg=True)\n",
    "\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {}\n",
    "        for i in range(1, self.hidden_layer+2):\n",
    "            grads['W' + str(i)] = self.layers['Affine' + str(i)].dW + self.weight_decay_lambda * self.params['W' + str(i)]\n",
    "            grads['b' + str(i)] = self.layers['Affine' + str(i)].db\n",
    "            \n",
    "            if self.batchnorm and i != self.hidden_layer+1:\n",
    "                grads['gamma' + str(i)] = self.layers['BN' + str(i)].dgamma\n",
    "                grads['beta' + str(i)] = self.layers['BN' + str(i)].dbeta\n",
    "\n",
    "        return grads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
